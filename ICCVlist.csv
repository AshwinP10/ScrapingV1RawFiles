"Document Title",Authors,"Author Affiliations","Publication Title",Date Added To Xplore,"Publication Year","Volume","Issue","Start Page","End Page","Abstract","ISSN",ISBNs,"DOI",Funding Information,PDF Link,"Author Keywords","IEEE Terms","INSPEC Controlled Terms","INSPEC Non-Controlled Terms","Mesh_Terms",Article Citation Count,Patent Citation Count,"Reference Count","License",Online Date,Issue Date,"Meeting Date","Publisher",Document Identifier
"Measuring Crowd Collectiveness via Global Motion Correlation","L. Mei; J. Lai; Z. Chen; X. Xie","School of Electronics and Information Technology, Sun Yat-sen University, China; School of Data and Computer Science, Sun Yat-Sen University, Guangzhou, China; School of Data and Computer Science, Sun Yat-Sen University, Guangzhou, China; School of Data and Computer Science, Sun Yat-Sen University, Guangzhou, China","2019 IEEE/CVF International Conference on Computer Vision Workshop (ICCVW)","5 Mar 2020","2019","","","1222","1231","Crowd collectiveness refers to the behavior consistency of crowd scenes, which reflects the degree of collective movements among massive individuals in crowd systems. The existing methods focus on measuring the discrepancy of motion direction among the individuals. However, few studies consider the magnitude discrepancy of velocity in a crowd and the collectiveness among different crowds, which can also affect the overall crowd collectiveness. In this paper, we propose a novel descriptor which combines intra-crowd collectiveness with inter-crowd collectiveness to solve the problem. For intra-crowd collectiveness, we introduce the energy spread process to identify the impacting factors of collectiveness, then measure the collectiveness of individuals within a crowd cluster by computing their similarities of magnitude and direction from the optical flow. For inter-crowd collectiveness, we assess the motion consistency among various crowd clusters generated from collective merging. Experimental results demonstrate that how the new collectiveness descriptor improves performance on three different crowd datasets, thus validating the superiority of the proposed descriptor.","2473-9944","978-1-7281-5023-9","10.1109/ICCVW.2019.00155","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9022173","Measuring Crowd collectiveness;Collective human behaviour;Crowd motion analysis;Global motion correlation;Energy spread process","Merging;Motion measurement;Optical imaging;Trajectory;Optical variables measurement;Microorganisms;Motion segmentation","image motion analysis;image sequences","optical flow;energy spread process;crowd scene behavior consistency;crowd systems;collective movements;global motion correlation;collectiveness descriptor;collective merging;inter-crowd collectiveness;intra-crowd collectiveness","","1","","48","IEEE","5 Mar 2020","","","IEEE","IEEE Conferences"
"Social and Scene-Aware Trajectory Prediction in Crowded Spaces","M. Lisotto; P. Coscia; L. Ballan","Department of Mathematics “Tullio Levi-Civita”, University of Padova, Italy; Department of Mathematics “Tullio Levi-Civita”, University of Padova, Italy; Department of Mathematics “Tullio Levi-Civita”, University of Padova, Italy","2019 IEEE/CVF International Conference on Computer Vision Workshop (ICCVW)","5 Mar 2020","2019","","","2567","2574","Mimicking human ability to forecast future positions or interpret complex interactions in urban scenarios, such as streets, shopping malls or squares, is essential to develop socially compliant robots or self-driving cars. Autonomous systems may gain advantage on anticipating human motion to avoid collisions or to naturally behave alongside people. To foresee plausible trajectories, we construct an LSTM (long short-term memory)-based model considering three fundamental factors: people interactions, past observations in terms of previously crossed areas and semantics of surrounding space. Our model encompasses several pooling mechanisms to join the above elements defining multiple tensors, namely social, navigation and semantic tensors. The network is tested in unstructured environments where complex paths emerge according to both internal (intentions) and external (other people, not accessible areas) motivations. As demonstrated, modeling paths unaware of social interactions or context information, is insufficient to correctly predict future positions. Experimental results corroborate the effectiveness of the proposed framework in comparison to LSTM-based models for human path prediction.","2473-9944","978-1-7281-5023-9","10.1109/ICCVW.2019.00314","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9021955","human trajectory forecasting;LSTM based model;scene aware","Semantics;Tensile stress;Trajectory;Navigation;Predictive models;Mathematical model;Buildings","collision avoidance;human-robot interaction;mobile robots;recurrent neural nets","long short-term memory-based model;crossed areas;surrounding space;pooling mechanisms;semantic tensors;complex paths;accessible areas;social interactions;LSTM-based models;human path prediction;social and scene-aware trajectory prediction;plausible trajectories;human motion;autonomous systems;self-driving cars;socially compliant robots;shopping malls;urban scenarios;complex interactions;human ability;crowded spaces","","42","","22","IEEE","5 Mar 2020","","","IEEE","IEEE Conferences"
"SLAMANTIC - Leveraging Semantics to Improve VSLAM in Dynamic Environments","M. Schörghuber; D. Steininger; Y. Cabon; M. Humenberger; M. Gelautz","Austrian Institute of Technology, Vienna, Austria; Austrian Institute of Technology; NAVER LABS Europe; NAVER LABS Europe; Vienna University of Technology","2019 IEEE/CVF International Conference on Computer Vision Workshop (ICCVW)","5 Mar 2020","2019","","","3759","3768","In this paper, we tackle the challenge for VSLAM of handling non-static environments. We propose to include semantic information obtained by deep learning methods in the traditional geometric pipeline. Specifically, we compute a confidence measure for each map point as a function of its semantic class (car, person, building, etc.) and its detection consistency over time. The confidence is then applied to guide the usage of each point in the mapping and localization stage. Points with high confidence are used to verify points with low confidence in order to select the final set of points for pose computation and mapping. Furthermore, we can handle map points whose state may change between static and dynamic (a car can be parked or in motion). Evaluating our method on public datasets, we show that it can successfully solve challenging situations in dynamic environments which cause state-of-the-art baseline VSLAM algorithms to fail and that it maintains performance on static scenes. Code is available at github.com/mthz/slamantic.","2473-9944","978-1-7281-5023-9","10.1109/ICCVW.2019.00468","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9022073","vslam;semantic segmentation;dynamic environments;robust vslam","Semantics;Automobiles;Dynamics;Heuristic algorithms;Cameras;Three-dimensional displays;Buildings","learning (artificial intelligence);semantic networks;SLAM (robots)","pose computation;geometric pipeline;SLAMANTIC;VSLAM;static scenes;localization stage;semantic class;map point;confidence measure;deep learning methods;semantic information;nonstatic environments;dynamic environments","","14","","35","IEEE","5 Mar 2020","","","IEEE","IEEE Conferences"
"Towards Learning Multi-Agent Negotiations via Self-Play","Y. Tang","Apple Inc, USA","2019 IEEE/CVF International Conference on Computer Vision Workshop (ICCVW)","5 Mar 2020","2019","","","2427","2435","Making sophisticated, robust, and safe sequential decisions is at the heart of intelligent systems. This is especially critical for planning in complex multi-agent environments, where agents need to anticipate other agents' intentions and possible future actions. Traditional methods formulate the problem as a Markov Decision Process, but the solutions often rely on various assumptions and become brittle when presented with corner cases. In contrast, deep reinforcement learning (Deep RL) has been very effective at finding policies by simultaneously exploring, interacting, and learning from environments. Leveraging the powerful Deep RL paradigm, we demonstrate that an iterative procedure of self-play can create progressively more diverse environments, leading to the learning of sophisticated and robust multi-agent policies. We demonstrate this in a challenging multi-agent simulation of merging traffic, where agents must interact and negotiate with others in order to successfully merge on or off the road. While the environment starts off simple, we increase its complexity by iteratively adding an increasingly diverse set of agents to the agent zoo as training progresses. Qualitatively, we find that through self-play, our policies automatically learn interesting behaviors such as defensive driving, overtaking, yielding, and the use of signal lights to communicate intentions to other agents. In addition, quantitatively, we show a dramatic improvement in the success rate of merging maneuvers from 63% to over 98%.","2473-9944","978-1-7281-5023-9","10.1109/ICCVW.2019.00297","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9022183","Deep Reinforcement Learning;Autonomous Driving;Multi agent Systems;Self Play","Roads;Games;Robustness;Planning;Complexity theory;Training;Learning (artificial intelligence)","decision theory;learning (artificial intelligence);Markov processes;multi-agent systems","sophisticated decisions;safe sequential decisions;intelligent systems;complex multiagent environments;Markov decision process;corner cases;deep reinforcement learning;iterative procedure;diverse environments;multiagent simulation;deep RL paradigm;multiagent negotiations learning;self-play","","10","","35","IEEE","5 Mar 2020","","","IEEE","IEEE Conferences"
"Crowd Counting on Images with Scale Variation and Isolated Clusters","H. Bai; S. Wen; S. . -H. G. Chan","Department of Computer Science and Engineering, The Hong Kong University of Science and Technology, Hong Kong, China; Department of Computer Science and Engineering, The Hong Kong University of Science and Technology, Hong Kong, China; Department of Computer Science and Engineering, The Hong Kong University of Science and Technology, Hong Kong, China","2019 IEEE/CVF International Conference on Computer Vision Workshop (ICCVW)","5 Mar 2020","2019","","","18","27","Crowd counting is to estimate the number of objects (e.g., people or vehicles) in an image of unconstrained congested scenes. Designing a general crowd counting algorithm applicable to a wide range of crowd images is challenging, mainly due to the possibly large variation in object scales and the presence of many isolated small clusters. Previous approaches based on convolution operations with multi-branch architecture are effective for only some narrow bands of scales, and have not captured the long-range contextual relationship due to isolated clustering. To address that, we propose SACANet, a novel scale-adaptive long-range context-aware network for crowd counting. SACANet consists of three major modules: the pyramid contextual module which extracts long-range contextual information and enlarges the receptive field, a scale-adaptive self-attention multi-branch module to attain high scale sensitivity and detection accuracy of isolated clusters, and a hierarchical fusion module to fuse multi-level self-attention features. With group normalization, SACANet achieves better optimality in the training process. We have conducted extensive experiments using the VisDrone2019 People dataset, the VisDrone2019 Vehicle dataset, and some other challenging benchmarks. As compared with the state-of-the-art methods, SACANet is shown to be effective, especially for extremely crowded conditions with diverse scales and scattered clusters, and achieves much lower MAE as compared with baselines.","2473-9944","978-1-7281-5023-9","10.1109/ICCVW.2019.00009","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9022196","Crowd Counting;Scale Variation;Isolated Clusters","Feature extraction;Surveillance;Cameras;Estimation;Data mining;Drones;Convolution","feature extraction;image classification;image segmentation;object detection;pattern clustering","VisDrone2019 Vehicle dataset;VisDrone2019 People dataset;scale-adaptive long-range context-aware network;crowd counting algorithm;isolated clustering;long-range contextual relationship;multibranch architecture;isolated small clusters;crowd images;unconstrained congested scenes;scattered clusters;extremely crowded conditions;multilevel self-attention features;hierarchical fusion module;high scale sensitivity;scale-adaptive self-attention multibranch module;long-range contextual information;pyramid contextual module;SACANet","","11","","38","IEEE","5 Mar 2020","","","IEEE","IEEE Conferences"
"DECCNet: Depth Enhanced Crowd Counting","S. -D. Yang; H. -T. Su; W. H. Hsu; W. -C. Chen",National Taiwan University; National Taiwan University; National Taiwan University; National Taiwan University,"2019 IEEE/CVF International Conference on Computer Vision Workshop (ICCVW)","5 Mar 2020","2019","","","4521","4530","Crowd counting which aims to calculate the number of total instances on an image is a classic but crucial task that supports many applications. Most of the prior works are based on the RGB channels on the images and achieve satisfied performance. However, previous approaches suffer from counting highly congested region due to the incomplete and blurry shapes. In this paper, we present an effective crowd counting method, Depth Enhanced Crowd Counting Network (DECCNet), which leverages the estimated depth information with our novel Bidirectional Cross-modal Attention (BCA) mechanism. Utilizing the depth information enables our model to explicitly learn to pay attention to those congested regions on the basis of the depth information. Our BCA mechanism interactively fuses two different input modalities by learning to focus on the informative parts according to each other. In our experiments, we demonstrate that DECCNet outperforms the state-of-the-art on the two largest crowd counting datasets available, including UCF-QNRF, which has the highest crowd density. The visualized result shows that our method can accurately regress dense regions through leveraging depth information. Ablation studies also indicate that each component of our method is beneficial to final prediction.","2473-9944","978-1-7281-5023-9","10.1109/ICCVW.2019.00553","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9022022","crowd counting;cross modal fusion;RGBD","Decoding;Convolution;Estimation;Task analysis;Fuses;Feature extraction;Image color analysis","feature extraction;image colour analysis;learning (artificial intelligence);neural nets;object detection;regression analysis","UCF-QNRF;crowd counting datasets;bidirectional cross-modal attention mechanism;depth enhanced crowd counting network;blurry shapes;RGB channels;BCA mechanism;DECCNet","","5","","45","IEEE","5 Mar 2020","","","IEEE","IEEE Conferences"
"Characterizing Scattered Occlusions for Effective Dense-Mode Crowd Counting","K. J. Almalki; B. -Y. Choi; Y. Chen; S. Song","School of Computing and Engineering, University of Missouri, Kansas City, MO, USA; School of Computing and Engineering, University of Missouri, Kansas City, MO, USA; Binghamton University, Binghamton, NY, USA; School of Computing and Engineering, University of Missouri, Kansas City, MO, USA","2021 IEEE/CVF International Conference on Computer Vision Workshops (ICCVW)","24 Nov 2021","2021","","","3833","3842","We propose a novel deep learning approach for effective dense crowd counting by characterizing scattered occlusions, named CSONet. CSONet recognizes the implications of event-induced, scene-embedded, and multitudinous obstacles such as umbrellas and picket signs to achieve an accurate crowd analysis result. CSONet is the first deep learning model for characterizing scattered occlusions of effective dense-mode crowd counting to the best of our knowledge. We have collected and annotated two new scattered occlusion object datasets, which contain crowd images occluded with umbrellas (csoumbrellas dataset) and picket signs (cso-pickets dataset). We have designed and implemented a new crowd overfit reduction network by adding both spatial pyramid pooling and dilated convolution layers over modified VGG16 for capturing high-level features of extended receptive fields. CSONet was trained on the two new scattered occlusion datasets and the ShanghaiTech A and B datasets. We also have built an algorithm that merges scattered object maps and density heatmaps of visible humans to generate a more accurate crowd density heatmap output. Through extensive evaluations, we demonstrate that the accuracy of CSONet with scattered occlusion images outperforms over the state-of-art existing crowd counting approaches by 30% to 100% in both mean absolute error and mean square error.","2473-9944","978-1-6654-0191-3","10.1109/ICCVW54120.2021.00428","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9607560","","Heating systems;Deep learning;Computer vision;Convolution;Conferences;Mean square error methods;Computer architecture","deep learning (artificial intelligence);image classification","effective dense-mode crowd counting;picket signs;deep learning model;scattered occlusion object datasets;crowd images;crowd overfit reduction network;scattered occlusion images;crowd density heatmap output accuracy;CSONet;cso-umbrellas dataset;spatial pyramid pooling;dilated convolution layers","","3","","43","IEEE","24 Nov 2021","","","IEEE","IEEE Conferences"
"Pushing the Frontiers of Unconstrained Crowd Counting: New Dataset and Benchmark Method","V. Sindagi; R. Yasarla; V. Patel","Johns Hopkins University; Department of Electrical and Computer Engineering, Johns Hopkins University, Baltimore, MD, USA; Johns Hopkins University","2019 IEEE/CVF International Conference on Computer Vision (ICCV)","27 Feb 2020","2019","","","1221","1231","In this work, we propose a novel crowd counting network that progressively generates crowd density maps via residual error estimation. The proposed method uses VGG16 as the backbone network and employs density map generated by the final layer as a coarse prediction to refine and generate finer density maps in a progressive fashion using residual learning. Additionally, the residual learning is guided by an uncertainty-based confidence weighting mechanism that permits the flow of only high-confidence residuals in the refinement path. The proposed Confidence Guided Deep Residual Counting Network (CG-DRCN) is evaluated on recent complex datasets, and it achieves significant improvements in errors. Furthermore, we introduce a new large scale unconstrained crowd counting dataset (JHU-CROWD) that is ~2.8 larger than the most recent crowd counting datasets in terms of the number of images. It contains 4,250 images with 1.11 million annotations. In comparison to existing datasets, the proposed dataset is collected under a variety of diverse scenarios and environmental conditions. Specifically, the dataset includes several images with weather-based degradations and illumination variations in addition to many distractor images, making it a very challenging dataset. Additionally, the dataset consists of rich annotations at both image-level and head-level. Several recent methods are evaluated and compared on this dataset.","2380-7504","978-1-7281-4803-8","10.1109/ICCV.2019.00131","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9009496","","Image resolution;Task analysis;Training;Head;Estimation;Benchmark testing;Error analysis","feature extraction;image classification;learning (artificial intelligence);object detection;video surveillance","benchmark method;crowd counting network;crowd density maps;residual error estimation;backbone network;finer density maps;residual learning;uncertainty-based confidence weighting mechanism;high-confidence residuals;refinement path;complex datasets;challenging dataset;confidence guided deep residual counting network;large scale unconstrained crowd counting dataset;weather-based degradations;illumination variations;distractor images","","44","","65","IEEE","27 Feb 2020","","","IEEE","IEEE Conferences"
"VisDrone-CC2021: The Vision Meets Drone Crowd Counting Challenge Results","Z. Liu; Z. He; L. Wang; W. Wang; Y. Yuan; D. Zhang; J. Zhang; P. Zhu; L. V. Gool; J. Han; S. Hoi; Q. Hu; M. Liu; J. Pan; B. Yin; B. Zhang; C. Liu; D. Ding; D. Liang; G. Ding; H. Lu; H. Lin; J. Chen; J. Li; L. Liu; L. Zhou; M. Shi; Q. Yang; Q. He; S. Peng; W. Xu; W. Han; X. Bai; X. Chen; Y. Wang; Y. Xia; Y. Tao; Z. Chen; Z. Cao","Tianjin University, Tianjin, China; The Hong Kong University of Science and Technology, Hong Kong, China; The Hong Kong University of Science and Technology, Hong Kong, China; ETH Zurich, Zurich, Switzerland; City University of Hong Kong, Hong Kong, China; Northwestern Polytechnical University, Xian, China; Nanjing University of Information Science and Technology, Nanjing, China; Tianjin University, Tianjin, China; ETH Zurich, Zurich, Switzerland; Northwestern Polytechnical University, Xian, China; Singapore Management University, Singapore; Tianjin University, Tianjin, China; The Hong Kong University of Science and Technology, Hong Kong, China; Tianjin University, Tianjin, China; University of Science and Technology of China, Hefei, China; Beijing University of Posts and Telecommunications, Beijing, China; Huazhong University of Science and Technology, Wuhan, China; Huazhong University of Science and Technology, Wuhan, China; Huazhong University of Science and Technology, Wuhan, China; Wuhan University, Wuhan, China; Huazhong University of Science and Technology, Wuhan, China; Xi’an Jiaotong University, Xi’an, China; Wuhan University, Wuhan, China; Xi’an Jiaotong University, Xi’an, China; Huazhong University of Science and Technology, Wuhan, China; Wuhan University, Wuhan, China; Huazhong University of Science and Technology, Wuhan, China; University of Science and Technology of China, Hefei, China; University of Science and Technology of China, Hefei, China; University of Science and Technology of China, Hefei, China; Beijing University of Posts and Telecommunications, Beijing, China; Wuhan University, Wuhan, China; Huazhong University of Science and Technology, Wuhan, China; Huazhong University of Science and Technology, Wuhan, China; Xi’an Jiaotong University, Xi’an, China; University of Science and Technology of China, Hefei, China; Wuhan University, Wuhan, China; Wuhan University, Wuhan, China; Huazhong University of Science and Technology, Wuhan, China","2021 IEEE/CVF International Conference on Computer Vision Workshops (ICCVW)","24 Nov 2021","2021","","","2830","2838","Crowding counting research evolves quickly by the lever-age of development in deep learning. Many researchers put their efforts into crowd counting tasks and have achieved many significant improvements. However, current datasets still barely satisfy this evolution and high quality evaluation data is urgent. Motivated by high quality and quantity study in crowding counting, we collect a drone-captured dataset formed by 5,468 images(images in RGB and thermal appear in pairs and 2,734 respectively). There are 1,807 pairs of images for training, and 927 pairs for testing. We manually annotate persons with points in each frame. Based on this dataset, we organized the Vision Meets Drone Crowd Counting Challenge(Visdrone-CC2021) in conjunction with the International Conference on Computer Vision (ICCV 2021). Our challenge attracts many researchers to join, which pave the road of speed up the milestone in crowding counting. To summarize the competition, we select the most remarkable algorithms from participants’ sub-missions and provide a detailed analysis of the evaluation results. More information can be found at the website: http://www.aiskyeye.com/.","2473-9944","978-1-6654-0191-3","10.1109/ICCVW54120.2021.00317","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9607764","","Training;Deep learning;Computer vision;Roads;Conferences;Data mining;Task analysis","autonomous aerial vehicles;image colour analysis;image motion analysis;learning (artificial intelligence);object detection;robot vision;visual databases","VisDrone-CC2021;counting research;deep learning;crowd counting tasks;high quality evaluation data;crowding counting;drone-captured dataset;computer vision;ICCV 2021;RGB images;thermal images;Vision Meets Drone Crowd Counting Challenge","","10","","31","IEEE","24 Nov 2021","","","IEEE","IEEE Conferences"
"Audio-Visual Transformer Based Crowd Counting","U. Sajid; X. Chen; H. Sajid; T. Kim; G. Wang","Electrical Engineering and Computer Science, University of Kansas, Lawrence, KS, USA; Electrical Engineering and Computer Science, University of Kansas, Lawrence, KS, USA; School of Mechanical and Manufacturing Engineering, NUST, Islamabad, Pakistan; Electrical Engineering and Computer Science, University of Kansas, Lawrence, KS, USA; Department of Computer Science, Ryerson University, Toronto, ON, Canada","2021 IEEE/CVF International Conference on Computer Vision Workshops (ICCVW)","24 Nov 2021","2021","","","2249","2259","Crowd estimation is a very challenging problem. The most recent study tries to exploit auditory information to aid the visual models, however, the performance is limited due to the lack of an effective approach for feature extraction and integration. The paper proposes a new audiovisual multi-task network to address the critical challenges in crowd counting by effectively utilizing both visual and audio inputs for better modalities association and productive feature extraction. The proposed network introduces the notion of auxiliary and explicit image patch-importance ranking (PIR) and patch-wise crowd estimate (PCE) information to produce a third (run-time) modality. These modalities (audio, visual, run-time) undergo a transformer-inspired cross-modality co-attention mechanism to finally output the crowd estimate. To acquire rich visual features, we propose a multi-branch structure with transformer-style fusion in-between. Extensive experimental evaluations show that the proposed scheme outperforms the state-of-the-art networks under all evaluation settings with up to 33.8% improvement. We also analyze and compare the vision-only variant of our network and empirically demonstrate its superiority over previous approaches.","2473-9944","978-1-6654-0191-3","10.1109/ICCVW54120.2021.00254","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9607793","","Visualization;Computer vision;Conferences;Computational modeling;Estimation;Benchmark testing;Feature extraction","feature extraction;image fusion;image motion analysis","audio-visual transformer;crowd counting;crowd estimation;auditory information;audiovisual multitask network;modalities association;productive feature extraction;transformer-inspired cross-modality co-attention mechanism;visual features;multibranch structure;transformer-style fusion;explicit image patch-importance ranking;PIR;patch-wise crowd estimate information;PCE;vision-only variant","","8","","60","IEEE","24 Nov 2021","","","IEEE","IEEE Conferences"
"Counterfactual Critic Multi-Agent Training for Scene Graph Generation","L. Chen; H. Zhang; J. Xiao; X. He; S. Pu; S. -F. Chang","DCD Lab, College of Computer Science and Technology, Zhejiang University; MReal Lab, Nanyang Technological University; DCD Lab, College of Computer Science and Technology, Zhejiang University; University of Science and Technology of China; Hikvision Research Institute; DVMM Lab, Columbia University","2019 IEEE/CVF International Conference on Computer Vision (ICCV)","27 Feb 2020","2019","","","4612","4622","Scene graphs --- objects as nodes and visual relationships as edges --- describe the whereabouts and interactions of objects in an image for comprehensive scene understanding. To generate coherent scene graphs, almost all existing methods exploit the fruitful visual context by modeling message passing among objects. For example, ``person'' on ``bike'' can help to determine the relationship ``ride'', which in turn contributes to the confidence of the two objects. However, we argue that the visual context is not properly learned by using the prevailing cross-entropy based supervised learning paradigm, which is not sensitive to graph inconsistency: errors at the hub or non-hub nodes should not be penalized equally. To this end, we propose a Counterfactual critic Multi-Agent Training (CMAT) approach. CMAT is a multi-agent policy gradient method that frames objects into cooperative agents, and then directly maximizes a graph-level metric as the reward. In particular, to assign the reward properly to each agent, CMAT uses a counterfactual baseline that disentangles the agent-specific reward by fixing the predictions of other agents. Extensive validations on the challenging Visual Genome benchmark show that CMAT achieves a state-of-the-art performance by significant gains under various settings and metrics.","2380-7504","978-1-7281-4803-8","10.1109/ICCV.2019.00471","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9010810","","Visualization;Training;Context modeling;Measurement;Proposals;Image edge detection;Task analysis","biology computing;entropy;genomics;gradient methods;graph theory;image processing;learning (artificial intelligence);message passing;multi-agent systems","CMAT;counterfactual baseline;agent-specific reward;scene graph generation;visual relationships;comprehensive scene understanding;coherent scene graphs;fruitful visual context;modeling message passing;relationship ride;nonhub nodes;multiagent policy gradient method;frames objects;graph-level metric;cross-entropy based supervised learning paradigm;counterfactual critic multiagent training","","63","","80","IEEE","27 Feb 2020","","","IEEE","IEEE Conferences"
"Prior Guided Dropout for Robust Visual Localization in Dynamic Environments","Z. Huang; Y. Xu; J. Shi; X. Zhou; H. Bao; G. Zhang","State Key Lab of CAD&CG, Zhejiang University; SenseTime Research; SenseTime Research; State Key Lab of CAD&CG, Zhejiang University; State Key Lab of CAD&CG, Zhejiang University; State Key Lab of CAD&CG, Zhejiang University","2019 IEEE/CVF International Conference on Computer Vision (ICCV)","27 Feb 2020","2019","","","2791","2800","Camera localization from monocular images has been a long-standing problem, but its robustness in dynamic environments is still not adequately addressed. Compared with classic geometric approaches, modern CNN-based methods (e.g. PoseNet) have manifested the reliability against illumination or viewpoint variations, but they still have the following limitations. First, foreground moving objects are not explicitly handled, which results in poor performance and instability in dynamic environments. Second, the output for each image is a point estimate without uncertainty quantification. In this paper, we propose a framework which can be generally applied to existing CNN-based pose regressors to improve their robustness in dynamic environments. The key idea is a prior guided dropout module coupled with a self-attention module which can guide CNNs to ignore foreground objects during both training and inference. Additionally, the dropout module enables the pose regressor to output multiple hypotheses from which the uncertainty of pose estimates can be quantified and leveraged in the following uncertainty-aware pose graph optimization to improve the robustness further. We achieve an average accuracy of 9.98m/3.63° on RobotCar dataset, which outperforms the state-of-the-art method by 62.97%/47.08%. The source code of our implementation is available at https://github.com/zju3dv/RVL-Dynamic.","2380-7504","978-1-7281-4803-8","10.1109/ICCV.2019.00288","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9008288","","Feature extraction;Robustness;Cameras;Visualization;Training;Neural networks;Uncertainty","cameras;convolutional neural nets;geometry;graph theory;image motion analysis;object detection;optimisation;pose estimation;robot vision;SLAM (robots)","robust visual localization;dynamic environments;camera localization;monocular images;geometric approaches;dropout module;prior guided dropout;CNN-based methods;PoseNet;RobotCardataset;uncertainty-aware pose graph optimization","","26","","44","IEEE","27 Feb 2020","","","IEEE","IEEE Conferences"
"Perspective-Guided Convolution Networks for Crowd Counting","Z. Yan; Y. Yuan; W. Zuo; X. Tan; Y. Wang; S. Wen; E. Ding","Harbin Institute of Technology; Department of Computer Vision Technology (VIS), Baidu Inc.,; Harbin Institute of Technology; Department of Computer Vision Technology (VIS), Baidu Inc.,; Harbin Institute of Technology; Department of Computer Vision Technology (VIS), Baidu Inc.,; Department of Computer Vision Technology (VIS), Baidu Inc.,","2019 IEEE/CVF International Conference on Computer Vision (ICCV)","27 Feb 2020","2019","","","952","961","In this paper, we propose a novel perspective-guided convolution (PGC) for convolutional neural network (CNN) based crowd counting (i.e. PGCNet), which aims to overcome the dramatic intra-scene scale variations of people due to the perspective effect. While most state-of-the-arts adopt multi-scale or multi-column architectures to address such issue, they generally fail in modeling continuous scale variations since only discrete representative scales are considered. PGCNet, on the other hand, utilizes perspective information to guide the spatially variant smoothing of feature maps before feeding them to the successive convolutions. An effective perspective estimation branch is also introduced to PGCNet, which can be trained in either supervised setting or weakly-supervised setting when the branch has been pre-trained. Our PGCNet is single-column with moderate increase in computation, and extensive experimental results on four benchmark datasets show the improvements of our method against the state-of-the-arts. Additionally, we also introduce Crowd Surveillance, a large scale dataset for crowd counting that contains 13,000+ high-resolution images with challenging scenarios. Code is available at https://github.com/Zhaoyi-Yan/PGCNet.","2380-7504","978-1-7281-4803-8","10.1109/ICCV.2019.00104","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9010874","","Convolution;Estimation;Smoothing methods;Kernel;Computer architecture;Computer vision;Benchmark testing","convolutional neural nets;image resolution;object detection","perspective-guided convolution networks;convolutional neural network based crowd;multicolumn architectures;spatially variant smoothing;crowd surveillance;CNN based crowd counting;PGCNet;perspective estimation branch;high-resolution images","","111","","30","IEEE","27 Feb 2020","","","IEEE","IEEE Conferences"
"Bayesian Loss for Crowd Count Estimation With Point Supervision","Z. Ma; X. Wei; X. Hong; Y. Gong","Faculty of Electronic and Information Engineering, Xi’an Jiaotong University; Faculty of Electronic and Information Engineering, Xi’an Jiaotong University; Faculty of Electronic and Information Engineering, Xi’an Jiaotong University; Faculty of Electronic and Information Engineering, Xi’an Jiaotong University","2019 IEEE/CVF International Conference on Computer Vision (ICCV)","27 Feb 2020","2019","","","6141","6150","In crowd counting datasets, each person is annotated by a point, which is usually the center of the head. And the task is to estimate the total count in a crowd scene. Most of the state-of-the-art methods are based on density map estimation, which convert the sparse point annotations into a “ground truth” density map through a Gaussian kernel, and then use it as the learning target to train a density map estimator. However, such a ""ground-truth"" density map is imperfect due to occlusions, perspective effects, variations in object shapes, etc. On the contrary, we propose Bayesian loss, a novel loss function which constructs a density contribution probability model from the point annotations. Instead of constraining the value at every pixel in the density map, the proposed training loss adopts a more reliable supervision on the count expectation at each annotated point. Without bells and whistles, the loss function makes substantial improvements over the baseline loss on all tested datasets. Moreover, our proposed loss function equipped with a standard backbone network, without using any external detectors or multi-scale architectures, plays favourably against the state of the arts. Our method outperforms previous best approaches by a large margin on the latest and largest UCF-QNRF dataset.","2380-7504","978-1-7281-4803-8","10.1109/ICCV.2019.00624","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9009503","","Training;Estimation;Kernel;Head;Bayes methods;Shape;Feature extraction","Bayes methods;learning (artificial intelligence);object detection;probability","density contribution probability model;training loss;count expectation;annotated point;baseline loss;Bayesian loss;crowd count estimation;point supervision;crowd counting datasets;total count;crowd scene;density map estimation;sparse point annotations;density map estimator;ground-truth density map;loss function;Gaussian kernel;external detectors;UCF-QNRF dataset","","233","","61","IEEE","27 Feb 2020","","","IEEE","IEEE Conferences"
"PRECOG: PREdiction Conditioned on Goals in Visual Multi-Agent Settings","N. Rhinehart; R. Mcallister; K. Kitani; S. Levine","Carnegie Mellon University; University of California, Berkeley; Carnegie Mellon University; University of California, Berkeley","2019 IEEE/CVF International Conference on Computer Vision (ICCV)","27 Feb 2020","2019","","","2821","2830","For autonomous vehicles (AVs) to behave appropriately on roads populated by human-driven vehicles, they must be able to reason about the uncertain intentions and decisions of other drivers from rich perceptual information. Towards these capabilities, we present a probabilistic forecasting model of future interactions between a variable number of agents. We perform both standard forecasting and the novel task of conditional forecasting, which reasons about how all agents will likely respond to the goal of a controlled agent (here, the AV). We train models on real and simulated data to forecast vehicle trajectories given past positions and LIDAR. Our evaluation shows that our model is substantially more accurate in multi-agent driving scenarios compared to existing state-of-the-art. Beyond its general ability to perform conditional forecasting queries, we show that our model's predictions of all agents improve when conditioned on knowledge of the AV's goal, further illustrating its capability to model agent interactions.","2380-7504","978-1-7281-4803-8","10.1109/ICCV.2019.00291","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9009551","","Forecasting;Predictive models;Robots;Automobiles;Planning;Computational modeling;Zirconium","multi-agent systems;remotely operated vehicles;road vehicles","visual multiagent settings;autonomous vehicles;roads;human-driven vehicles;uncertain intentions;perceptual information;probabilistic forecasting model;standard forecasting;controlled agent;vehicle trajectories;multiagent driving scenarios;conditional forecasting queries;model agent interactions;AV goal;PRECOG;PREdiction Conditioned On Goals","","155","2","37","IEEE","27 Feb 2020","","","IEEE","IEEE Conferences"
"An Indoor Crowd Detection Network Framework Based on Feature Aggregation Module and Hybrid Attention Selection Module","W. Shen; P. Qin; J. Zeng","Taiyuan City, Shanxi Province, China; Taiyuan City, Shanxi Province, China; Taiyuan City, Shanxi Province, China","2019 IEEE/CVF International Conference on Computer Vision Workshop (ICCVW)","5 Mar 2020","2019","","","82","90","In this paper, we present an indoor crowd detection network framework based on feature aggregation module and hybrid attention selection module (HSFA2Net). In order to better provide the details needed for small scale pupulation detection, we propose a novel feature aggregation module (FAM), which uses the idea of fusion and decomposition to aggregate contextual feature information. Since the indoor population feature and background feature overlap and the classification boundaries are not obvious, the proposed improved hybrid attention selection module (HASM) combines the selection mechanism with the previously proposed mixed attention module. Ultimately, we implement an indoor crowd detection network framework and achieve a recall rate of 0.92 and an F1 score of 0.92 on a public dataset SCUT-HEAD.","2473-9944","978-1-7281-5023-9","10.1109/ICCVW.2019.00016","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9022292","Indoor crowd detection;Attention mechanism;Feature aggregation","Feature extraction;Convolution;Head;Task analysis;Detectors;Object detection;Semantics","feature extraction;image fusion;object detection","indoor crowd detection network framework;hybrid attention selection module;novel feature aggregation module;contextual feature information;indoor population feature;small scale pupulation detection","","7","","35","IEEE","5 Mar 2020","","","IEEE","IEEE Conferences"
"DnD: Dense Depth Estimation in Crowded Dynamic Indoor Scenes","D. Jung; J. Choi; Y. Lee; D. Kim; C. Kim; D. Manocha; D. Lee",NAVER LABS; NAVER LABS; NAVER LABS; NAVER LABS; KAIST; University of Maryland; NAVER LABS,"2021 IEEE/CVF International Conference on Computer Vision (ICCV)","28 Feb 2022","2021","","","12777","12787","We present a novel approach for estimating depth from a monocular camera as it moves through complex and crowded indoor environments, e.g., a department store or a metro station. Our approach predicts absolute scale depth maps over the entire scene consisting of a static background and multiple moving people, by training on dynamic scenes. Since it is difficult to collect dense depth maps from crowded indoor environments, we design our training framework without requiring depths produced from depth sensing devices. Our network leverages RGB images and sparse depth maps generated from traditional 3D reconstruction methods to estimate dense depth maps. We use two constraints to handle depth for non-rigidly moving people without tracking their motion explicitly. We demonstrate that our approach offers consistent improvements over recent depth estimation methods on the NAVERLABS dataset, which includes complex and crowded scenes.","2380-7504","978-1-6654-2812-5","10.1109/ICCV48922.2021.01256","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9709913","3D from a single image and shape-from-x;Vision for robotics and autonomous vehicles","Training;Computer vision;Three-dimensional displays;Tracking;Dynamics;Estimation;Cameras","","","","2","","69","IEEE","28 Feb 2022","","","IEEE","IEEE Conferences"
"Adaptive Density Map Generation for Crowd Counting","J. Wan; A. Chan","Department of Computer Science, City University of Hong kong; Department of Computer Science, City University of Hong kong","2019 IEEE/CVF International Conference on Computer Vision (ICCV)","27 Feb 2020","2019","","","1130","1139","Crowd counting is an important topic in computer vision due to its practical usage in surveillance systems. The typical design of crowd counting algorithms is divided into two steps. First, the ground-truth density maps of crowd images are generated from the ground-truth dot maps (density map generation), e.g., by convolving with a Gaussian kernel. Second, deep learning models are designed to predict a density map from an input image (density map estimation). Most research efforts have concentrated on the density map estimation problem, while the problem of density map generation has not been adequately explored. In particular, the density map could be considered as an intermediate representation used to train a crowd counting network. In the sense of end-to-end training, the hand-crafted methods used for generating the density maps may not be optimal for the particular network or dataset used. To address this issue, we first show the impact of different density maps and that better ground-truth density maps can be obtained by refining the existing ones using a learned refinement network, which is jointly trained with the counter. Then, we propose an adaptive density map generator, which takes the annotation dot map as input, and learns a density map representation for a counter. The counter and generator are trained jointly within an end-to-end framework. The experiment results on popular counting datasets confirm the effectiveness of the proposed learnable density map representations.","2380-7504","978-1-7281-4803-8","10.1109/ICCV.2019.00122","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9009065","","Kernel;Estimation;Bandwidth;Prediction algorithms;Training;Generators;Feature extraction","computer vision;estimation theory;feature extraction;learning (artificial intelligence);neural nets","learned refinement network;deep learning models;ground-truth dot maps;surveillance systems;computer vision;adaptive density map generation;learnable density map representations;ground-truth density maps;crowd counting network;density map estimation problem;crowd images","","87","","42","IEEE","27 Feb 2020","","","IEEE","IEEE Conferences"
"Crowd Counting With Deep Structured Scale Integration Network","L. Liu; Z. Qiu; G. Li; S. Liu; W. Ouyang; L. Lin","Sun Yat-sen University; Sun Yat-sen University; Sun Yat-sen University; SenseTime Computer Vision Research Group, The University of Sydney, Australia; SenseTime Computer Vision Research Group, The University of Sydney, Australia; Sun Yat-sen University","2019 IEEE/CVF International Conference on Computer Vision (ICCV)","27 Feb 2020","2019","","","1774","1783","Automatic estimation of the number of people in unconstrained crowded scenes is a challenging task and one major difficulty stems from the huge scale variation of people. In this paper, we propose a novel Deep Structured Scale Integration Network (DSSINet) for crowd counting, which addresses the scale variation of people by using structured feature representation learning and hierarchically structured loss function optimization. Unlike conventional methods which directly fuse multiple features with weighted average or concatenation, we first introduce a Structured Feature Enhancement Module based on conditional random fields (CRFs) to refine multiscale features mutually with a message passing mechanism. Specifically, each scale-specific feature is considered as a continuous random variable and passes complementary information to refine the features at other scales. Second, we utilize a Dilated Multiscale Structural Similarity loss to enforce our DSSINet to learn the local correlation of people's scales within regions of various size, thus yielding high-quality density maps. Extensive experiments on four challenging benchmarks well demonstrate the effectiveness of our method. In particular, our DSSINet achieves improvements of 9.5% error reduction on Shanghaitech dataset and 24.9% on UCF-QNRF dataset against the state-of-the-art methods.","2380-7504","978-1-7281-4803-8","10.1109/ICCV.2019.00186","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9010246","","Correlation;Task analysis;Robustness;Optimization;Fuses;Message passing;Head","feature extraction;image enhancement;image fusion;image representation;learning (artificial intelligence);message passing;optimisation;random processes","high-quality density maps;continuous random variable;concatenation;dilated multiscale structural similarity loss;scale-specific feature;message passing mechanism;multiscale features;conditional random fields;structured feature enhancement module;weighted average;hierarchically structured loss function optimization;structured feature representation learning;DSSINet;deep structured scale integration network;unconstrained crowded scenes;automatic estimation;crowd counting","","148","","49","IEEE","27 Feb 2020","","","IEEE","IEEE Conferences"
"EM-Fusion: Dynamic Object-Level SLAM With Probabilistic Data Association","M. Strecke; J. Stueckler","Embodied Vision Group Max Planck Institute for Intelligent Systems; Embodied Vision Group, Max Planck Institute for Intelligent Systems, Stuttgart, Germany","2019 IEEE/CVF International Conference on Computer Vision (ICCV)","27 Feb 2020","2019","","","5864","5873","The majority of approaches for acquiring dense 3D environment maps with RGB-D cameras assumes static environments or rejects moving objects as outliers. The representation and tracking of moving objects, however, has significant potential for applications in robotics or augmented reality. In this paper, we propose a novel approach to dynamic SLAM with dense object-level representations. We represent rigid objects in local volumetric signed distance function (SDF) maps, and formulate multi-object tracking as direct alignment of RGB-D images with the SDF representations. Our main novelty is a probabilistic formulation which naturally leads to strategies for data association and occlusion handling. We analyze our approach in experiments and demonstrate that our approach compares favorably with the state-of-the-art methods in terms of robustness and accuracy.","2380-7504","978-1-7281-4803-8","10.1109/ICCV.2019.00596","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9010932","","Simultaneous localization and mapping;Cameras;Image segmentation;Probabilistic logic;Three-dimensional displays;Clocks;Tracking","augmented reality;image fusion;image motion analysis;image segmentation;image sequences;mobile robots;object detection;object tracking;path planning;probability;robot vision;SLAM (robots);target tracking;video signal processing","augmented reality;dynamic SLAM;dense object-level representations;rigid objects;local volumetric signed distance function maps;multiobject tracking;RGB-D images;SDF representations;probabilistic formulation;occlusion handling;dynamic object-level SLAM;probabilistic data association;dense 3D environment maps;RGB-D cameras;static environments;moving objects","","36","","27","IEEE","27 Feb 2020","","","IEEE","IEEE Conferences"
"Social NCE: Contrastive Learning of Socially-aware Motion Representations","Y. Liu; Q. Yan; A. Alahi",École Polytechnique Fédérale de Lausanne (EPFL); École Polytechnique Fédérale de Lausanne (EPFL); École Polytechnique Fédérale de Lausanne (EPFL),"2021 IEEE/CVF International Conference on Computer Vision (ICCV)","28 Feb 2022","2021","","","15098","15109","Learning socially-aware motion representations is at the core of recent advances in multi-agent problems, such as human motion forecasting and robot navigation in crowds. Despite promising progress, existing representations learned with neural networks still struggle to generalize in closed-loop predictions (e.g., output colliding trajectories). This issue largely arises from the non-i.i.d. nature of sequential prediction in conjunction with ill-distributed training data. Intuitively, if the training data only comes from human behaviors in safe spaces, i.e., from ""positive"" examples, it is difficult for learning algorithms to capture the notion of ""negative"" examples like collisions. In this work, we aim to address this issue by explicitly modeling negative examples through self-supervision: (i) we intro-duce a social contrastive loss that regularizes the extracted motion representation by discerning the ground-truth positive events from synthetic negative ones; (ii) we construct informative negative samples based on our prior knowledge of rare but dangerous circumstances. Our method substantially reduces the collision rates of recent trajectory forecasting, behavioral cloning and reinforcement learning algorithms, outperforming state-of-the-art methods on several benchmarks. Our code is available at https://github.com/vita-epfl/social-nce.","2380-7504","978-1-6654-2812-5","10.1109/ICCV48922.2021.01484","National Science Foundation; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9711444","Vision for robotics and autonomous vehicles;Transfer/Low-shot/Semi/Unsupervised Learning","Computer vision;Codes;Navigation;Neural networks;Training data;Reinforcement learning;Data collection","","","","27","","101","IEEE","28 Feb 2022","","","IEEE","IEEE Conferences"
"Learn to Scale: Generating Multipolar Normalized Density Maps for Crowd Counting","C. Xu; K. Qiu; J. Fu; S. Bai; Y. Xu; X. Bai",Huazhong University of Science and Technology; Microsoft Research Asia; Microsoft Research Asia; University of Oxford; Huazhong University of Science and Technology; Huazhong University of Science and Technology,"2019 IEEE/CVF International Conference on Computer Vision (ICCV)","27 Feb 2020","2019","","","8381","8389","Dense crowd counting aims to predict thousands of human instances from an image, by calculating integrals of a density map over image pixels. Existing approaches mainly suffer from the extreme density variations. Such density pattern shift poses challenges even for multi-scale model ensembling. In this paper, we propose a simple yet effective approach to tackle this problem. First, a patch-level density map is extracted by a density estimation model and further grouped into several density levels which are determined over full datasets. Second, each patch density map is automatically normalized by an online center learning strategy with a multipolar center loss. Such a design can significantly condense the density distribution into several clusters, and enable that the density variance can be learned by a single model. Extensive experiments demonstrate the superiority of the proposed method. Our work outperforms the state-of-the-art by 4.2%, 14.3%, 27.1% and 20.1% in MAE, on the ShanghaiTech Part A, ShanghaiTech Part B, UCF_CC_50 and UCF-QNRF datasets, respectively.","2380-7504","978-1-7281-4803-8","10.1109/ICCV.2019.00847","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9009068","","Head;Feature extraction;Estimation;Kernel;Robustness;Training;Asia","image motion analysis;image resolution;learning (artificial intelligence)","multipolar normalized density maps;crowd counting;dense crowd;human instances;image pixels;extreme density variations;density pattern shift;multiscale model ensembling;patch-level density map;density estimation model;density levels;patch density map;online center learning strategy;multipolar center loss;density distribution;density variance","","63","","33","IEEE","27 Feb 2020","","","IEEE","IEEE Conferences"
"Multi-Agent Reinforcement Learning Based Frame Sampling for Effective Untrimmed Video Recognition","W. Wu; D. He; X. Tan; S. Chen; S. Wen","Shenzhen Institutes of Advanced Technology, Shenzhen, China; Department of Computer Vision Technology, Baidu Inc., Beijing, China; Department of Computer Vision Technology, Baidu Inc., Beijing, China; Shenzhen Institutes of Advanced Technology, Shenzhen, China; Department of Computer Vision Technology, Baidu Inc., Beijing, China","2019 IEEE/CVF International Conference on Computer Vision (ICCV)","27 Feb 2020","2019","","","6221","6230","Video Recognition has drawn great research interest and great progress has been made. A suitable frame sampling strategy can improve the accuracy and efficiency of recognition. However, mainstream solutions generally adopt hand-crafted frame sampling strategies for recognition. It could degrade the performance, especially in untrimmed videos, due to the variation of frame-level saliency. To this end, we concentrate on improving untrimmed video classification via developing a learning-based frame sampling strategy. We intuitively formulate the frame sampling procedure as multiple parallel Markov decision processes, each of which aims at picking out a frame/clip by gradually adjusting an initial sampling. Then we propose to solve the problems with multi-agent reinforcement learning (MARL). Our MARL framework is composed of a novel RNN-based context-aware observation network which jointly models context information among nearby agents and historical states of a specific agent, a policy network which generates the probability distribution over a predefined action space at each step and a classification network for reward calculation as well as final recognition. Extensive experimental results show that our MARL-based scheme remarkably outperforms hand-crafted strategies with various 2D and 3D baseline methods. Our single RGB model achieves a comparable performance of ActivityNet v1.3 champion submission with multi-modal multi-model fusion and new state-of-the-art results on YouTube Birds and YouTube Cars.","2380-7504","978-1-7281-4803-8","10.1109/ICCV.2019.00632","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9009461","","Learning (artificial intelligence);YouTube;Markov processes;Two dimensional displays;Three-dimensional displays;Decision making;Computer vision","computer vision;image classification;image motion analysis;learning (artificial intelligence);Markov processes;multi-agent systems;probability;recurrent neural nets;statistical distributions;video signal processing","multimodal multimodel fusion;multiagent reinforcement learning;frame sampling strategy;mainstream solutions;hand-crafted frame sampling strategies;untrimmed videos;frame-level saliency;untrimmed video classification;learning-based frame sampling strategy;frame sampling procedure;multiple parallel Markov decision processes;initial sampling;nearby agents;classification network;MARL-based scheme;hand-crafted strategies;context information;RNN-based context-aware observation network;untrimmed video recognition;probability distribution","","58","","56","IEEE","27 Feb 2020","","","IEEE","IEEE Conferences"
"FullFusion: A Framework for Semantic Reconstruction of Dynamic Scenes","M. Bujanca; M. Lujan; B. Lennox","The University of Manchester, Manchester, UK; University of Manchester, Manchester, UK; The University of Manchester, Manchester, UK","2019 IEEE/CVF International Conference on Computer Vision Workshop (ICCVW)","5 Mar 2020","2019","","","2168","2177","Assuming that scenes are static is common in SLAM research. However, the world is complex, dynamic, and features interactive agents. Mobile robots operating in a variety of environments in real-life scenarios require an advanced level of understanding of their surroundings. Therefore, it is crucial to find effective ways of representing the world in its dynamic complexity, beyond the geometry of static scene elements. We present a framework that enables incremental reconstruction of semantically-annotated 3D models in dynamic settings using commodity RGB-D sensors. Our method is the first to perform semantic reconstruction of non-rigidly deforming objects along with a static background. FullFusion is a step towards enabling robots to have a deeper and richer understanding of their surroundings, and can facilitate the study of interaction and scene dynamics. To showcase the potential of FullFusion, we provide a quantitative and qualitative evaluation on a baseline implementation which employs specific reconstruction and segmentation pipelines. It is, however, important to highlight that the modular design of the framework allows us to easily replace any of the components with new or existing counterparts.","2473-9944","978-1-7281-5023-9","10.1109/ICCVW.2019.00272","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9022128","3D reconstruction;SLAM;dynamic SLAM","Semantics;Three-dimensional displays;Cameras;Image segmentation;Simultaneous localization and mapping;Image reconstruction;Pose estimation","image colour analysis;image motion analysis;image reconstruction;image segmentation;mobile robots;robot vision;SLAM (robots)","real-life scenarios;dynamic complexity;static scene elements;incremental reconstruction;commodity RGB-D sensors;semantic reconstruction;static background;FullFusion;scene dynamics;specific reconstruction;segmentation pipelines;dynamic scenes;SLAM research;mobile robots","","4","","48","IEEE","5 Mar 2020","","","IEEE","IEEE Conferences"
"The Trajectron: Probabilistic Multi-Agent Trajectory Modeling With Dynamic Spatiotemporal Graphs","B. Ivanovic; M. Pavone",Stanford University; Stanford University,"2019 IEEE/CVF International Conference on Computer Vision (ICCV)","27 Feb 2020","2019","","","2375","2384","Developing safe human-robot interaction systems is a necessary step towards the widespread integration of autonomous agents in society. A key component of such systems is the ability to reason about the many potential futures (e.g. trajectories) of other agents in the scene. Towards this end, we present the Trajectron, a graph-structured model that predicts many potential future trajectories of multiple agents simultaneously in both highly dynamic and multimodal scenarios (i.e. where the number of agents in the scene is time-varying and there are many possible highly-distinct futures for each agent). It combines tools from recurrent sequence modeling and variational deep generative modeling to produce a distribution of future trajectories for each agent in a scene. We demonstrate the performance of our model on several datasets, obtaining state-of-the-art results on standard trajectory prediction metrics as well as introducing a new metric for comparing models that output distributions.","2380-7504","978-1-7281-4803-8","10.1109/ICCV.2019.00246","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9009454","","Trajectory;Predictive models;Forecasting;Graphical models;Spatiotemporal phenomena;Standards;Autonomous systems","control engineering computing;graph theory;human-robot interaction;inference mechanisms;multi-agent systems;probability","autonomous agents;graph-structured model;highly dynamic scenarios;multimodal scenarios;recurrent sequence modeling;variational deep generative modeling;probabilistic multiagent trajectory modeling;dynamic spatiotemporal graphs;human-robot interaction systems;trajectory prediction metrics","","202","","51","IEEE","27 Feb 2020","","","IEEE","IEEE Conferences"
"Learning Spatial Awareness to Improve Crowd Counting","Z. -Q. Cheng; J. -X. Li; Q. Dai; X. Wu; A. Hauptmann",Southwest Jiaotong University; Southwest Jiaotong University; Microsoft Research; Southwest Jiaotong University; Carnegie Mellon University,"2019 IEEE/CVF International Conference on Computer Vision (ICCV)","27 Feb 2020","2019","","","6151","6160","The aim of crowd counting is to estimate the number of people in images by leveraging the annotation of center positions for pedestrians' heads. Promising progresses have been made with the prevalence of deep Convolutional Neural Networks. Existing methods widely employ the Euclidean distance (i.e., $L_2$ loss) to optimize the model, which, however, has two main drawbacks: (1) the loss has difficulty in learning the spatial awareness (i.e., the position of head) since it struggles to retain the high-frequency variation in the density map, and (2) the loss is highly sensitive to various noises in crowd counting, such as the zero-mean noise, head size changes, and occlusions. Although the Maximum Excess over SubArrays (MESA) loss has been previously proposed by~\cite{nips-10} to address the above issues by finding the rectangular subregion whose predicted density map has the maximum difference from the ground truth, it cannot be solved by gradient descent, thus can hardly be integrated into the deep learning framework. In this paper, we present a novel architecture called SPatial Awareness Network (SPANet) to incorporate spatial context for crowd counting. The Maximum Excess over Pixels (MEP) loss is proposed to achieve this by finding the pixel-level subregion with high discrepancy to the ground truth. To this end, we devise a weakly supervised learning scheme to generate such region with a multi-branch architecture. The proposed framework can be integrated into existing deep crowd counting methods and is end-to-end trainable. Extensive experiments on four challenging benchmarks show that our method can significantly improve the performance of baselines. More remarkably, our approach outperforms the state-of-the-art methods on all benchmark datasets.","2380-7504","978-1-7281-4803-8","10.1109/ICCV.2019.00625","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9010678","","Head;Feature extraction;Estimation;Optimization;Task analysis;Training;Benchmark testing","convolutional neural nets;gradient methods;learning (artificial intelligence)","head size changes;maximum excess over pixels;SubArrays loss;density map;ground truth;deep learning framework;spatial awareness network;spatial context;Pixels loss;weakly supervised learning scheme;deep crowd counting methods;center positions;pedestrians;deep convolutional neural networks;high-frequency variation","","77","","54","IEEE","27 Feb 2020","","","IEEE","IEEE Conferences"
"Env-QA: A Video Question Answering Benchmark for Comprehensive Understanding of Dynamic Environments","D. Gao; R. Wang; Z. Bai; X. Chen","Key Laboratory of Intelligent Information Processing of Chinese Academy of Sciences (CAS), Institute of Computing Technology, CAS, Beijing, China; Key Laboratory of Intelligent Information Processing of Chinese Academy of Sciences (CAS), Institute of Computing Technology, CAS, Beijing, China; Key Laboratory of Intelligent Information Processing of Chinese Academy of Sciences (CAS), Institute of Computing Technology, CAS, Beijing, China; Key Laboratory of Intelligent Information Processing of Chinese Academy of Sciences (CAS), Institute of Computing Technology, CAS, Beijing, China","2021 IEEE/CVF International Conference on Computer Vision (ICCV)","28 Feb 2022","2021","","","1655","1665","Visual understanding goes well beyond the study of images or videos on the web. To achieve complex tasks in volatile situations, the human can deeply understand the environment, quickly perceive events happening around, and continuously track objects’ state changes, which are still challenging for current AI systems. To equip AI system with the ability to understand dynamic ENVironments, we build a video Question Answering dataset named Env-QA. Env-QA contains 23K egocentric videos, where each video is composed of a series of events about exploring and interacting in the environment. It also provides 85K questions to evaluate the ability of understanding the composition, layout, and state changes of the environment presented by the events in videos. Moreover, we propose a video QA model, Temporal Segmentation and Event Attention network (TSEA), which introduces event-level video representation and corresponding attention mechanisms to better extract environment information and answer questions. Comprehensive experiments demonstrate the effectiveness of our framework and show the formidable challenges of Env-QA in terms of long-term state tracking, multi-event temporal reasoning and event counting, etc.","2380-7504","978-1-6654-2812-5","10.1109/ICCV48922.2021.00170","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9711383","Vision + language;Video analysis and understanding;Visual reasoning and logical representation","Visualization;Three-dimensional displays;Layout;Feature extraction;Transformers;Cognition;Data mining","","","","3","","61","IEEE","28 Feb 2022","","","IEEE","IEEE Conferences"
"Multi-Level Bottom-Top and Top-Bottom Feature Fusion for Crowd Counting","V. Sindagi; V. Patel","Department of Electrical and Computer Engineering, Johns Hopkins University, Baltimore, MD, USA; Department of Electrical and Computer Engineering, Johns Hopkins University, Baltimore, MD, USA","2019 IEEE/CVF International Conference on Computer Vision (ICCV)","27 Feb 2020","2019","","","1002","1012","Crowd counting presents enormous challenges in the form of large variation in scales within images and across the dataset. These issues are further exacerbated in highly congested scenes. Approaches based on straightforward fusion of multi-scale features from a deep network seem to be obvious solutions to this problem. However, these fusion approaches do not yield significant improvements in the case of crowd counting in congested scenes. This is usually due to their limited abilities in effectively combining the multi-scale features for problems like crowd counting. To overcome this, we focus on how to efficiently leverage information present in different layers of the network. Specifically, we present a network that involves: (i) a multi-level bottom-top and top-bottom fusion (MBTTBF) method to combine information from shallower to deeper layers and vice versa at multiple levels, (ii) scale complementary feature extraction blocks (SCFB) involving cross-scale residual functions to explicitly enable flow of complementary features from adjacent conv layers along the fusion paths. Furthermore, in order to increase the effectiveness of the multi-scale fusion, we employ a principled way of generating scale-aware ground-truth density maps for training. Experiments conducted on three datasets that contain highly congested scenes (ShanghaiTech, UCF_CC_50, and UCF-QNRF) demonstrate that the proposed method is able to outperform several recent methods in all the datasets.","2380-7504","978-1-7281-4803-8","10.1109/ICCV.2019.00109","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9009788","","Feature extraction;Fuses;Head;Semantics;Task analysis;Flyback transformers;Training","feature extraction;image classification;image fusion","multiscale fusion;scale-aware ground-truth density maps;highly congested scenes;straightforward fusion;multiscale features;deep network;multilevel bottom-top and top-bottom feature fusion;crowd counting;scale complementary feature extraction blocks;MBTTBF;SCFB","","108","","78","IEEE","27 Feb 2020","","","IEEE","IEEE Conferences"
"Relational Attention Network for Crowd Counting","A. Zhang; J. Shen; Z. Xiao; F. Zhu; X. Zhen; X. Cao; L. Shao","School of Electronic and Information Engineering, Beihang University, Beijing, China; School of Electronic and Information Engineering, Beihang University, Beijing, China; School of Electronic and Information Engineering, Beihang University, Beijing, China; Inception Institute of Artificial Intelligence, Abu Dhabi, UAE; Inception Institute of Artificial Intelligence, Abu Dhabi, UAE; School of Electronic and Information Engineering, Beihang University, Beijing, China; Inception Institute of Artificial Intelligence, Abu Dhabi, UAE","2019 IEEE/CVF International Conference on Computer Vision (ICCV)","27 Feb 2020","2019","","","6787","6796","Crowd counting is receiving rapidly growing research interests due to its potential application value in numerous real-world scenarios. However, due to various challenges such as occlusion, insufficient resolution and dynamic backgrounds, crowd counting remains an unsolved problem in computer vision. Density estimation is a popular strategy for crowd counting, where conventional density estimation methods perform pixel-wise regression without explicitly accounting the interdependence of pixels. As a result, independent pixel-wise predictions can be noisy and inconsistent. In order to address such an issue, we propose a Relational Attention Network (RANet) with a self-attention mechanism for capturing interdependence of pixels. The RANet enhances the self-attention mechanism by accounting both short-range and long-range interdependence of pixels, where we respectively denote these implementations as local self-attention (LSA) and global self-attention (GSA). We further introduce a relation module to fuse LSA and GSA to achieve more informative aggregated feature representations. We conduct extensive experiments on four public datasets, including ShanghaiTech A, ShanghaiTech B, UCF-CC-50 and UCF-QNRF. Experimental results on all datasets suggest RANet consistently reduces estimation errors and surpasses the state-of-the-art approaches by large margins.","2380-7504","978-1-7281-4803-8","10.1109/ICCV.2019.00689","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9010829","","Feature extraction;Computational modeling;Task analysis;Estimation;Fuses;Correlation;Image reconstruction","computer vision;image fusion;image representation;learning (artificial intelligence);neural nets;object detection;pose estimation;regression analysis","RANet;self-attention mechanism;global self-attention;crowd counting;density estimation;pixel-wise regression;independent pixel-wise prediction;relational attention network;local self-attention;ShanghaiTech A dataset;ShanghaiTech B dataset;UCF-CC-50 dataset;UCF-QNRF dataset;computer vision","","91","","46","IEEE","27 Feb 2020","","","IEEE","IEEE Conferences"
"LPD-Net: 3D Point Cloud Learning for Large-Scale Place Recognition and Environment Analysis","Z. Liu; S. Zhou; C. Suo; P. Yin; W. Chen; H. Wang; H. Li; Y. Liu",The Chinese University of Hong Kong; Shanghai Jiao Tong University; The Chinese University of Hong Kong; Carnegie Mellon University; The Chinese University of Hong Kong; Shanghai Jiao Tong University; The Chinese University of Hong Kong; CUHK,"2019 IEEE/CVF International Conference on Computer Vision (ICCV)","27 Feb 2020","2019","","","2831","2840","Point cloud based place recognition is still an open issue due to the difficulty in extracting local features from the raw 3D point cloud and generating the global descriptor, and it's even harder in the large-scale dynamic environments. In this paper, we develop a novel deep neural network, named LPD-Net (Large-scale Place Description Network), which can extract discriminative and generalizable global descriptors from the raw 3D point cloud. Two modules, the adaptive local feature extraction module and the graph-based neighborhood aggregation module, are proposed, which contribute to extract the local structures and reveal the spatial distribution of local features in the large-scale point cloud, with an end-to-end manner. We implement the proposed global descriptor in solving point cloud based retrieval tasks to achieve the large-scale place recognition. Comparison results show that our LPD-Net is much better than PointNetVLAD and reaches the state-of-the-art. We also compare our LPD-Net with the vision-based solutions to show the robustness of our approach to different weather and light conditions.","2380-7504","978-1-7281-4803-8","10.1109/ICCV.2019.00292","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9009029","","Three-dimensional displays;Feature extraction;Task analysis;Two dimensional displays;Graphical models;Distribution functions;Robustness","feature extraction;graph theory;neural nets;object recognition","discriminative descriptors;adaptive local feature extraction module;graph-based neighborhood aggregation module;global descriptor;point cloud based place recognition;large-scale dynamic environments;LPD-Net;large-scale place description network;PointNetVLAD;spatial distribution;environment analysis","","127","","28","IEEE","27 Feb 2020","","","IEEE","IEEE Conferences"
"Attentional Neural Fields for Crowd Counting","A. Zhang; L. Yue; J. Shen; F. Zhu; X. Zhen; X. Cao; L. Shao","School of Electronic and Information Engineering, Beihang University, Beijing, China; School of Electronic and Information Engineering, Beihang University, Beijing, China; School of Electronic and Information Engineering, Beihang University, Beijing, China; Inception Institute of Artificial Intelligence, Abu Dhabi, UAE; Inception Institute of Artificial Intelligence, Abu Dhabi, UAE; School of Electronic and Information Engineering, Beihang University, Beijing, China; Inception Institute of Artificial Intelligence, Abu Dhabi, UAE","2019 IEEE/CVF International Conference on Computer Vision (ICCV)","27 Feb 2020","2019","","","5713","5722","Crowd counting has recently generated huge popularity in computer vision, and is extremely challenging due to the huge scale variations of objects. In this paper, we propose the Attentional Neural Field (ANF) for crowd counting via density estimation. Within the encoder-decoder network, we introduce conditional random fields (CRFs) to aggregate multi-scale features, which can build more informative representations. To better model pair-wise potentials in CRFs, we incorperate non-local attention mechanism implemented as inter- and intra-layer attentions to expand the receptive field to the entire image respectively within the same layer and across different layers, which captures long-range dependencies to conquer huge scale variations. The CRFs coupled with the attention mechanism are seamlessly integrated into the encoder-decoder network, establishing an ANF that can be optimized end-to-end by back propagation. We conduct extensive experiments on four public datasets, including ShanghaiTech, WorldEXPO 10, UCF-CC-50 and UCF-QNRF. The results show that our ANF achieves high counting performance, surpassing most previous methods.","2380-7504","978-1-7281-4803-8","10.1109/ICCV.2019.00581","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9009565","","Estimation;Task analysis;Feature extraction;Computational modeling;Adaptation models;Aggregates;Machine learning","computer vision;feature extraction;image representation;object detection;random processes","nonlocal attention mechanism;intra-layer attentions;CRF;encoder-decoder network;ANF;crowd counting;computer vision;conditional random fields;model pair-wise potentials;attentional neural fields","","65","","43","IEEE","27 Feb 2020","","","IEEE","IEEE Conferences"
"LaLaLoc: Latent Layout Localisation in Dynamic, Unvisited Environments","H. Howard-Jenkins; J. -R. Ruiz-Sarmiento; V. A. Prisacariu","Active Vision Laboratory, University of Oxford; Machine Perception and Intelligent Robotics Group, University of Málaga; Active Vision Laboratory, University of Oxford","2021 IEEE/CVF International Conference on Computer Vision (ICCV)","28 Feb 2022","2021","","","10087","10096","We present LaLaLoc to localise in environments without the need for prior visitation, and in a manner that is robust to large changes in scene appearance, such as a full rearrangement of furniture. Specifically, LaLaLoc performs localisation through latent representations of room layout. LaLaLoc learns a rich embedding space shared between RGB panoramas and layouts inferred from a known floor plan that encodes the structural similarity between locations. Further, LaLaLoc introduces direct, cross-modal pose optimisation in its latent space. Thus, LaLaLoc enables fine-grained pose estimation in a scene without the need for prior visitation, as well as being robust to dynamics, such as a change in furniture configuration. We show that in a domestic environment LaLaLoc is able to accurately localise a single RGB panorama image to within 8.3cm, given only a floor plan as a prior.","2380-7504","978-1-6654-2812-5","10.1109/ICCV48922.2021.00995","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9710879","Representation learning;Scene analysis and understanding","Computer vision;Layout;Pose estimation;Rendering (computer graphics);Floors;Optimization","","","","5","","42","IEEE","28 Feb 2022","","","IEEE","IEEE Conferences"
"Conditional Vehicle Trajectories Prediction in CARLA Urban Environment","T. Buhet; E. Wirbel; X. Perrotton","Valeo Driving Assistance Research, Bobigny; Valeo Driving Assistance Research, Bobigny; Valeo Driving Assistance Research, Bobigny","2019 IEEE/CVF International Conference on Computer Vision Workshop (ICCVW)","5 Mar 2020","2019","","","2310","2319","Imitation learning is becoming more and more successful for autonomous driving. End-to-end (raw signal to command) performs well on relatively simple tasks (lane keeping and navigation). Mid-to-mid (environment abstraction to mid-level trajectory representation) or direct perception (raw signal to performance) approaches strive to handle more complex, real life environment and tasks (e.g. complex intersection). In this work, we show that complex urban situations can be handled with raw signal input and mid-level representation. We build a hybrid end-to-mid approach predicting trajectories for neighbor vehicles and for the ego vehicle with a conditional navigation goal. We propose an original architecture inspired from social pooling LSTM taking low and mid level data as input and producing trajectories as polynomials of time. We introduce a label augmentation mechanism to get the level of generalization that is required to control a vehicle. The performance is evaluated on CARLA 0.8 benchmark, showing significant improvements over previously published state of the art.","2473-9944","978-1-7281-5023-9","10.1109/ICCVW.2019.00284","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9022290","imitation learning;CARLA;autonomous driving;trajectory prediction","Trajectory;Navigation;Task analysis;Autonomous vehicles;Cameras;Three-dimensional displays;Benchmark testing","computer vision;driver information systems;feature extraction;image colour analysis;intelligent transportation systems;learning (artificial intelligence);mobile robots;recurrent neural nets;road vehicles","conditional vehicle trajectories prediction;CARLA urban environment;end-to-end imitation learning;autonomous driving;trajectory representation;neighbor vehicles;ego vehicle;conditional navigation goal;hybrid end-to-mid approach;social pooling LSTM;label augmentation;CARLA autopilot;front facing camera RGB image;image detector;YOLO detector","","7","","36","IEEE","5 Mar 2020","","","IEEE","IEEE Conferences"
"Instance Segmentation in CARLA: Methodology and Analysis for Pedestrian-oriented Synthetic Data Generation in Crowded Scenes","M. Lyssenko; C. Gladisch; C. Heinzemann; M. Woehrle; R. Triebel","Robert Bosch GmbH, Corporate Research; Robert Bosch GmbH, Corporate Research; Robert Bosch GmbH, Corporate Research; Robert Bosch GmbH, Corporate Research; Robert Bosch GmbH, Corporate Research","2021 IEEE/CVF International Conference on Computer Vision Workshops (ICCVW)","24 Nov 2021","2021","","","988","996","The evaluation of camera-based perception functions in automated driving (AD) is a significant challenge and requires large-scale high-quality datasets. Recently proposed metrics for safety evaluation additionally require detailed per-instance annotations of dynamic properties such as distance and velocities that may not be available in openly accessible AD datasets. Synthetic data from 3D simulators like CARLA may provide a solution to this problem as labeled data can be produced in a structured manner. However, CARLA currently lacks instance segmentation ground truth. In this paper, we present a back projection pipeline that allows us to obtain accurate instance segmentation maps for CARLA, which is necessary for precise per-instance ground truth information. Our evaluation results show that per-pedestrian depth aggregation obtained from our instance segmentation is more precise than previously available approximations based on bounding boxes especially in the context of crowded scenes in urban automated driving.","2473-9944","978-1-6654-0191-3","10.1109/ICCVW54120.2021.00115","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9607548","","Measurement;Computer vision;Three-dimensional displays;Annotations;Conferences;Pipelines;Safety","cameras;image segmentation;object detection","CARLA;pedestrian-oriented synthetic data generation;crowded scenes;camera-based perception;large-scale high-quality datasets;recently proposed metrics;safety evaluation;per-instance annotations;openly accessible AD datasets;accurate instance segmentation maps;per-instance ground truth information;per-pedestrian depth aggregation;urban automated driving","","3","","26","IEEE","24 Nov 2021","","","IEEE","IEEE Conferences"
"MeteorNet: Deep Learning on Dynamic 3D Point Cloud Sequences","X. Liu; M. Yan; J. Bohg",Stanford University; Stanford University; Stanford University,"2019 IEEE/CVF International Conference on Computer Vision (ICCV)","27 Feb 2020","2019","","","9245","9254","Understanding dynamic 3D environment is crucial for robotic agents and many other applications. We propose a novel neural network architecture called MeteorNet for learning representations for dynamic 3D point cloud sequences. Different from previous work that adopts a grid-based representation and applies 3D or 4D convolutions, our network directly processes point clouds. We propose two ways to construct spatiotemporal neighborhoods for each point in the point cloud sequence. Information from these neighborhoods is aggregated to learn features per point. We benchmark our network on a variety of 3D recognition tasks including action recognition, semantic segmentation and scene flow estimation. MeteorNet shows stronger performance than previous grid-based methods while achieving state-of-the-art performance on Synthia. MeteorNet also outperforms previous baseline methods that are able to process at most two consecutive point clouds. To the best of our knowledge, this is the first work on deep learning for dynamic raw point cloud sequences.","2380-7504","978-1-7281-4803-8","10.1109/ICCV.2019.00934","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9010250","","Three-dimensional displays;Machine learning;Neural networks;Estimation;Spatiotemporal phenomena;Feature extraction;Task analysis","feature extraction;image representation;image segmentation;image sequences;learning (artificial intelligence);neural net architecture;object recognition;solid modelling","MeteorNet;deep learning;robotic agents;spatiotemporal neighborhoods;semantic segmentation;scene flow estimation;consecutive point clouds;dynamic raw point cloud sequences;3D recognition tasks;action recognition;dynamic 3D point cloud sequences;neural network architecture","","87","","38","IEEE","27 Feb 2020","","","IEEE","IEEE Conferences"
"Spatial Uncertainty-Aware Semi-Supervised Crowd Counting","Y. Meng; H. Zhang; Y. Zhao; X. Yang; X. Qian; X. Huang; Y. Zheng","Department of Eye and Vision Science, University of Liverpool, Liverpool, United Kingdom; Department of Eye and Vision Science, University of Liverpool, Liverpool, United Kingdom; Chinese Academy of Sciences, Cixi Institute of Biomedical Engineering, Ningbo, China; Remark AI UK Limited, London, United Kingdom; China Science IntelliCloud Technology Co., Ltd, Shanghai, China; Department of Computer Science, University of Liverpool, Liverpool, United Kingdom; Department of Eye and Vision Science, University of Liverpool, Liverpool, United Kingdom","2021 IEEE/CVF International Conference on Computer Vision (ICCV)","28 Feb 2022","2021","","","15529","15539","Semi-supervised approaches for crowd counting attract attention, as the fully supervised paradigm is expensive and laborious due to its request for a large number of images of dense crowd scenarios and their annotations. This paper proposes a spatial uncertainty-aware semi-supervised approach via regularized surrogate task (binary segmentation) for crowd counting problems. Different from existing semi-supervised learning-based crowd counting methods, to exploit the unlabeled data, our proposed spatial uncertainty-aware teacher-student framework focuses on high confident regions’ information while addressing the noisy supervision from the unlabeled data in an end-to-end manner. Specifically, we estimate the spatial uncertainty maps from the teacher model’s surrogate task to guide the feature learning of the main task (density regression) and the surrogate task of the student model at the same time. Besides, we introduce a simple yet effective differential transformation layer to enforce the inherent spatial consistency regularization between the main task and the surrogate task in the student model, which helps the surrogate task to yield more reliable predictions and generates high-quality uncertainty maps. Thus, our model can also address the task-level perturbation problems that occur spatial inconsistency between the primary and surrogate tasks in the student model. Experimental results on four challenging crowd counting datasets demonstrate that our method achieves superior performance to the state-of-the-art semi-supervised methods. Code is available at : https://github.com/smallmax00/SUA_crowd_counting","2380-7504","978-1-6654-2812-5","10.1109/ICCV48922.2021.01526","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9710789","Scene analysis and understanding","Representation learning;Image segmentation;Uncertainty;Annotations;Perturbation methods;Predictive models;Spatial databases","","","","39","","68","IEEE","28 Feb 2022","","","IEEE","IEEE Conferences"
"Exploiting sample correlation for crowd counting with multi-expert network","X. Liu; G. Li; Z. Han; W. Zhang; Y. Yang; Q. Huang; N. Sebe","University of Chinese Academy of Sciences, Beijing, China; University of Chinese Academy of Sciences, Beijing, China; University of Chinese Academy of Sciences, Beijing, China; Harbin Institute of Technology, Weihai, China; University of Chinese Academy of Sciences, Beijing, China; University of Chinese Academy of Sciences, Beijing, China; University of Trento, Trento, Italy","2021 IEEE/CVF International Conference on Computer Vision (ICCV)","28 Feb 2022","2021","","","3195","3204","Crowd counting is a difficult task because of the diversity of scenes. Most of the existing crowd counting methods adopt complex structures with massive backbones to enhance the generalization ability. Unfortunately, the performance of existing methods on large-scale data sets is not satisfactory. In order to handle various scenarios with less complex network, we explored how to efficiently use the multi-expert model for crowd counting tasks. We mainly focus on how to train more efficient expert networks and how to choose the most suitable expert. Specifically, we propose a task-driven similarity metric based on sample’s mutual enhancement, referred as co-fine-tune similarity, which can find a more efficient subset of data for training the expert network. Similar samples are considered as a cluster which is used to obtain parameters of an expert. Besides, to make better use of the proposed method, we design a simple network called FPN with Deconvolution Counting Network, which is a more suitable base model for the multi-expert counting network. Experimental results show that multiple experts FDC (MFDC) achieves the best performance on four public data sets, including the large scale NWPU-Crowd data set. Furthermore, the MFDC trained on an extensive dense crowd data set can generalize well on the other data sets without extra training or fine-tuning.1","2380-7504","978-1-6654-2812-5","10.1109/ICCV48922.2021.00320","National Natural Science Foundation of China; Youth Innovation Promotion Association; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9710485","Detection and localization in 2D and 3D;Efficient training and inference methods","Training;Measurement;Computer vision;Deconvolution;Correlation;Design methodology;Training data","","","","12","","33","IEEE","28 Feb 2022","","","IEEE","IEEE Conferences"
"Graph CNN for Moving Object Detection in Complex Environments from Unseen Videos","J. H. Giraldo; S. Javed; N. Werghi; T. Bouwmans",La Rochelle Université; Khalifa University of Science and Technology; Khalifa University of Science and Technology; La Rochelle Université,"2021 IEEE/CVF International Conference on Computer Vision Workshops (ICCVW)","24 Nov 2021","2021","","","225","233","Moving Object Detection (MOD) is a fundamental step for many computer vision applications. MOD becomes very challenging when a video sequence captured from a static or moving camera suffers from the challenges: camouflage, shadow, dynamic backgrounds, and lighting variations, to name a few. Deep learning methods have been successfully applied to address MOD with competitive performance. However, in order to handle the overfitting problem, deep learning methods require a large amount of labeled data which is a laborious task as exhaustive annotations are always not available. Moreover, some MOD deep learning methods show performance degradation in the presence of unseen video sequences because the testing and training splits of the same sequences are involved during the network learning process. In this work, we pose the problem of MOD as a node classification problem using Graph Convolutional Neural Networks (GCNNs). Our algorithm, dubbed as GraphMOD-Net, encompasses instance segmentation, background initialization, feature extraction, and graph construction. GraphMOD-Net is tested on unseen videos and outperforms state-of-the-art methods in unsupervised, semi-supervised, and supervised learning in several challenges of the Change Detection 2014 (CDNet2014) and UCSD background subtraction datasets.","2473-9944","978-1-6654-0191-3","10.1109/ICCVW54120.2021.00030","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9607835","","Deep learning;Training;Computer vision;Video sequences;Supervised learning;Pipelines;Object detection","computer vision;convolutional neural nets;deep learning (artificial intelligence);feature extraction;graph theory;image segmentation;image sequences;object detection;video signal processing","graph CNN;moving object detection;complex environments;computer vision applications;static camera;moving camera;dynamic backgrounds;lighting variations;MOD deep learning methods;unseen video sequences;training splits;network learning process;node classification problem;graph convolutional neural networks;GraphMOD-Net;graph construction;instance segmentation;background initialization;feature extraction;Change Detection 2014;UCSD background subtraction datasets;supervised learning;unsupervised learning;semisupervised learning;camouflage;shadow background","","13","","54","IEEE","24 Nov 2021","","","IEEE","IEEE Conferences"
"Rethinking Counting and Localization in Crowds: A Purely Point-Based Framework","Q. Song; C. Wang; Z. Jiang; Y. Wang; Y. Tai; C. Wang; J. Li; F. Huang; Y. Wu","Tencent Youtu Lab; Tencent Youtu Lab; Tencent Youtu Lab; Tencent Youtu Lab; Tencent Youtu Lab; Tencent Youtu Lab; Tencent Youtu Lab; Tencent Youtu Lab; Applied Research Center (ARC), Tencent PCG","2021 IEEE/CVF International Conference on Computer Vision (ICCV)","28 Feb 2022","2021","","","3345","3354","Localizing individuals in crowds is more in accordance with the practical demands of subsequent high-level crowd analysis tasks than simply counting. However, existing localization based methods relying on intermediate representations (i.e., density maps or pseudo boxes) serving as learning targets are counter-intuitive and error-prone. In this paper, we propose a purely point-based framework for joint crowd counting and individual localization. For this framework, instead of merely reporting the absolute counting error at image level, we propose a new metric, called density Normalized Average Precision (nAP), to provide more comprehensive and more precise performance evaluation. Moreover, we design an intuitive solution under this framework, which is called Point to Point Network (P2PNet). P2PNet discards superfluous steps and directly predicts a set of point proposals to represent heads in an image, being consistent with the human annotation results. By thorough analysis, we reveal the key step towards implementing such a novel idea is to assign optimal learning targets for these proposals. Therefore, we propose to conduct this crucial association in an one-to-one matching manner using the Hungarian algorithm. The P2PNet not only significantly surpasses state-of-the-art methods on popular counting benchmarks, but also achieves promising localization accuracy. The codes will be available at: TencentYoutuResearch/CrowdCounting-P2PNet.","2380-7504","978-1-6654-2812-5","10.1109/ICCV48922.2021.00335","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9710076","Detection and localization in 2D and 3D","Location awareness;Performance evaluation;Computer vision;Head;Codes;Annotations;Benchmark testing","","","","74","","42","IEEE","28 Feb 2022","","","IEEE","IEEE Conferences"
"Crowd Counting With Partial Annotations in an Image","Y. Xu; Z. Zhong; D. Lian; J. Li; Z. Li; X. Xu; S. Gao","IHPC, A*STAR, Singapore; ShanghaiTech University, China; ShanghaiTech University, China; ShanghaiTech University, China; ShanghaiTech University, China; IHPC, A*STAR, Singapore; ShanghaiTech University, China","2021 IEEE/CVF International Conference on Computer Vision (ICCV)","28 Feb 2022","2021","","","15550","15559","To fully leverage the data captured from different scenes with different view angles while reducing the annotation cost, this paper studies a novel crowd counting setting, i.e. only using partial annotations in each image as training data. Inspired by the repetitive patterns in the annotated and unannotated regions as well as the ones between them, we design a network with three components to tackle those unannotated regions: i) in an Unannotated Regions Characterization (URC) module, we employ a memory bank to only store the annotated features, which could help the visual features extracted from these annotated regions flow to these unannotated regions; ii) For each image, Feature Distribution Consistency (FDC) regularizes the feature distributions of annotated head and unannotated head regions to be consistent; iii) a Cross-regressor Consistency Regularization (CCR) module is designed to learn the visual features of unannotated regions in a self-supervised style. The experimental results validate the effectiveness of our proposed model under the partial annotation setting for several datasets, such as ShanghaiTech, UCF-CC-50, UCF-QNRF, NWPU-Crowd and JHU-CROWD++. With only 10% annotated regions in each image, our proposed model achieves better performance than the recent methods and baselines under semi-supervised or active learning settings on all datasets. The code is https://github.com/svip-lab/CrwodCountingPAL.","2380-7504","978-1-6654-2812-5","10.1109/ICCV48922.2021.01528","Science and Technology Commission of Shanghai Municipality; Shanghai Municipal Education Commission; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9711085","Scene analysis and understanding","Visualization;Computer vision;Costs;Head;Codes;Annotations;Computational modeling","","","","20","","48","IEEE","28 Feb 2022","","","IEEE","IEEE Conferences"
"Towards A Universal Model for Cross-Dataset Crowd Counting","Z. Ma; X. Hong; X. Wei; Y. Qiu; Y. Gong","College of Artificial Intelligence, Xi’an Jiaotong University; School of Cyber Science and Engineering, Xi’an Jiaotong University; School of Software Engineering, Xi’an Jiaotong University; School of Software Engineering, Xi’an Jiaotong University; School of Software Engineering, Xi’an Jiaotong University","2021 IEEE/CVF International Conference on Computer Vision (ICCV)","28 Feb 2022","2021","","","3185","3194","This paper proposes to handle the practical problem of learning a universal model for crowd counting across scenes and datasets. We dissect that the crux of this problem is the catastrophic sensitivity of crowd counters to scale shift, which is very common in the real world and caused by factors such as different scene layouts and image resolutions. Therefore it is difficult to train a universal model that can be applied to various scenes. To address this problem, we propose scale alignment as a prime module for establishing a novel crowd counting framework. We derive a closed-form solution to get the optimal image rescaling factors for alignment by minimizing the distances between their scale distributions. A novel neural network together with a loss function based on an efficient sliced Wasserstein distance is also proposed for scale distribution estimation. Benefiting from the proposed method, we have learned a universal model that generally works well on several datasets where can even outperform state-of-the-art models that are particularly fine-tuned for each dataset significantly. Experiments also demonstrate the much better generalizability of our model to unseen scenes.","2380-7504","978-1-6654-2812-5","10.1109/ICCV48922.2021.00319","Research and Development; National Natural Science Foundation of China; China Postdoctoral Science Foundation; Fundamental Research Funds for the Central Universities; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9710112","Detection and localization in 2D and 3D;Scene analysis and understanding","Computer vision;Sensitivity;Closed-form solutions;Image resolution;Neural networks;Layout;Estimation","","","","19","","60","IEEE","28 Feb 2022","","","IEEE","IEEE Conferences"
"STGAT: Modeling Spatial-Temporal Interactions for Human Trajectory Prediction","Y. Huang; H. Bi; Z. Li; T. Mao; Z. Wang","Beijing Key Laboratory of Mobile Computing and Pervasive Device, Institute of Computing Technology, Beijing, China; Beijing Key Laboratory of Mobile Computing and Pervasive Device, Institute of Computing Technology, Beijing, China; Beijing Key Laboratory of Mobile Computing and Pervasive Device, Institute of Computing Technology, Beijing, China; Beijing Key Laboratory of Mobile Computing and Pervasive Device, Institute of Computing Technology, Beijing, China; Beijing Key Laboratory of Mobile Computing and Pervasive Device, Institute of Computing Technology, Beijing, China","2019 IEEE/CVF International Conference on Computer Vision (ICCV)","27 Feb 2020","2019","","","6271","6280","Human trajectory prediction is challenging and critical in various applications (e.g., autonomous vehicles and social robots). Because of the continuity and foresight of the pedestrian movements, the moving pedestrians in crowded spaces will consider both spatial and temporal interactions to avoid future collisions. However, most of the existing methods ignore the temporal correlations of interactions with other pedestrians involved in a scene. In this work, we propose a Spatial-Temporal Graph Attention network (STGAT), based on a sequence-to-sequence architecture to predict future trajectories of pedestrians. Besides the spatial interactions captured by the graph attention mechanism at each time-step, we adopt an extra LSTM to encode the temporal correlations of interactions. Through comparisons with state-of-the-art methods, our model achieves superior performance on two publicly available crowd datasets (ETH and UCY) and produces more ""socially"" plausible trajectories for pedestrians.","2380-7504","978-1-7281-4803-8","10.1109/ICCV.2019.00637","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9010834","","Trajectory;Predictive models;Correlation;Task analysis;Neural networks;Computational modeling;Dynamics","image motion analysis;image sequences;object detection;pedestrians;traffic engineering computing;video surveillance","STGAT;spatial-temporal interactions;human trajectory prediction;autonomous vehicles;social robots;pedestrian movements;moving pedestrians;crowded spaces;spatial interactions;sequence-to-sequence architecture;graph attention mechanism;plausible trajectories;spatial-temporal graph attention network;ETH crowd dataset;UCY crowd dataset","","242","","43","IEEE","27 Feb 2020","","","IEEE","IEEE Conferences"
"AgentFormer: Agent-Aware Transformers for Socio-Temporal Multi-Agent Forecasting","Y. Yuan; X. Weng; Y. Ou; K. Kitani",Carnegie Mellon University; Carnegie Mellon University; Penn State University; Carnegie Mellon University,"2021 IEEE/CVF International Conference on Computer Vision (ICCV)","28 Feb 2022","2021","","","9793","9803","Predicting accurate future trajectories of multiple agents is essential for autonomous systems but is challenging due to the complex interaction between agents and the uncertainty in each agent’s future behavior. Forecasting multi-agent trajectories requires modeling two key dimensions: (1) time dimension, where we model the influence of past agent states over future states; (2) social dimension, where we model how the state of each agent affects others. Most prior methods model these two dimensions separately, e.g., first using a temporal model to summarize features over time for each agent independently and then modeling the interaction of the summarized features with a social model. This approach is suboptimal since independent feature encoding over either the time or social dimension can result in a loss of information. Instead, we would prefer a method that allows an agent’s state at one time to directly affect another agent’s state at a future time. To this end, we propose a new Transformer, termed AgentFormer, that simultaneously models the time and social dimensions. The model leverages a sequence representation of multi-agent trajectories by flattening trajectory features across time and agents. Since standard attention operations disregard the agent identity of each element in the sequence, AgentFormer uses a novel agent-aware attention mechanism that preserves agent identities by attending to elements of the same agent differently than elements of other agents. Based on AgentFormer, we propose a stochastic multi-agent trajectory prediction model that can attend to features of any agent at any previous timestep when inferring an agent’s future position. The latent intent of all agents is also jointly modeled, allowing the stochasticity in one agent’s behavior to affect other agents. Extensive experiments show that our method significantly improves the state of the art on well-established pedestrian and autonomous driving datasets.","2380-7504","978-1-6654-2812-5","10.1109/ICCV48922.2021.00967","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9710708","Motion and tracking;Vision for robotics and autonomous vehicles","Computer vision;Uncertainty;Stochastic processes;Predictive models;Transformers;Encoding;Trajectory","","","","97","","61","IEEE","28 Feb 2022","","","IEEE","IEEE Conferences"
"Adversarial Attacks On Multi-Agent Communication","J. Tu; T. Wang; J. Wang; S. Manivasagam; M. Ren; R. Urtasun",Waabi; MIT; Waabi; Waabi; Waabi; Waabi,"2021 IEEE/CVF International Conference on Computer Vision (ICCV)","28 Feb 2022","2021","","","7748","7757","Growing at a fast pace, modern autonomous systems will soon be deployed at scale, opening up the possibility for cooperative multi-agent systems. Sharing information and distributing workloads allow autonomous agents to better perform tasks and increase computation efficiency. However, shared information can be modified to execute adversarial attacks on deep learning models that are widely employed in modern systems. Thus, we aim to study the robustness of such systems and focus on exploring adversarial at-tacks in a novel multi-agent setting where communication is done through sharing learned intermediate representations of neural networks. We observe that an indistinguishable adversarial message can severely degrade performance, but becomes weaker as the number of benign agents increases. Furthermore, we show that black-box transfer attacks are more difficult in this setting when compared to directly perturbing the inputs, as it is necessary to align the distribution of learned representations with domain adaptation. Our work studies robustness at the neural network level to con-tribute an additional layer of fault tolerance to modern security protocols for more secure multi-agent systems.","2380-7504","978-1-6654-2812-5","10.1109/ICCV48922.2021.00767","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9711249","Adversarial learning;Detection and localization in 2D and 3D;Vision applications and systems;Vision for robotics and autonomous vehicles","Deep learning;Fault tolerance;Protocols;Computational modeling;Neural networks;Fault tolerant systems;Robustness","","","","10","","61","IEEE","28 Feb 2022","","","IEEE","IEEE Conferences"
"Variational Attention: Propagating Domain-Specific Knowledge for Multi-Domain Learning in Crowd Counting","B. Chen; Z. Yan; K. Li; P. Li; B. Wang; W. Zuo; L. Zhang",Harbin Institute of Technology; Harbin Institute of Technology; Harbin Institute of Technology; Harbin Institute of Technology; Harbin Institute of Technology; Harbin Institute of Technology; The Hong Kong Polytechnic University,"2021 IEEE/CVF International Conference on Computer Vision (ICCV)","28 Feb 2022","2021","","","16045","16055","In crowd counting, due to the problem of laborious labelling, it is perceived intractability of collecting a new large-scale dataset which has plentiful images with large diversity in density, scene, etc. Thus, for learning a general model, training with data from multiple different datasets might be a remedy and be of great value. In this paper, we resort to the multi-domain joint learning and propose a simple but effective Domain-specific Knowledge Propagating Network (DKPNet) for unbiasedly learning the knowledge from multiple diverse data domains at the same time. It is mainly achieved by proposing the novel Variational Attention(VA) technique for explicitly modeling the attention distributions for different domains. And as an extension to VA, Intrinsic Variational Attention(InVA) is proposed to handle the problems of over-lapped domains and sub-domains. Extensive experiments have been conducted to validate the superiority of our DKPNet over several popular datasets, including ShanghaiTech A/B, UCF-QNRF and NWPU.","2380-7504","978-1-6654-2812-5","10.1109/ICCV48922.2021.01576","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9710532","Scene analysis and understanding;Recognition and classification;Segmentation;grouping and shape;Vision applications and systems","Training;Knowledge engineering;Computer vision;Benchmark testing;Data models;Labeling","","","","20","","64","IEEE","28 Feb 2022","","","IEEE","IEEE Conferences"
"Uniformity in Heterogeneity: Diving Deep into Count Interval Partition for Crowd Counting","C. Wang; Q. Song; B. Zhang; Y. Wang; Y. Tai; X. Hu; C. Wang; J. Li; J. Ma; Y. Wu","Tencent Youtu Lab; Tencent Youtu Lab; Tencent Youtu Lab; Tencent Youtu Lab; Tencent Youtu Lab; Tencent Youtu Lab; Tencent Youtu Lab; Tencent Youtu Lab; Electronic Information School, Wuhan University, Wuhan, China; Applied Research Center (ARC), Tencent PCG","2021 IEEE/CVF International Conference on Computer Vision (ICCV)","28 Feb 2022","2021","","","3214","3222","Recently, the problem of inaccurate learning targets in crowd counting draws increasing attention. Inspired by a few pioneering work, we solve this problem by trying to predict the indices of pre-defined interval bins of counts instead of the count values themselves. However, an inappropriate interval setting might make the count error contributions from different intervals extremely imbalanced, leading to inferior counting performance. Therefore, we propose a novel count interval partition criterion called Uniform Error Partition (UEP), which always keeps the expected counting error contributions equal for all intervals to minimize the prediction risk. Then to mitigate the inevitably introduced discretization errors in the count quantization process, we propose another criterion called Mean Count Proxies (MCP). The MCP criterion selects the best count proxy for each interval to represent its count value during inference, making the overall expected discretization error of an image nearly negligible. As far as we are aware, this work is the first to delve into such a classification task and ends up with a promising solution for count interval partition. Following the above two theoretically demonstrated criterions, we propose a simple yet effective model termed Uniform Error Partition Network (UEPNet), which achieves state-of-the-art performance on several challenging datasets. The codes will be available at: TencentYoutuResearch/CrowdCounting-UEPNet.","2380-7504","978-1-6654-2812-5","10.1109/ICCV48922.2021.00322","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9710074","Detection and localization in 2D and 3D","Computer vision;Quantization (signal);Costs;Codes;Task analysis","","","","15","","28","IEEE","28 Feb 2022","","","IEEE","IEEE Conferences"
"Dynamic Context-Sensitive Filtering Network for Video Salient Object Detection","M. Zhang; J. Liu; Y. Wang; Y. Piao; S. Yao; W. Ji; J. Li; H. Lu; Z. Luo","Dalian University of Technology, China; Dalian University of Technology, China; Dalian University of Technology, China; Dalian University of Technology, China; Dalian University of Technology, China; University of Alberta, Canada; University of Alberta, Canada; Dalian University of Technology, China; Dalian University of Technology, China","2021 IEEE/CVF International Conference on Computer Vision (ICCV)","28 Feb 2022","2021","","","1533","1543","The ability to capture inter-frame dynamics has been critical to the development of video salient object detection (VSOD). While many works have achieved great success in this field, a deeper insight into its dynamic nature should be developed. In this work, we aim to answer the following questions: How can a model adjust itself to dynamic variations as well as perceive fine differences in the real-world environment; How are the temporal dynamics well introduced into spatial information over time? To this end, we propose a dynamic context-sensitive filtering network (DCFNet) equipped with a dynamic context-sensitive filtering module (DCFM) and an effective bidirectional dynamic fusion strategy. The proposed DCFM sheds new light on dynamic filter generation by extracting location-related affinities between consecutive frames. Our bidirectional dynamic fusion strategy encourages the interaction of spatial and temporal information in a dynamic manner. Experimental results demonstrate that our proposed method can achieve state-of-the-art performance on most VSOD datasets while ensuring a real-time speed of 28 fps. The source code is publicly available at https://github.com/OIPLab-DUT/DCFNet.","2380-7504","978-1-6654-2812-5","10.1109/ICCV48922.2021.00158","National Natural Science Foundation of China; Fundamental Research Funds for the Central Universities; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9710498","Video analysis and understanding;Low-level and physics-based vision","Computer vision;Convolution;Computational modeling;Object detection;Streaming media;Information filters;Real-time systems","","","","29","","62","IEEE","28 Feb 2022","","","IEEE","IEEE Conferences"
"Online Multi-Task Clustering for Human Motion Segmentation","G. Sun; Y. Cong; L. Wang; Z. Ding; Y. Fu","State Key Laboratory of Robotics, Shenyang Institute of Automation, Chinese Academy of Sciences, Shenyang, China; State Key Laboratory of Robotics, Shenyang Institute of Automation, Chinese Academy of Sciences, Shenyang, China; Northeastern University, USA; Indiana University Purdue University, Indianapolis, USA; Northeastern University, USA","2019 IEEE/CVF International Conference on Computer Vision Workshop (ICCVW)","5 Mar 2020","2019","","","970","979","Human motion segmentation in time space becomes attractive recently due to its wide range of potential applications on action recognition, event detection, and scene understanding tasks. However, most existing state-of-the-arts address this problem upon an offline and single-agent scenario, while there are a lot of urgent requirements to segment videos captured from multiple agents for real-time application (e.g., surveillance system). In this paper, we propose an Online Multi-task Clustering (OMTC) model for an online and multi-agent segmentation scenario, where each agent corresponds to one task. Specifically, a linear autoencoder framework is designed to project motion sequences into a common motion-aware space across multiple collaborating tasks, while the decoder obtains motion-aware representation of each task via a temporal preserved regularizer. To tackle distribution shifts problem between each pair of tasks, the task-specific projections are further proposed to align representation across the motion segmentation tasks. By this way, significant motion knowledge can be shared among multiple tasks, and the temporal data structures are also well preserved. For the model optimization, an efficient and effective online optimization mechanism is derived to solve the large-scale formulation in real-time applications. Experiment results on Keck, MAD and our collected human motion datasets demonstrate the robustness, high-accuracy and efficiency of our OMTC model.","2473-9944","978-1-7281-5023-9","10.1109/ICCVW.2019.00126","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9022059","Human Motion Segmentation;Multi task Clustering;Online Learning;Autoencoder Framework","Task analysis;Motion segmentation;Computer vision;Matrix decomposition;Decoding;Optimization;Encoding","image motion analysis;image representation;image segmentation;image sequences;multi-agent systems;optimisation;pattern clustering;video signal processing","single-agent scenario;video segmentation;multiple agents;real-time application;motion sequences;motion-aware representation;distribution shifts problem;task-specific projections;human motion datasets;human motion segmentation;scene understanding tasks;online optimization mechanism;motion knowledge;motion-aware space;online multitask clustering model;action recognition;event detection;multiagent segmentation scenario;linear autoencoder framework","","2","","44","IEEE","5 Mar 2020","","","IEEE","IEEE Conferences"
"ClusterSLAM: A SLAM Backend for Simultaneous Rigid Body Clustering and Motion Estimation","J. Huang; S. Yang; Z. Zhao; Y. -K. Lai; S. Hu","BNRist, Tsinghua University, Beijing; BNRist, Tsinghua University, Beijing; BNRist, Tsinghua University, Beijing; Cardiff University, UK; Tsinghua University","2019 IEEE/CVF International Conference on Computer Vision (ICCV)","27 Feb 2020","2019","","","5874","5883","We present a practical backend for stereo visual SLAM which can simultaneously discover individual rigid bodies and compute their motions in dynamic environments. While recent factor graph based state optimization algorithms have shown their ability to robustly solve SLAM problems by treating dynamic objects as outliers, the dynamic motions are rarely considered. In this paper, we exploit the consensus of 3D motions among the landmarks extracted from the same rigid body for clustering and estimating static and dynamic objects in a unified manner. Specifically, our algorithm builds a noise-aware motion affinity matrix upon landmarks, and uses agglomerative clustering for distinguishing those rigid bodies. Accompanied by a decoupled factor graph optimization for revising their shape and trajectory, we obtain an iterative scheme to update both cluster assignments and motion estimation reciprocally. Evaluations on both synthetic scenes and KITTI demonstrate the capability of our approach, and further experiments considering online efficiency also show the effectiveness of our method for simultaneous tracking of ego-motion and multiple objects.","2380-7504","978-1-7281-4803-8","10.1109/ICCV.2019.00597","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9008108","","Simultaneous localization and mapping;Motion segmentation;Dynamics;Three-dimensional displays;Tracking;Cameras;Optimization","control engineering computing;graph theory;iterative methods;motion estimation;optimisation;pattern clustering;SLAM (robots);stereo image processing","dynamic objects;dynamic motions;noise-aware motion affinity matrix;agglomerative clustering;decoupled factor graph optimization;cluster assignments;motion estimation;SLAM backend;simultaneous rigid body clustering;stereo visual SLAM;ClusterSLAM;factor graph based state optimization algorithms;3D motions;static objects;iterative scheme","","29","","47","IEEE","27 Feb 2020","","","IEEE","IEEE Conferences"
"Neighbourhood Context Embeddings in Deep Inverse Reinforcement Learning for Predicting Pedestrian Motion Over Long Time Horizons","T. Fernando; S. Denman; S. Sridharan; C. Fookes","Image and Video Research Lab, SAIVT, Queensland University of Technology (QUT), Australia; Image and Video Research Lab, SAIVT, Queensland University of Technology (QUT), Australia; Image and Video Research Lab, SAIVT, Queensland University of Technology (QUT), Australia; Image and Video Research Lab, SAIVT, Queensland University of Technology (QUT), Australia","2019 IEEE/CVF International Conference on Computer Vision Workshop (ICCVW)","5 Mar 2020","2019","","","1179","1187","Predicting crowd behaviour in the distant future has increased in prominence among the computer vision community as it provides intelligence and flexibility for autonomous systems, enabling the early detection of abnormal events and better and more natural interactions between humans and autonomous systems such as driverless vehicles and field robots. Despite the fact that Deep Inverse Reinforcement Learning (D-IRL) based modelling paradigms offer flexibility and robustness when anticipating human behaviour across long time horizons, compared to their supervised learning counterparts, no existing state-of-the-art D-IRL methods consider path planning in situations where there are multiple moving pedestrians in the environment. To address this, we present a novel recurrent neural network based method for embedding pedestrian dynamics in a D-IRL setting, where there are multiple moving agents. We propose to capture the motion of the pedestrian of interest as well as the motion of other pedestrians in the neighbourhood through Long-Short-Term Memory networks. The neighbourhood dynamics are encoded into a feature map, preserving the spatial integrity of the observed trajectories. Utilising the maximum-entropy based non-linear inverse reinforcement learning framework, we map these features to a reward map. We perform extensive evaluations on the publicly available Stanford Drone and SAIVT Multi-Spectral Trajectory datasets where the proposed method exhibits robustness towards lengthier predictions into the distant future, demonstrating the importance of capturing the dynamic evolution of the environment using the proposed embedding scheme.","2473-9944","978-1-7281-5023-9","10.1109/ICCVW.2019.00149","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9022234","Trajectory Prediction;Deep Inverse Reinforcement Learning;LSTM","Trajectory;Supervised learning;Recurrent neural networks;Entropy;Learning (artificial intelligence);Robustness","behavioural sciences computing;computer vision;entropy;feature extraction;image motion analysis;pedestrians;recurrent neural nets;supervised learning","maximum entropy;nonlinear inverse reinforcement learning;human behaviour;computer vision;crowd behaviour;pedestrian motion prediction;deep inverse reinforcement learning;long short term memory networks;pedestrian dynamics;recurrent neural network;multiple moving pedestrians;D-IRL methods;supervised learning;long time horizons","","7","","27","IEEE","5 Mar 2020","","","IEEE","IEEE Conferences"
"Learning an Event Sequence Embedding for Dense Event-Based Deep Stereo","S. Tulyakov; F. Fleuret; M. Kiefel; P. Gehler; M. Hirsch","Space Engineering Center at École Polytechnique Fédérale de Lausanne; École Polytechnique Fédérale de Lausanne and Idiap Research Institute; Amazon, Tübingen, Germany; Amazon, Tübingen, Germany; Amazon, Tübingen, Germany","2019 IEEE/CVF International Conference on Computer Vision (ICCV)","27 Feb 2020","2019","","","1527","1537","Today, a frame-based camera is the sensor of choice for machine vision applications. However, these cameras, originally developed for acquisition of static images rather than for sensing of dynamic uncontrolled visual environments, suffer from high power consumption, data rate, latency and low dynamic range. An event-based image sensor addresses these drawbacks by mimicking a biological retina. Instead of measuring the intensity of every pixel in a fixed time-interval, it reports events of significant pixel intensity changes. Every such event is represented by its position, sign of change, and timestamp, accurate to the microsecond. Asynchronous event sequences require special handling, since traditional algorithms work only with synchronous, spatially gridded data. To address this problem we introduce a new module for event sequence embedding, for use in difference applications. The module builds a representation of an event sequence by firstly aggregating information locally across time, using a novel fully-connected layer for an irregularly sampled continuous domain, and then across discrete spatial domain. Based on this module, we design a deep learning-based stereo method for event-based cameras. The proposed method is the first learning-based stereo method for an event-based camera and the only method that produces dense results. We show that large performance increases on the Multi Vehicle Stereo Event Camera Dataset (MVSEC), which became the standard set for benchmarking of event-based stereo methods.","2380-7504","978-1-7281-4803-8","10.1109/ICCV.2019.00161","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9008838","","Cameras;Image sensors;Tensile stress;Three-dimensional displays;Robot sensing systems;Machine learning;Power demand","biomimetics;cameras;computer vision;image representation;image sensors;learning (artificial intelligence);stereo image processing","deep learning-based stereo method;MultiVehicle Stereo Event Camera Dataset;event-based stereo methods;event sequence embedding;frame-based camera;machine vision applications;static images;dynamic uncontrolled visual environments;power consumption;event-based image sensor;asynchronous event sequences;spatially gridded data;pixel intensity changes;dense event-based deep stereo;MVSEC;biological retina","","38","","69","IEEE","27 Feb 2020","","","IEEE","IEEE Conferences"
"A Dataset of Multi-Illumination Images in the Wild","L. Murmann; M. Gharbi; M. Aittala; F. Durand",MIT CSAIL; Adobe; MIT CSAIL; MIT,"2019 IEEE/CVF International Conference on Computer Vision (ICCV)","27 Feb 2020","2019","","","4079","4088","Collections of images under a single, uncontrolled illumination have enabled the rapid advancement of core computer vision tasks like classification, detection, and segmentation. But even with modern learning techniques, many inverse problems involving lighting and material understanding remain too severely ill-posed to be solved with single-illumination datasets. The data simply does not contain the necessary supervisory signals. Multi-illumination datasets are notoriously hard to capture, so the data is typically collected at small scale, in controlled environments, either using multiple light sources, or robotic gantries. This leads to image collections that are not representative of the variety and complexity of real world scenes. We introduce a new multi-illumination dataset of more than 1000 real scenes, each captured in high dynamic range and high resolution, under 25 lighting conditions. We demonstrate the richness of this dataset by training state-of-the-art models for three challenging applications: single-image illumination estimation, image relighting, and mixed-illuminant white balance.","2380-7504","978-1-7281-4803-8","10.1109/ICCV.2019.00418","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9008252","","Lighting;Cameras;Light sources;Dynamic range;Estimation;Probes;Computer vision","computer vision;image colour analysis;image processing;image sensors;inverse problems;learning (artificial intelligence)","image relighting;mixed-illuminant white balance;multiillumination images;single illumination;uncontrolled illumination;rapid advancement;core computer vision tasks;modern learning techniques;single-illumination datasets;necessary supervisory signals;multiillumination dataset;multiple light sources;image collections;25 lighting conditions;single-image illumination estimation","","32","","54","IEEE","27 Feb 2020","","","IEEE","IEEE Conferences"
"FuseMODNet: Real-Time Camera and LiDAR Based Moving Object Detection for Robust Low-Light Autonomous Driving","H. Rashed; M. Ramzy; V. Vaquero; A. El Sallab; G. Sistu; S. Yogamani","Valeo R&D, Egypt; Cairo University; IRI BarcelonaTech, Spain; Valeo R&D; Valeo Vision Systems, Ireland; Valeo Vision Systems, Ireland","2019 IEEE/CVF International Conference on Computer Vision Workshop (ICCVW)","5 Mar 2020","2019","","","2393","2402","Moving object detection is a critical task for autonomous vehicles. As dynamic objects represent higher collision risk than static ones, our own ego-trajectories have to be planned attending to the future states of the moving elements of the scene. Motion can be perceived using temporal information such as optical flow. Conventional optical flow computation is based on camera sensors only, which makes it prone to failure in conditions with low illumination. On the other hand, LiDAR sensors are independent of illumination, as they measure the time-of-flight of their own emitted lasers. In this work we propose a robust and real-time CNN architecture for Moving Object Detection (MOD) under low-light conditions by capturing motion information from both camera and LiDAR sensors. We demonstrate the impact of our algorithm on KITTI dataset where we simulate a low-light environment creating a novel dataset ""Dark-KITTI"". We obtain a 10.1 % relative improvement on Dark-KITTI, and a 4.25 % improvement on standard KITTI relative to our baselines. The proposed algorithm runs at 29 fps on a standard desktop GPU using 256x1224 resolution images.","2473-9944","978-1-7281-5023-9","10.1109/ICCVW.2019.00293","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9022585","Autonomous Driving;Motion Segmentation;Deep Fusion;Low light","Laser radar;Cameras;Sensors;Three-dimensional displays;Lighting;Optical imaging;Optical distortion","cameras;convolutional neural nets;image motion analysis;image sensors;image sequences;mobile robots;object detection;optical radar;robot vision","FuseMODNet;low-light autonomous driving;real-time camera;low-light environment;motion information;low-light conditions;object detection;real-time CNN architecture;robust time CNN architecture;time-of-flight;LiDAR sensors;low illumination;camera sensors;conventional optical flow computation;temporal information;moving elements;collision risk;dynamic objects;autonomous vehicles","","42","","44","IEEE","5 Mar 2020","","","IEEE","IEEE Conferences"
"Real-Time Aerial Suspicious Analysis (ASANA) System for the Identification and Re-Identification of Suspicious Individuals using the Bayesian ScatterNet Hybrid (BSH) Network","A. Singh; K. Kiran G.V.; O. Harsh; R. Kumar; K. Singh Rajput; C. S .S. Vamsi","Skylark Labs LLC., San Francisco, USA; NIT Warangal; Skylark Labs LLP., Warangal, India; Skylark Labs LLP., Warangal, India; Skylark Labs LLP, Warangal, India; Skylark Labs LLP, Warangal, India","2019 IEEE/CVF International Conference on Computer Vision Workshop (ICCVW)","5 Mar 2020","2019","","","73","81","Video monitoring and safety systems have been used to keep track of hostiles, conduct border control operations as well as to monitor the suspicious entities in public spaces. However, these systems are inadequate for the monitoring of large crowds due to the limited field of view of cameras. This paper introduces the Aerial Suspicious Analysis (ASANA) System for the Identification and Re-Identification of suspicious Individuals in large public areas using the Bayesian ScatterNet Hybrid (BSH) Network. The BSH network first estimates the human pose in each frame. Next, a batch of frames is used by the Bayesian 3D ResNext to identify individuals with suspicious postures. The system can also re-identify the identified suspicious individuals as they tend to move after committing the suspicious event. The proposed architecture is advantageous as it can learn meaningful representations quickly using the ScatterNet with fewer labelled examples. This is of great importance as real-life annotated training samples are hard to collect, especially for these applications. The pose estimation, suspicious individual identification, and re-identification performance of the proposed framework is compared with the state-of-the-art techniques. The proposed dataset is also made public which may encourage other researchers who are interested in using the deep learning technique for aerial visual crowd monitoring.","2473-9944","978-1-7281-5023-9","10.1109/ICCVW.2019.00015","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9022217","Drone;Suspicious Analysis;Suspicious Individuals;Identification;Re identification;Bayesian;ScatterNet;Deep Learning","Personal area networks;Drones;Feature extraction;Bayes methods;Monitoring;Real-time systems;Training","Bayes methods;learning (artificial intelligence);object detection;pose estimation;video signal processing;video surveillance","deep learning technique;Bayesian ScatterNet hybrid network;aerial suspicious analysis system;safety systems;video monitoring;aerial visual crowd monitoring;re-identification performance;suspicious individual identification;suspicious event;suspicious postures;Bayesian 3D ResNext;BSH network;public areas;ASANA;public spaces;suspicious entities;border control operations","","3","1","27","IEEE","5 Mar 2020","","","IEEE","IEEE Conferences"
"A Light Stage on Every Desk","S. Sengupta; B. Curless; I. Kemelmacher-Shlizerman; S. Seitz",University of Washington; University of Washington; University of Washington; University of Washington,"2021 IEEE/CVF International Conference on Computer Vision (ICCV)","28 Feb 2022","2021","","","2400","2409","Every time you sit in front of a TV or monitor, your face is actively illuminated by time-varying patterns of light. This paper proposes to use this time-varying illumination for synthetic relighting of your face with any new illumination condition. In doing so, we take inspiration from the light stage work of Debevec et al. [4], who first demonstrated the ability to relight people captured in a controlled lighting environment. Whereas existing light stages require expensive, room-scale spherical capture gantries and exist in only a few labs in the world, we demonstrate how to acquire useful data from a normal TV or desktop monitor. Instead of subjecting the user to uncomfortable rapidly flashing light patterns, we operate on images of the user watching a YouTube video or other standard content. We train a deep network on images plus monitor patterns of a given user and learn to predict images of that user under any target illumination (monitor pattern). Experimental evaluation shows that our method produces realistic relighting results. Video results are available at grail.cs.washington.edu/projects/Light_Stage_on_Every_Desk/.","2380-7504","978-1-6654-2812-5","10.1109/ICCV48922.2021.00242","Facebook; Google; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9709911","Computational photography","Privacy;TV;Heuristic algorithms;Lighting;Dynamic range;Motion pictures;Forgery","","","","6","","28","IEEE","28 Feb 2022","","","IEEE","IEEE Conferences"
"Bayesian Relational Memory for Semantic Visual Navigation","Y. Wu; Y. Wu; A. Tamar; S. Russell; G. Gkioxari; Y. Tian",UC Berkeley; UC Berkeley; Technion; UC Berkeley; OpenAI; OpenAI,"2019 IEEE/CVF International Conference on Computer Vision (ICCV)","27 Feb 2020","2019","","","2769","2779","We introduce a new memory architecture, Bayesian Relational Memory (BRM), to improve the generalization ability for semantic visual navigation agents in unseen environments, where an agent is given a semantic target to navigate towards. BRM takes the form of a probabilistic relation graph over semantic entities (e.g., room types), which allows (1) capturing the layout prior from training environments, i.e., prior knowledge, (2) estimating posterior layout at test time, i.e., memory update, and (3) efficient planning for navigation, altogether. We develop a BRM agent consisting of a BRM module for producing sub-goals and a goal-conditioned locomotion module for control. When testing in unseen environments, the BRM agent outperforms baselines that do not explicitly utilize the probabilistic relational memory structure.","2380-7504","978-1-7281-4803-8","10.1109/ICCV.2019.00286","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9009539","","Semantics;Navigation;Visualization;Planning;Probabilistic logic;Bayes methods;Layout","Bayes methods;graph theory;memory architecture;multi-agent systems;navigation;path planning;robot vision","semantic target;probabilistic relation graph;semantic entities;memory update;BRM agent;BRM module;probabilistic relational memory structure;memory architecture;generalization ability;semantic visual navigation agents;Bayesian relational memory;goal-conditioned locomotion module","","49","","64","IEEE","27 Feb 2020","","","IEEE","IEEE Conferences"
"Embodied Amodal Recognition: Learning to Move to Perceive Objects","J. Yang; Z. Ren; M. Xu; X. Chen; D. Crandall; D. Parikh; D. Batra",Georgia Institute of Technology; Georgia Institute of Technology; Indiana University; Facebook AI Research; Indiana University; Georgia Institute of Technology; Georgia Institute of Technology,"2019 IEEE/CVF International Conference on Computer Vision (ICCV)","27 Feb 2020","2019","","","2040","2050","Passive visual systems typically fail to recognize objects in the amodal setting where they are heavily occluded. In contrast, humans and other embodied agents have the ability to move in the environment and actively control the viewing angle to better understand object shapes and semantics. In this work, we introduce the task of Embodied Amodel Recognition (EAR): an agent is instantiated in a 3D environment close to an occluded target object, and is free to move in the environment to perform object classification, amodal object localization, and amodal object segmentation. To address this problem, we develop a new model called Embodied Mask R-CNN for agents to learn to move strategically to improve their visual recognition abilities. We conduct experiments using a simulator for indoor environments. Experimental results show that: 1) agents with embodiment (movement) achieve better visual recognition performance than passive ones and 2) in order to improve visual recognition abilities, agents can learn strategic paths that are different from shortest paths.","2380-7504","978-1-7281-4803-8","10.1109/ICCV.2019.00213","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9008379","","Task analysis;Shape;Ear;Target recognition;Three-dimensional displays;Visualization","convolutional neural nets;image segmentation;learning (artificial intelligence);multi-agent systems;path planning;shape recognition","perceive objects;passive visual systems;embodied agents;object shapes;occluded target object;object classification;amodal object localization;amodal object segmentation;EAR;embodied mask R-CNN;agents with embodiment;visual recognition;strategic paths;shortest paths;embodied amodal recognition","","14","","70","IEEE","27 Feb 2020","","","IEEE","IEEE Conferences"
"Learning to Caption Images Through a Lifetime by Asking Questions","T. Shen; A. Kar; S. Fidler",Vector Institute; Vector Institute; Vector Institute,"2019 IEEE/CVF International Conference on Computer Vision (ICCV)","27 Feb 2020","2019","","","10392","10401","In order to bring artificial agents into our lives, we will need to go beyond supervised learning on closed datasets to having the ability to continuously expand knowledge. Inspired by a student learning in a classroom, we present an agent that can continuously learn by posing natural language questions to humans. Our agent is composed of three interacting modules, one that performs captioning, another that generates questions and a decision maker that learns when to ask questions by implicitly reasoning about the uncertainty of the agent and expertise of the teacher. As compared to current active learning methods which query images for full captions, our agent is able to ask pointed questions to improve the generated captions. The agent trains on the improved captions, expanding its knowledge. We show that our approach achieves better performance using less human supervision than the baselines on the challenging MSCOCO dataset.","2380-7504","978-1-7281-4803-8","10.1109/ICCV.2019.01049","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9009050","","Generators;Task analysis;Natural languages;Training;Cognition;Dogs;Automobiles","image retrieval;multi-agent systems;natural language processing;question answering (information retrieval);supervised learning","active learning methods;MSCOCO dataset;artificial agents;supervised learning;closed datasets;natural language questions;decision maker;image caption","","6","","31","IEEE","27 Feb 2020","","","IEEE","IEEE Conferences"
"From Open Set to Closed Set: Counting Objects by Spatial Divide-and-Conquer","H. Xiong; H. Lu; C. Liu; L. Liu; Z. Cao; C. Shen","Huazhong University of Science and Technology, China; University of Adelaide, Australia; Huazhong University of Science and Technology, China; Huazhong University of Science and Technology, China; Huazhong University of Science and Technology, China; University of Adelaide, Australia","2019 IEEE/CVF International Conference on Computer Vision (ICCV)","27 Feb 2020","2019","","","8361","8370","Visual counting, a task that predicts the number of objects from an image/video, is an open-set problem by nature, i.e., the number of population can vary in [0,+∞) in theory. However, the collected images and labeled count values are limited in reality, which means only a small closed set is observed. Existing methods typically model this task in a regression manner, while they are likely to suffer from an unseen scene with counts out of the scope of the closed set. In fact, counting is decomposable. A dense region can always be divided until the count values of sub-regions are within the previously observed closed set. Inspired by this idea, we propose a simple but effective approach, Spatial Divide-and-Conquer Network (S-DCNet). S-DCNet learns to classify closed-set counts and can generalize to open-set counts via S-DC. S-DCNet is also efficient. To avoid repeatedly computing sub-region convolutional features, S-DC is executed on the feature map instead of on the input image. S-DCNet achieves the state-of-the-art performance on three crowd counting datasets (ShanghaiTech, UCF_CC_50 and UCF-QNRF), a vehicle counting dataset (TRANCOS) and a plant counting dataset (MTC). Compared to the previous best methods, S-DCNet brings a 20.2% relative improvement on the ShanghaiTechPart B, 20.9% on the UCF-QNRF, 22.5% on the TRANCOS and 15.1% on the MTC. Code has been made available at: https://github.com/xhp-hust-2018-2011/S-DCNet.","2380-7504","978-1-7281-4803-8","10.1109/ICCV.2019.00845","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9010835","","Decoding;Task analysis;Feature extraction;Image resolution;Visualization;Training;Estimation","divide and conquer methods;image processing;learning (artificial intelligence);neural nets;regression analysis","TRANCOS;UCF-QNRF;Spatial Divide-and-Conquer Network;open set;plant counting dataset;vehicle counting dataset;crowd counting datasets;open-set counts;closed-set counts;S-DCNet;observed closed set;labeled count values;open-set problem;visual counting;counting objects","","87","","38","IEEE","27 Feb 2020","","","IEEE","IEEE Conferences"
"Joint Prediction for Kinematic Trajectories in Vehicle-Pedestrian-Mixed Scenes","H. Bi; Z. Fang; T. Mao; Z. Wang; Z. Deng","Beijing Key Laboratory of Mobile Computing and Pervasive Device, Institute of Computing Technology, Chinese Academy of Sciences; Beijing Key Laboratory of Mobile Computing and Pervasive Device, Institute of Computing Technology, Chinese Academy of Sciences; Beijing Key Laboratory of Mobile Computing and Pervasive Device, Institute of Computing Technology, Chinese Academy of Sciences; Beijing Key Laboratory of Mobile Computing and Pervasive Device, Institute of Computing Technology, Chinese Academy of Sciences; Institute of Computing Technology, Chinese Academy of Sciences, University of Houston","2019 IEEE/CVF International Conference on Computer Vision (ICCV)","27 Feb 2020","2019","","","10382","10391","Trajectory prediction for objects is challenging and critical for various applications (e.g., autonomous driving, and anomaly detection). Most of the existing methods focus on homogeneous pedestrian trajectories prediction, where pedestrians are treated as particles without size. However, they fall short of handling crowded vehicle-pedestrian-mixed scenes directly since vehicles, limited with kinematics in reality, should be treated as rigid, non-particle objects ideally. In this paper, we tackle this problem using separate LSTMs for heterogeneous vehicles and pedestrians. Specifically, we use an oriented bounding box to represent each vehicle, calculated based on its position and orientation, to denote its kinematic trajectories. We then propose a framework called VP-LSTM to predict the kinematic trajectories of both vehicles and pedestrians simultaneously. In order to evaluate our model, a large dataset containing the trajectories of both vehicles and pedestrians in vehicle-pedestrian-mixed scenes is specially built. Through comparisons between our method with state-of-the-art approaches, we show the effectiveness and advantages of our method on kinematic trajectories prediction in vehicle-pedestrian-mixed scenes.","2380-7504","978-1-7281-4803-8","10.1109/ICCV.2019.01048","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9008812","","Trajectory;Kinematics;Predictive models;Vehicle dynamics;Computer vision;Autonomous vehicles;Anomaly detection","computer vision;image motion analysis;image representation;object tracking;pedestrians;recurrent neural nets;road traffic;road vehicles;traffic engineering computing","homogeneous pedestrian trajectories prediction;crowded vehicle-pedestrian-mixed scenes;heterogeneous vehicles;kinematic trajectory prediction;object trajectory prediction;oriented bounding box;vehicle representation;VP-LSTM framework;motion patterns;traffic agents;computer vision","","18","","35","IEEE","27 Feb 2020","","","IEEE","IEEE Conferences"
"STIRNet: A Spatial-temporal Interaction-aware Recursive Network for Human Trajectory Prediction","Y. Peng; G. Zhang; X. Li; L. Zheng","School of Computer Science and Information Engineering, Hefei University of Technology; School of Software, Hefei University of Technology; School of Computer Science and Information Engineering, Hefei University of Technology; School of Computer Science and Information Engineering, Hefei University of Technology","2021 IEEE/CVF International Conference on Computer Vision Workshops (ICCVW)","24 Nov 2021","2021","","","2285","2293","Pedestrian trajectory prediction is one of the important research topics in the field of computer vision and a key technology of autonomous driving system. However, it’s full of challenges due to the uncertainties of crowd motions and complex interactions among pedestrians. We pro-pose a Spatiotemporal Interaction-aware Recursive Net-work (STIRNet) to predict multiply socially acceptable trajectories of pedestrians. In this paper, a recursive structure is used to capture spatio-temporal interactions by spatial modeling and temporal modeling alternately. At each time-step, the spatial interactions are modeled by a graph attention network, in which the nodes feature are represented by temporal motion features. The learned spatial interaction context is used to capture temporal motion features through an LSTM model. The temporal motion features are used to infer future positions and update nodes features. Experimental results on two public pedestrian trajectory datasets (ETH and UCY) demonstrate that our proposed model achieves superior performances compared with state-of-the-art methods on ADE and FDE metrics.","2473-9944","978-1-6654-0191-3","10.1109/ICCVW54120.2021.00258","National Natural Science Foundation of China; Fundamental Research Funds for the Central Universities; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9607725","","Measurement;Computer vision;Uncertainty;Computational modeling;Conferences;Predictive models;Trajectory","computer vision;feature extraction;image motion analysis;image sequences;learning (artificial intelligence);pedestrians;recurrent neural nets;spatiotemporal phenomena","STIRNet;human trajectory prediction;pedestrian trajectory prediction;autonomous driving system;crowd motions;complex interactions;pedestrians;socially acceptable trajectories;recursive structure;spatio-temporal interactions;spatial modeling;temporal modeling;spatial interactions;graph attention network;nodes feature;temporal motion features;spatial interaction context;LSTM model;public pedestrian trajectory datasets;spatial-temporal Interaction-aware recursive network;computer vision;spatiotemporal interaction-aware recursive network","","5","","39","IEEE","24 Nov 2021","","","IEEE","IEEE Conferences"
"A Framework for Semi-automatic Collection of Temporal Satellite Imagery for Analysis of Dynamic Regions","N. K. Motlagh; A. Radhakrishnan; J. Davis; R. Ilin","Department of Computer Science and Engineering, Ohio State University; Department of Computer Science and Engineering, Ohio State University; Department of Computer Science and Engineering, Ohio State University; AFRL/RYAP, Wright-Patterson AFB","2021 IEEE/CVF International Conference on Computer Vision Workshops (ICCVW)","24 Nov 2021","2021","","","704","712","Analyzing natural and anthropogenic activities using re-mote sensing data has become a problem of increasing interest. However, this generally involves tediously labeling extensive imagery, perhaps on a global scale. The lack of a streamlined method to collect and label imagery over time makes it challenging to tackle these problems using popular, supervised deep learning approaches. We address this need by presenting a framework to semi-automatically collect and label dynamic regions in satellite imagery using crowd-sourced OpenStreetMap data and available satellite imagery resources. The generated labels can be quickly verified to ease the burden of full manual labeling. We leverage this framework for the ability to gather image sequences of areas that have label reclassification over time. One possible application of our framework is demonstrated to collect and classify construction vs. non-construction sites. Overall, the proposed framework can be adapted for similar change detection or classification tasks in various re-mote sensing applications.","2473-9944","978-1-6654-0191-3","10.1109/ICCVW54120.2021.00084","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9607433","","Training;Satellites;Manuals;Data collection;Aerodynamics;Sensors;Image sequences","feature extraction;geophysical image processing;image classification;image sequences;learning (artificial intelligence);pattern classification","generated labels;manual labeling;label reclassification;re-mote sensing applications;semiautomatic collection;temporal satellite imagery;dynamic regions;analyzing natural activities;anthropogenic activities;re-mote sensing data;extensive imagery;global scale;streamlined method;popular learning approaches;supervised deep learning approaches;crowd-sourced OpenStreetMap data;available satellite imagery resources","","","","22","IEEE","24 Nov 2021","","","IEEE","IEEE Conferences"
"Learning to Localise and Count with Incomplete Dot-annotations","F. Chen; M. P. Pound; A. P. French","School of Computer Science, University of Nottingham, U.K.; School of Computer Science, University of Nottingham, U.K.; School of Computer Science, University of Nottingham, U.K.","2021 IEEE/CVF International Conference on Computer Vision Workshops (ICCVW)","24 Nov 2021","2021","","","1612","1620","Annotating training data is a time consuming and labor intensive process in deep learning, especially for images with many objects present. In this paper, we propose a method to allow deep networks to be trained on data with reduced numbers of annotations per image in heatmap regression tasks (e.g. object localisation and counting), by applying an asymmetric loss function. This reduction of annotations can be imposed by the researchers by asking annotators to intentionally label only 50% of what they see in each image - a form of ’few-click’ annotation. Our method also has a secondary benefit of counteracting unintentionally missing labels from the annotators. We conduct experiments on wheat spikelet localisation and crowd counting to assess the effectiveness and robustness of our method. Results show that an asymmetric loss function is effective across different models and datasets, even in very extreme cases with limited annotations provided (e.g. 90% of the original annotations reduced). Whilst tuning of the key parameters is required, we find that setting conservative parameter values can help more realistic situations, where only small amounts of data have been missed by annotators.","2473-9944","978-1-6654-0191-3","10.1109/ICCVW54120.2021.00186","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9607724","","Training;Heating systems;Head;Annotations;Training data;Semisupervised learning;Fatigue","data analysis;deep learning (artificial intelligence);image processing;regression analysis","original annotations;incomplete dot-annotations;training data annotation;labor intensive process;deep learning;asymmetric loss function;few-click annotation;wheat spikelet localisation;crowd counting;conservative parameter values;heatmap regression tasks","","","","25","IEEE","24 Nov 2021","","","IEEE","IEEE Conferences"
"Exploring the Limitations of Behavior Cloning for Autonomous Driving","F. Codevilla; E. Santana; A. Lopez; A. Gaidon","Computer Vision Center (CVC) Campus UAB, Barcelona, Spain; Toyota Research Institute (TRI), Los Altos, CA, USA; Computer Vision Center, Barcelona, Spain; Toyota Research Institute (TRI), Los Altos, CA, USA","2019 IEEE/CVF International Conference on Computer Vision (ICCV)","27 Feb 2020","2019","","","9328","9337","Driving requires reacting to a wide variety of complex environment conditions and agent behaviors. Explicitly modeling each possible scenario is unrealistic. In contrast, imitation learning can, in theory, leverage data from large fleets of human-driven cars. Behavior cloning in particular has been successfully used to learn simple visuomotor policies end-to-end, but scaling to the full spectrum of driving behaviors remains an unsolved problem. In this paper, we propose a new benchmark to experimentally investigate the scalability and limitations of behavior cloning. We show that behavior cloning leads to state-ofthe-art results, executing complex lateral and longitudinal maneuvers, even in unseen environments, without being explicitly programmed to do so. However, we confirm some limitations of the behavior cloning approach: some wellknown limitations (e.g., dataset bias and overfitting), new generalization issues (e.g., dynamic objects and the lack of a causal modeling), and training instabilities, all requiring further research before behavior cloning can graduate to real-world driving. The code, dataset, benchmark, and agent studied in this paper can be found at http:// github.com/felipecode/coiltraine/blob/ master/docs/exploring_limitations.md.","2380-7504","978-1-7281-4803-8","10.1109/ICCV.2019.00942","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9009463","","Cloning;Training;Benchmark testing;Training data;Computer vision;Autonomous vehicles;Vehicle dynamics","behavioural sciences computing;learning (artificial intelligence);neural nets","autonomous driving;complex environment conditions;driving behaviors;simple visuomotor policies;agent behaviors;imitation learning;human-driven cars;longitudinal maneuvers;complex lateral maneuvers;behavior cloning approach;training instabilities","","174","1","46","IEEE","27 Feb 2020","","","IEEE","IEEE Conferences"
"Neural Re-Simulation for Generating Bounces in Single Images","C. Innamorati; B. Russell; D. Kaufman; N. Mitra",University College London; Adobe Research; Adobe Research; university college london,"2019 IEEE/CVF International Conference on Computer Vision (ICCV)","27 Feb 2020","2019","","","8718","8727","We introduce a method to generate videos of dynamic virtual objects plausibly interacting via collisions with a still image's environment. Given a starting trajectory, physically simulated with the estimated geometry of a single, static input image, we learn to 'correct' this trajectory to a visually plausible one via a neural network. The neural network can then be seen as learning to `correct' traditional simulation output, generated with incomplete and imprecise world information, to obtain context-specific, visually plausible re-simulated output - a process we call neural re-simulation. We train our system on a set of 50k synthetic scenes where a virtual moving object (ball) has been physically simulated. We demonstrate our approach on both our synthetic dataset and a collection of real-life images depicting everyday scenes, obtaining consistent improvement over baseline alternatives throughout.","2380-7504","978-1-7281-4803-8","10.1109/ICCV.2019.00881","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9009056","","Trajectory;Videos;Neural networks;Visualization;Geometry;Predictive models;Task analysis","geometry;image processing;learning (artificial intelligence);neural nets","starting trajectory;geometry;static input image;neural network;neural re-simulation;virtual moving object;real-life images;dynamic virtual objects;single image bounce generation;visually plausible resimulated output","","3","","56","IEEE","27 Feb 2020","","","IEEE","IEEE Conferences"
"Human Trajectory Prediction via Counterfactual Analysis","G. Chen; J. Li; J. Lu; J. Zhou","Department of Automation, Tsinghua University, China; Department of Automation, Tsinghua University, China; Department of Automation, Tsinghua University, China; Department of Automation, Tsinghua University, China","2021 IEEE/CVF International Conference on Computer Vision (ICCV)","28 Feb 2022","2021","","","9804","9813","Forecasting human trajectories in complex dynamic environments plays a critical role in autonomous vehicles and intelligent robots. Most existing methods learn to predict future trajectories by behavior clues from history trajectories and interaction clues from environments. However, the inherent bias between training and deployment environments is ignored. Hence, we propose a counterfactual analysis method for human trajectory prediction to investigate the causality between the predicted trajectories and input clues and alleviate the negative effects brought by the environment bias. We first build a causal graph for trajectory forecasting with history trajectory, future trajectory, and the environment interactions. Then, we cut off the inference from environment to trajectory by constructing the counterfactual intervention on the trajectory itself. Finally, we compare the factual and counterfactual trajectory clues to alleviate the effects of environment bias and highlight the trajectory clues. Our counterfactual analysis is a plug-and-play module that can be applied to any baseline prediction methods including RNN- and CNN-based ones. We show that our method achieves consistent improvement for different baselines and obtains the state-of-the-art results on public pedestrian trajectory forecasting benchmarks.1","2380-7504","978-1-6654-2812-5","10.1109/ICCV48922.2021.00968","Research and Development; National Natural Science Foundation of China; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9710303","Motion and tracking","Training;Computer vision;Computational modeling;Predictive models;Benchmark testing;Trajectory;History","","","","17","","57","IEEE","28 Feb 2022","","","IEEE","IEEE Conferences"
"In-the-Wild Single Camera 3D Reconstruction Through Moving Water Surfaces","J. Xiong; W. Heidrich",KAUST; KAUST,"2021 IEEE/CVF International Conference on Computer Vision (ICCV)","28 Feb 2022","2021","","","12538","12547","We present a method for reconstructing the 3D shape of underwater environments from a single, stationary camera placed above the water. We propose a novel differentiable framework, which, to our knowledge, is the first single-camera solution that is capable of simultaneously retrieving the structure of dynamic water surfaces and static underwater scene geometry in the wild. This framework integrates ray casting of Snell’s law at the refractive interface, multi-view triangulation and specially designed loss functions.Our method is calibration-free, and thus it is easy to collect data outdoors in uncontrolled environments. Experimental results show that our method is able to realize robust and quality reconstructions on a variety of scenes, both in a laboratory environment and in the wild, and even in a salt water environment. We believe the method is promising for applications in surveying and environmental monitoring.","2380-7504","978-1-6654-2812-5","10.1109/ICCV48922.2021.01233","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9710376","3D from a single image and shape-from-x;Stereo;3D from multiview and other sensors","Geometry;Surface reconstruction;Computer vision;Casting;Three-dimensional displays;Shape;Laboratories","","","","6","","37","IEEE","28 Feb 2022","","","IEEE","IEEE Conferences"
"Inference of Black Hole Fluid-Dynamics from Sparse Interferometric Measurements","A. Levis; D. Lee; J. A. Tropp; C. F. Gammie; K. L. Bouman",California Institute of Technology; University of Illinois; California Institute of Technology; University of Illinois; California Institute of Technology,"2021 IEEE/CVF International Conference on Computer Vision (ICCV)","28 Feb 2022","2021","","","2320","2329","We develop an approach to recover the underlying properties of fluid-dynamical processes from sparse measurements. We are motivated by the task of imaging the stochastically evolving environment surrounding black holes, and demonstrate how flow parameters can be estimated from sparse interferometric measurements used in radio astronomical imaging. To model the stochastic flow we use spatio-temporal Gaussian Random Fields (GRFs). The high dimensionality of the underlying source video makes direct representation via a GRF’s full covariance matrix intractable. In contrast, stochastic partial differential equations are able to capture correlations at multiple scales by specifying only local interaction coefficients. Our approach estimates the coefficients of a space-time diffusion equation that dictates the stationary statistics of the dynamical process. We analyze our approach on realistic simulations of black hole evolution and demonstrate its advantage over state-of-the-art dynamic black hole imaging techniques.","2380-7504","978-1-6654-2812-5","10.1109/ICCV48922.2021.00234","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9711239","Computational photography;Low-level and physics-based vision","Fluid flow measurement;Correlation;Partial differential equations;Imaging;Stochastic processes;Radio interferometry;Extraterrestrial measurements","","","","1","","39","IEEE","28 Feb 2022","","","IEEE","IEEE Conferences"
"MOTSynth: How Can Synthetic Data Help Pedestrian Detection and Tracking?","M. Fabbri; G. Brasó; G. Maugeri; O. Cetintas; R. Gasparini; A. Ošep; S. Calderara; L. Leal-Taixé; R. Cucchiara","University of Modena and Reggio Emilia, Italy; Technical University of Munich, Germany; University of Modena and Reggio Emilia, Italy; Technical University of Munich, Germany; University of Modena and Reggio Emilia, Italy; Technical University of Munich, Germany; University of Modena and Reggio Emilia, Italy; Technical University of Munich, Germany; University of Modena and Reggio Emilia, Italy","2021 IEEE/CVF International Conference on Computer Vision (ICCV)","28 Feb 2022","2021","","","10829","10839","Deep learning-based methods for video pedestrian detection and tracking require large volumes of training data to achieve good performance. However, data acquisition in crowded public environments raises data privacy concerns – we are not allowed to simply record and store data without the explicit consent of all participants. Furthermore, the annotation of such data for computer vision applications usually requires a substantial amount of manual effort, especially in the video domain. Labeling instances of pedestrians in highly crowded scenarios can be challenging even for human annotators and may introduce errors in the training data. In this paper, we study how we can advance different aspects of multi-person tracking using solely synthetic data. To this end, we generate MOTSynth, a large, highly diverse synthetic dataset for object detection and tracking using a rendering game engine. Our experiments show that MOTSynth can be used as a replacement for real data on tasks such as pedestrian detection, re-identification, segmentation, and tracking.","2380-7504","978-1-6654-2812-5","10.1109/ICCV48922.2021.01067","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9711285","Motion and tracking","Training;Learning systems;Computer vision;Roads;Training data;Object detection;Manuals","","","","44","","85","IEEE","28 Feb 2022","","","IEEE","IEEE Conferences"
"Learning to drive from a world on rails","D. Chen; V. Koltun; P. Krähenbühl",UT Austin; Intel Labs; UT Austin,"2021 IEEE/CVF International Conference on Computer Vision (ICCV)","28 Feb 2022","2021","","","15570","15579","We learn an interactive vision-based driving policy from pre-recorded driving logs via a model-based approach. A forward model of the world supervises a driving policy that predicts the outcome of any potential driving trajectory. To support learning from pre-recorded logs, we assume that the world is on rails, meaning neither the agent nor its actions influence the environment. This assumption greatly simplifies the learning problem, factorizing the dynamics into a non-reactive world model and a low-dimensional and compact forward model of the ego-vehicle. Our approach computes action-values for each training trajectory using a tabular dynamic-programming evaluation of the Bellman equations; these action-values in turn supervise the final vision-based driving policy. Despite the world-on-rails assumption, the final driving policy acts well in a dynamic and reactive world. It outperforms imitation learning as well as model-based and model-free reinforcement learning on the challenging CARLA NoCrash benchmark. It is also an order of magnitude more sample-efficient than state-of-the-art model-free reinforcement learning techniques on navigational tasks in the ProcGen benchmark.","2380-7504","978-1-6654-2812-5","10.1109/ICCV48922.2021.01530","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9709920","Vision for robotics and autonomous vehicles","Rails;Training;Navigation;Computational modeling;Reinforcement learning;Benchmark testing;Mathematical models","","","","24","","41","IEEE","28 Feb 2022","","","IEEE","IEEE Conferences"
"BEV-Net: Assessing Social Distancing Compliance by Joint People Localization and Geometric Reasoning","Z. Dai; Y. Jiang; Y. Li; B. Liu; A. B. Chan; N. Vasconcelos","Department of Electrical and Computer Engineering, UC San Diego; Department of Electrical and Computer Engineering, UC San Diego; Department of Electrical and Computer Engineering, UC San Diego; Wormpex AI Research; Department of Computer Science, City University of Hong Kong; Department of Electrical and Computer Engineering, UC San Diego","2021 IEEE/CVF International Conference on Computer Vision (ICCV)","28 Feb 2022","2021","","","5381","5391","Social distancing, an essential public health measure to limit the spread of contagious diseases, has gained significant attention since the outbreak of the COVID-19 pandemic. In this work, the problem of visual social distancing compliance assessment in busy public areas, with wide field-of-view cameras, is considered. A dataset of crowd scenes with people annotations under a bird’s eye view (BEV) and ground truth for metric distances is introduced, and several measures for the evaluation of social distance detection systems are proposed. A multi-branch network, BEV-Net, is proposed to localize individuals in world coordinates and identify high-risk regions where social distancing is violated. BEV-Net combines detection of head and feet locations, camera pose estimation, a differentiable homography module to map image into BEV coordinates, and geometric reasoning to produce a BEV map of the people locations in the scene. Experiments on complex crowded scenes demonstrate the power of the approach and show superior performance over baselines derived from methods in the literature. Applications of interest for public health decision makers are finally discussed. Datasets, code and pretrained models are publicly available at GitHub1.","2380-7504","978-1-6654-2812-5","10.1109/ICCV48922.2021.00535","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9710671","Vision applications and systems","Visualization;Head;Annotations;Pose estimation;Human factors;Cameras;Social factors","","","","2","","67","IEEE","28 Feb 2022","","","IEEE","IEEE Conferences"
"Episodic Transformer for Vision-and-Language Navigation","A. Pashevich; C. Schmid; C. Sun",Inria; Google Research; Google Research,"2021 IEEE/CVF International Conference on Computer Vision (ICCV)","28 Feb 2022","2021","","","15922","15932","Interaction and navigation defined by natural language instructions in dynamic environments pose significant challenges for neural agents. This paper focuses on addressing two challenges: handling long sequence of subtasks, and understanding complex human instructions. We propose Episodic Transformer (E.T.), a multimodal transformer that encodes language inputs and the full episode history of visual observations and actions. To improve training, we leverage synthetic instructions as an intermediate representation that decouples understanding the visual appearance of an environment from the variations of natural language instructions. We demonstrate that encoding the history with a transformer is critical to solve compositional tasks, and that pretraining and joint training with synthetic instructions further improve the performance. Our approach sets a new state of the art on the challenging ALFRED benchmark, achieving 38.4% and 8.5% task success rates on seen and unseen test splits.","2380-7504","978-1-6654-2812-5","10.1109/ICCV48922.2021.01564","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9710429","Vision for robotics and autonomous vehicles;Vision + language","Training;Visualization;Navigation;Natural languages;Detectors;Benchmark testing;Transformers","","","","28","","74","IEEE","28 Feb 2022","","","IEEE","IEEE Conferences"
"DRIVE: Deep Reinforced Accident Anticipation with Visual Explanation","W. Bao; Q. Yu; Y. Kong","Golisano College of Computing and Information Sciences, Rochester Institute of Technology; Golisano College of Computing and Information Sciences, Rochester Institute of Technology; Golisano College of Computing and Information Sciences, Rochester Institute of Technology","2021 IEEE/CVF International Conference on Computer Vision (ICCV)","28 Feb 2022","2021","","","7599","7608","Traffic accident anticipation aims to accurately and promptly predict the occurrence of a future accident from dashcam videos, which is vital for a safety-guaranteed self-driving system. To encourage an early and accurate decision, existing approaches typically focus on capturing the cues of spatial and temporal context before a future accident occurs. However, their decision-making lacks visual explanation and ignores the dynamic interaction with the environment. In this paper, we propose Deep ReInforced accident anticipation with Visual Explanation, named DRIVE. The method simulates both the bottom-up and top-down visual attention mechanism in a dashcam observation environment so that the decision from the pro-posed stochastic multi-task agent can be visually explained by attentive regions. Moreover, the proposed dense anticipation reward and sparse fixation reward are effective in training the DRIVE model with our improved reinforcement learning algorithm. Experimental results show that the DRIVE model achieves state-of-the-art performance on multiple real-world traffic accident datasets. Code and pre-trained model are available at https://www.rit.edu/actionlab/drive.","2380-7504","978-1-6654-2812-5","10.1109/ICCV48922.2021.00752","Office of Naval Research; Army Research Office; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9710981","Video analysis and understanding;Machine learning architectures and formulations;Vision for robotics and autonomous vehicles","Training;Visualization;Computer vision;Heuristic algorithms;Decision making;Stochastic processes;Reinforcement learning","","","","6","","50","IEEE","28 Feb 2022","","","IEEE","IEEE Conferences"
"X-World: Accessibility, Vision, and Autonomy Meet","J. Zhang; M. Zheng; M. Boyd; E. Ohn-Bar",Boston University; Boston University; Boston University; Boston University,"2021 IEEE/CVF International Conference on Computer Vision (ICCV)","28 Feb 2022","2021","","","9742","9751","An important issue facing vision-based intelligent systems today is the lack of accessibility-aware development. A main reason for this issue is the absence of any large-scale, standardized vision benchmarks that incorporate relevant tasks and scenarios related to people with disabilities. This lack of representation hinders even preliminary analysis with respect to underlying pose, appearance, and occlusion characteristics of diverse pedestrians. What is the impact of significant occlusion from a wheelchair on instance segmentation quality? How can interaction with mobility aids, e.g., a long and narrow walking cane, be recognized robustly? To begin addressing such questions, we introduce X-World, an accessibility-centered development environment for vision-based autonomous systems. We tackle inherent data scarcity by leveraging a simulation environment to spawn dynamic agents with various mobility aids. The simulation supports generation of ample amounts of finely annotated, multi-modal data in a safe, cheap, and privacy-preserving manner. Our analysis highlights novel challenges introduced by our benchmark and tasks, as well as numerous opportunities for future developments. We further broaden our analysis using a complementary real-world evaluation benchmark of in-situ navigation by pedestrians with disabilities. Our contributions provide an initial step towards widespread deployment of vision-based agents that can perceive and model the interaction needs of diverse people with disabilities.","2380-7504","978-1-6654-2812-5","10.1109/ICCV48922.2021.00962","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9710390","Datasets and evaluation;Recognition and classification;Vision applications and systems;Vision for robotics and autonomous vehicles","Legged locomotion;Computer vision;Autonomous systems;Navigation;Wheelchairs;Transfer learning;Benchmark testing","","","","4","","60","IEEE","28 Feb 2022","","","IEEE","IEEE Conferences"
"Relational Prior for Multi-Object Tracking","A. Moskalev; I. Sosnovik; A. Smeulders","UvA - Bosch Delta Lab, University of Amsterdam; UvA - Bosch Delta Lab, University of Amsterdam; UvA - Bosch Delta Lab, University of Amsterdam","2021 IEEE/CVF International Conference on Computer Vision Workshops (ICCVW)","24 Nov 2021","2021","","","1081","1085","Tracking multiple objects individually differs from tracking groups of related objects. When an object is a part of the group, its trajectory is conditioned on the trajectories of the other group members. Most of the current state-of-the-art trackers follow the approach of tracking each object independently, with the mechanism to handle the overlapping trajectories where necessary. Such an approach does not take inter-object relations into account, which may cause unreliable tracking for the members of the groups, especially in crowded scenarios, where individual cues become unreliable. To overcome these limitations, we propose a plug-in Relation Encoding Module (REM). REM encodes relations between tracked objects by running a message passing over a spatio-temporal graph of tracked instances, computing the relation embeddings. The relation embeddings then serve as a prior for predicting future positions of the objects. Our experiments on MOT17 and MOT20 benchmarks demonstrate that extending a tracker with relational prior improves tracking quality.","2473-9944","978-1-6654-0191-3","10.1109/ICCVW54120.2021.00126","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9607526","","Computer vision;Message passing;Conferences;Benchmark testing;Encoding;Trajectory","graph theory;image sequences;message passing;object detection;object tracking;target tracking;tracking;video signal processing","multiobject tracking;tracking multiple objects;related objects;group members;current state-of-the-art trackers;overlapping trajectories;inter-object relations;unreliable tracking;individual cues;plug-in Relation Encoding Module;REM;tracked objects;tracked instances;relation embeddings","","","","24","IEEE","24 Nov 2021","","","IEEE","IEEE Conferences"
"Wanderlust: Online Continual Object Detection in the Real World","J. Wang; X. Wang; Y. Shang-Guan; A. Gupta","Carnegie Mellon University; Microsoft Research; University of Texas, Austin; Carnegie Mellon University","2021 IEEE/CVF International Conference on Computer Vision (ICCV)","28 Feb 2022","2021","","","10809","10818","Online continual learning from data streams in dynamic environments is a critical direction in the computer vision field. However, realistic benchmarks and fundamental studies in this line are still missing. To bridge the gap, we present a new online continual object detection benchmark with an egocentric video dataset, Objects Around Krishna (OAK). OAK adopts the KrishnaCAM videos, an ego-centric video stream collected over nine months by a graduate student. OAK provides exhaustive bounding box annotations of 80 video snippets (~17.5 hours) for 105 object categories in outdoor scenes. The emergence of new object categories in our benchmark follows a pattern similar to what a single person might see in their day-to-day life. The dataset also captures the natural distribution shifts as the person travels to different places. These egocentric long running videos provide a realistic playground for continual learning algorithms, especially in online embodied settings. We also introduce new evaluation metrics to evaluate the model performance and catastrophic forgetting and provide baseline studies for online continual object detection. We believe this benchmark will pose new exciting challenges for learning from non-stationary data in continual learning. The OAK dataset and the associated benchmark are released at https://oakdata.github.io/.","2380-7504","978-1-6654-2812-5","10.1109/ICCV48922.2021.01065","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9710595","Datasets and evaluation;Detection and localization in 2D and 3D","Measurement;Computer vision;Annotations;Object detection;Machine learning;Detectors;Benchmark testing","","","","14","","49","IEEE","28 Feb 2022","","","IEEE","IEEE Conferences"
"Great Ape Detection in Challenging Jungle Camera Trap Footage via Attention-Based Spatial and Temporal Feature Blending","X. Yang; M. Mirmehdi; T. Burghardt","Dept of Computer Science, University of Bristol, UK; Dept of Computer Science, University of Bristol, UK; Dept of Computer Science, University of Bristol, UK","2019 IEEE/CVF International Conference on Computer Vision Workshop (ICCVW)","5 Mar 2020","2019","","","255","262","We propose the first multi-frame video object detection framework trained to detect great apes. It is applicable to challenging camera trap footage in complex jungle environments and extends a traditional feature pyramid architecture by adding self-attention driven feature blending in both the spatial as well as the temporal domain. We demonstrate that this extension can detect distinctive species appearance and motion signatures despite significant partial occlusion. We evaluate the framework using 500 camera trap videos of great apes from the Pan African Programme containing 180K frames, which we manually annotated with accurate per-frame animal bounding boxes. These clips contain significant partial occlusions, challenging lighting, dynamic backgrounds, and natural camouflage effects. We show that our approach performs highly robustly and significantly outperforms frame-based detectors. We also perform detailed ablation studies and a validation on the full ILSVRC 2015 VID data corpus to demonstrate wider applicability at adequate performance levels. We conclude that the framework is ready to assist human camera trap inspection efforts. We publish code, weights, and ground truth annotations with this paper.","2473-9944","978-1-7281-5023-9","10.1109/ICCVW.2019.00034","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9022562","Wildlife Detection;Video Object Detection","Feature extraction;Object detection;Animals;Cameras;Task analysis;Detectors;Lighting","cameras;computer vision;feature extraction;image classification;image motion analysis;image segmentation;image sequences;learning (artificial intelligence);object detection;object recognition;video signal processing","jungle camera trap footage;temporal feature blending;multiframe video object detection framework;feature pyramid architecture;self-attention driven feature blending;temporal domain;motion signatures;camera trap videos;Pan African Programme;per-frame animal bounding boxes;frame-based detectors;spatial feature blending","","9","","31","IEEE","5 Mar 2020","","","IEEE","IEEE Conferences"
"Attentive and Contrastive Learning for Joint Depth and Motion Field Estimation","S. Lee; F. Rameau; F. Pan; I. S. Kweon",Korea Advanced Institute of Science and Technology (KAIST); Korea Advanced Institute of Science and Technology (KAIST); Korea Advanced Institute of Science and Technology (KAIST); Korea Advanced Institute of Science and Technology (KAIST),"2021 IEEE/CVF International Conference on Computer Vision (ICCV)","28 Feb 2022","2021","","","4842","4851","Estimating the motion of the camera together with the 3D structure of the scene from a monocular vision system is a complex task that often relies on the so-called scene rigidity assumption. When observing a dynamic environment, this assumption is violated which leads to an ambiguity between the ego-motion of the camera and the motion of the objects. To solve this problem, we present a self-supervised learning framework for 3D object motion field estimation from monocular videos. Our contributions are two-fold. First, we propose a two-stage projection pipeline to explicitly disentangle the camera ego-motion and the object motions with dynamics attention module, called DAM. Specifically, we design an integrated motion model that estimates the motion of the camera and object in the first and second warping stages, respectively, controlled by the attention module through a shared motion encoder. Second, we propose an object motion field estimation through contrastive sample consensus, called CSAC, taking advantage of weak semantic prior (bounding box from an object detector) and geometric constraints (each object respects the rigid body motion model). Experiments on KITTI, Cityscapes, and Waymo Open Dataset demonstrate the relevance of our approach and show that our method outperforms state-of-the-art algorithms for the tasks of self-supervised monocular depth estimation, object motion segmentation, monocular scene flow estimation, and visual odometry.","2380-7504","978-1-6654-2812-5","10.1109/ICCV48922.2021.00482","National Research Foundation; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9710937","Vision applications and systems;3D from a single image and shape-from-x;Stereo;3D from multiview and other sensors;Vision for robotics and autonomous vehicles","Three-dimensional displays;Dynamics;Semantics;Estimation;Cameras;Rigidity;Vehicle dynamics","","","","6","","45","IEEE","28 Feb 2022","","","IEEE","IEEE Conferences"
"MAAD: A Model and Dataset for ""Attended Awareness"" in Driving","D. Gopinath; G. Rosman; S. Stent; K. Terahata; L. Fletcher; B. Argall; J. Leonard","Department of Mechanical Engineering, Northwestern University, Evanston, Illinois; Toyota Research Institute, Cambridge, MA; Toyota Research Institute, Cambridge, MA; Woven Planet Holdings, Tokyo, Japan; Motional, Cambridge, MA; Department of Mechanical Engineering, Northwestern University, Evanston, Illinois; Toyota Research Institute, Cambridge, MA","2021 IEEE/CVF International Conference on Computer Vision Workshops (ICCVW)","24 Nov 2021","2021","","","3419","3429","We propose a computational model to estimate a person’s attended awareness of their environment. We define ""attended awareness"" to be those parts of a potentially dynamic scene which a person has attended to in recent history and which they are still likely to be physically aware of. Our model takes as input scene information in the form of a video and noisy gaze estimates, and outputs visual saliency, a refined gaze estimate and an estimate of the person’s attended awareness. In order to test our model, we capture a new dataset with a high-precision gaze tracker including 24.5 hours of gaze sequences from 23 subjects attending to videos of driving scenes. The dataset also contains third-party annotations of the subjects’ attended awareness based on observations of their scan path. Our results show that our model is able to reasonably estimate attended awareness in a controlled setting, and in the future could potentially be extended to real egocentric driving data to help enable more effective ahead-of-time warnings in safety systems and thereby augment driver performance. We also demonstrate our model’s effectiveness on the tasks of saliency, gaze calibration and denoising, using both our dataset and an existing saliency dataset. We make our model and dataset available at https://github.com/ToyotaResearchInstitute/att-aware/.","2473-9944","978-1-6654-0191-3","10.1109/ICCVW54120.2021.00382","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9607423","","Visualization;Computer vision;Computational modeling;Conferences;Noise reduction;Data models;Safety","driver information systems;gaze tracking;video signal processing","input scene information;high-precision gaze tracker;gaze sequences;driving scenes;gaze calibration;saliency dataset;MAAD;attended awareness;video","","2","","63","IEEE","24 Nov 2021","","","IEEE","IEEE Conferences"
"A System for Fusing Color and Near-Infrared Images in Radiance Domain","K. C. Ng; J. Shen; C. M. Ho","OPPO US Research Center, Palo Alto, CA; OPPO US Research Center, Palo Alto, CA; OPPO US Research Center, Palo Alto, CA","2021 IEEE/CVF International Conference on Computer Vision Workshops (ICCVW)","24 Nov 2021","2021","","","2021","2030","We designed and demonstrated a system that fused color and near-infrared (NIR) images in the radiance domain. The system is designed to enhance image quality captured in outdoor environments, especially in hazy weather conditions. Previous dehazing methods based on RGB-NIR fusion exist but have rarely addressed the issue of color fidelity and potential see-through effect of fusing with NIR image. The proposed system can dehaze and enhance image details while maintaining the color fidelity and protect privacy. By working in the radiance domain, the system could handle large brightness differences among the color and NIR images and achieve High Dynamic Range (HDR). We proposed two methods to correct the fusion color: linear scalings when raw images were used and color swapping with base-detail image decomposition in the presence of nonlinearity in the ISP pipeline. The system also had two clothing see-through prevention mechanisms to avoid ethical issue arising from the see-through effect of NIR image.","2473-9944","978-1-6654-0191-3","10.1109/ICCVW54120.2021.00229","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9607811","","Image quality;Privacy;Ethics;Image color analysis;Fuses;Clothing;Pipelines","image colour analysis;image enhancement;image fusion;infrared imaging","base-detail image decomposition;near-infrared imaging;radiance domain;dehazing methods;fusion color fidelity;RGB-NIR image fusion;hazy weather conditions;color swapping;ISP pipeline;see-through effect","","","","32","IEEE","24 Nov 2021","","","IEEE","IEEE Conferences"
"Few-Shot Learning With Embedded Class Models and Shot-Free Meta Training","A. Ravichandran; R. Bhotika; S. Soatto",Amazon Web Services; Amazon Web Services; Amazon Web Services and UCLA,"2019 IEEE/CVF International Conference on Computer Vision (ICCV)","27 Feb 2020","2019","","","331","339","We propose a method for learning embeddings for few-shot learning that is suitable for use with any number of shots (shot-free). Rather than fixing the class prototypes to be the Euclidean average of sample embeddings, we allow them to live in a higher-dimensional space (embedded class models) and learn the prototypes along with the model parameters. The class representation function is defined implicitly, which allows us to deal with a variable number of shots per class with a simple constant-size architecture. The class embedding encompasses metric learning, that facilitates adding new classes without crowding the class representation space. Despite being general and not tuned to the benchmark, our approach achieves state-of-the-art performance on the standard few-shot benchmark datasets.","2380-7504","978-1-7281-4803-8","10.1109/ICCV.2019.00042","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9009038","","Training;Benchmark testing;Measurement;Prototypes;Standards;Computational modeling;Web services","image representation;learning (artificial intelligence)","higher-dimensional space;embedded class models;model parameters;class representation function;class embedding;metric learning;class representation space;few-shot benchmark datasets;few-shot learning;shot-free meta training;learning embeddings;constant-size architecture","","85","","22","IEEE","27 Feb 2020","","","IEEE","IEEE Conferences"
"Monocular, One-stage, Regression of Multiple 3D People","Y. Sun; Q. Bao; W. Liu; Y. Fu; M. J. Black; T. Mei","Harbin Institute of Technology; JD AI Research; JD AI Research; Harbin Institute of Technology; Max Planck Institute for Intelligent Systems, Tübingen, Germany; JD AI Research","2021 IEEE/CVF International Conference on Computer Vision (ICCV)","28 Feb 2022","2021","","","11159","11168","This paper focuses on the regression of multiple 3D people from a single RGB image. Existing approaches predominantly follow a multi-stage pipeline that first detects people in bounding boxes and then independently regresses their 3D body meshes. In contrast, we propose to Regress all meshes in a One-stage fashion for Multiple 3D People (termed ROMP). The approach is conceptually simple, bounding box-free, and able to learn a per-pixel representation in an end-to-end manner. Our method simultaneously predicts a Body Center heatmap and a Mesh Parameter map, which can jointly describe the 3D body mesh on the pixel level. Through a body-center-guided sampling process, the body mesh parameters of all people in the image are easily extracted from the Mesh Parameter map. Equipped with such a fine-grained representation, our one-stage framework is free of the complex multi-stage process and more robust to occlusion. Compared with state-of-the-art methods, ROMP achieves superior performance on the challenging multi-person benchmarks, including 3DPW and CMU Panoptic. Experiments on crowded/occluded datasets demonstrate the robustness under various types of occlusion. The code, released at https://github.com/Arthur151/ROMP, is the first real-time implementation of monocular multi-person 3D mesh regression.","2380-7504","978-1-6654-2812-5","10.1109/ICCV48922.2021.01099","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9710639","Gestures and body pose","Heating systems;Solid modeling;Three-dimensional displays;Pipelines;Estimation;Benchmark testing;Real-time systems","","","","69","","60","IEEE","28 Feb 2022","","","IEEE","IEEE Conferences"
"Counting With Focus for Free","Z. Shi; P. Mettes; C. Snoek",University of Amsterdam; University of Amsterdam; University of Amsterdam,"2019 IEEE/CVF International Conference on Computer Vision (ICCV)","27 Feb 2020","2019","","","4199","4208","This paper aims to count arbitrary objects in images. The leading counting approaches start from point annotations per object from which they construct density maps. Then, their training objective transforms input images to density maps through deep convolutional networks. We posit that the point annotations serve more supervision purposes than just constructing density maps. We introduce ways to repurpose the points for free. First, we propose supervised focus from segmentation, where points are converted into binary maps. The binary maps are combined with a network branch and accompanying loss function to focus on areas of interest. Second, we propose supervised focus from global density, where the ratio of point annotations to image pixels is used in another branch to regularize the overall density estimation. To assist both the density estimation and the focus from segmentation, we also introduce an improved kernel size estimator for the point annotations. Experiments on six datasets show that all our contributions reduce the counting error, regardless of the base network, resulting in state-of-the-art accuracy using only a single network. Finally, we are the first to count on WIDER FACE, allowing us to show the benefits of our approach in handling varying object scales and crowding levels. Code is available at https://github.com/shizenglin/Counting-with-Focus-for-Free.","2380-7504","978-1-7281-4803-8","10.1109/ICCV.2019.00430","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9010851","","Image segmentation;Kernel;Training;Estimation;Task analysis;Optimization;Convolution","convolutional neural nets;image segmentation;network theory (graphs);object detection;supervised learning;tree searching","density maps;deep convolutional networks;point annotations;supervised focus;binary maps;network branch;image pixels;density estimation;segmentation;improved kernel size estimator","","61","","41","IEEE","27 Feb 2020","","","IEEE","IEEE Conferences"
"Multi-Instance Pose Networks: Rethinking Top-Down Pose Estimation","R. Khirodkar; V. Chari; A. Agrawal; A. Tyagi",Carnegie Mellon University; Amazon Lab 126; Amazon Lab 126; Amazon Lab 126,"2021 IEEE/CVF International Conference on Computer Vision (ICCV)","28 Feb 2022","2021","","","3102","3111","A key assumption of top-down human pose estimation approaches is their expectation of having a single person/instance present in the input bounding box. This often leads to failures in crowded scenes with occlusions. We propose a novel solution to overcome the limitations of this fundamental assumption. Our Multi-Instance Pose Network (MIPNet) allows for predicting multiple 2D pose instances within a given bounding box. We introduce a Multi-Instance Modulation Block (MIMB) that can adaptively modulate channel-wise feature responses for each instance and is parameter efficient. We demonstrate the efficacy of our approach by evaluating on COCO, CrowdPose, and OCHuman datasets. Specifically, we achieve 70.0 AP on CrowdPose and 42.5 AP on OCHuman test sets, a significant improvement of 2.4 AP and 6.5 AP over the prior art, respectively. When using ground truth bounding boxes for inference, MIP-Net achieves an improvement of 0.7 AP on COCO, 0.9 AP on CrowdPose, and 9.1 AP on OCHuman validation sets compared to HRNet. Interestingly, when fewer, high confidence bounding boxes are used, HRNet’s performance degrades (by 5 AP) on OCHuman, whereas MIPNet maintains a relatively stable performance (drop of 1 AP) for the same inputs.","2380-7504","978-1-6654-2812-5","10.1109/ICCV48922.2021.00311","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9710259","Detection and localization in 2D and 3D;Gestures and body pose;Machine learning architectures and formulations","Computer vision;Art;Pose estimation;Modulation","","","","36","","48","IEEE","28 Feb 2022","","","IEEE","IEEE Conferences"
"Pixel-Perfect Structure-from-Motion with Featuremetric Refinement","P. Lindenberger; P. -E. Sarlin; V. Larsson; M. Pollefeys","Departments of Mathematics, ETH Zurich; Departments of Computer Science, ETH Zurich; Departments of Computer Science, ETH Zurich; Departments of Computer Science, ETH Zurich","2021 IEEE/CVF International Conference on Computer Vision (ICCV)","28 Feb 2022","2021","","","5967","5977","Finding local features that are repeatable across multiple views is a cornerstone of sparse 3D reconstruction. The classical image matching paradigm detects keypoints per-image once and for all, which can yield poorly-localized features and propagate large errors to the final geometry. In this paper, we refine two key steps of structure-from-motion by a direct alignment of low-level image information from multiple views: we first adjust the initial keypoint locations prior to any geometric estimation, and subsequently refine points and camera poses as a post-processing. This refinement is robust to large detection noise and appearance changes, as it optimizes a featuremetric error based on dense features predicted by a neural network. This significantly improves the accuracy of camera poses and scene geometry for a wide range of keypoint detectors, challenging viewing conditions, and off-the-shelf deep features. Our system easily scales to large image collections, enabling pixel-perfect crowd-sourced localization at scale. Our code is publicly available at github.com/cvg/pixel-perfect-sfm as an add-on to the popular SfM software COLMAP.","2380-7504","978-1-6654-2812-5","10.1109/ICCV48922.2021.00593","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9710072","Stereo;3D from multiview and other sensors;Vision applications and systems;Vision for robotics and autonomous vehicles","Location awareness;Geometry;Visualization;Solid modeling;Three-dimensional displays;Estimation;Benchmark testing","","","","35","","92","IEEE","28 Feb 2022","","","IEEE","IEEE Conferences"
"Foreground-Aware Pyramid Reconstruction for Alignment-Free Occluded Person Re-Identification","H. Lingxiao; Y. Wang; W. Liu; H. Zhao; Z. Sun; J. Feng","casia; JD AI Research; JD AI Research; JD AI Research; CRIPAC & NLPR, CASIA; National University of Singapore","2019 IEEE/CVF International Conference on Computer Vision (ICCV)","27 Feb 2020","2019","","","8449","8458","Re-identifying a person across multiple disjoint camera views is important for intelligent video surveillance, smart retailing and many other applications. However, existing person re-identification methods are challenged by the ubiquitous occlusion over persons and suffer performance degradation. This paper proposes a novel occlusion-robust and alignment-free model for occluded person ReID and extends its application to realistic and crowded scenarios. The proposed model first leverages the fully convolution network (FCN) and pyramid pooling to extract spatial pyramid features. Then an alignment-free matching approach namely Foreground-aware Pyramid Reconstruction (FPR) is developed to accurately compute matching scores between occluded persons, regardless of their different scales and sizes. FPR uses the error from robust reconstruction over spatial pyramid features to measure similarities between two persons. More importantly, we design a occlusion-sensitive foreground probability generator that focuses more on clean human body parts to robustify the similarity computation with less contamination from occlusion. The FPR is easily embedded into any end-to-end person ReID models. The effectiveness of the proposed method is clearly demonstrated by the experimental results (Rank-1 accuracy) on three occluded person datasets: Partial REID (78.30%), Partial iLIDS (68.08%), Occluded REID (81.00%), and three benchmark person datasets: Market1501 (95.42%), DukeMTMC (88.64%), CUHK03 (76.08%).","2380-7504","978-1-7281-4803-8","10.1109/ICCV.2019.00854","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9010027","","Image reconstruction;Feature extraction;Computational modeling;Generators;Probes;Cameras;Convolution","cameras;feature extraction;image matching;probability;video signal processing;video surveillance","Foreground-aware Pyramid Reconstruction;FPR;robust reconstruction;spatial pyramid features;occlusion-sensitive foreground probability generator;end-to-end person ReID models;occluded person datasets;benchmark person datasets;multiple disjoint camera views;intelligent video surveillance;re-identification methods;ubiquitous occlusion;occlusion-robust;alignment-free model;occluded person ReID;pyramid pooling;alignment-free matching approach","","24","","23","IEEE","27 Feb 2020","","","IEEE","IEEE Conferences"
"Making Higher Order MOT Scalable: An Efficient Approximate Solver for Lifted Disjoint Paths","A. Hornakova; T. Kaiser; P. Swoboda; M. Rolinek; B. Rosenhahn; R. Henschel","Saarland Informatics Campus, Max Planck Institute for Informatics; Leibniz University Hannover, Institute for Information Processing; Saarland Informatics Campus, Max Planck Institute for Informatics; Max Planck Institute for Intelligent Systems, Tübingen; Leibniz University Hannover, Institute for Information Processing; Leibniz University Hannover, Institute for Information Processing","2021 IEEE/CVF International Conference on Computer Vision (ICCV)","28 Feb 2022","2021","","","6310","6320","We present an efficient approximate message passing solver for the lifted disjoint paths problem (LDP), a natural but NP-hard model for multiple object tracking (MOT). Our tracker scales to very large instances that come from long and crowded MOT sequences. Our approximate solver enables us to process the MOT15/16/17 benchmarks without sacrificing solution quality and allows for solving MOT20, which has been out of reach up to now for LDP solvers due to its size and complexity. On all these four standard MOT benchmarks we achieve performance comparable or better than current state-of-the-art methods including a tracker based on an optimal LDP solver.","2380-7504","978-1-6654-2812-5","10.1109/ICCV48922.2021.00627","Ministry of Education; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9710859","Optimization and learning methods;Motion and tracking","Computer vision;Costs;Message passing;Computational modeling;Benchmark testing;Complexity theory;Object tracking","","","","20","","69","IEEE","28 Feb 2022","","","IEEE","IEEE Conferences"
"Image Retrieval on Real-life Images with Pre-trained Vision-and-Language Models","Z. Liu; C. Rodriguez-Opazo; D. Teney; S. Gould","Australian National University; Australian Institute for Machine Learning, University of Adelaide; Australian Institute for Machine Learning, University of Adelaide; Australian National University","2021 IEEE/CVF International Conference on Computer Vision (ICCV)","28 Feb 2022","2021","","","2105","2114","We extend the task of composed image retrieval, where an input query consists of an image and short textual description of how to modify the image. Existing methods have only been applied to non-complex images within narrow domains, such as fashion products, thereby limiting the scope of study on in-depth visual reasoning in rich image and language contexts. To address this issue, we collect the Compose Image Retrieval on Real-life images (CIRR) dataset, which consists of over 36,000 pairs of crowd-sourced, open-domain images with human-generated modifying text. To extend current methods to the open-domain, we propose CIRPLANT, a transformer based model that leverages rich pre-trained vision-and-language (V&L) knowledge for modifying visual features conditioned on natural language. Retrieval is then done by nearest neighbor lookup on the modified features. We demonstrate that with a relatively simple architecture, CIRPLANT outperforms existing methods on open-domain images, while matching state-of-the-art accuracy on the existing narrow datasets, such as fashion. Together with the release of CIRR, we believe this work will inspire further research on composed image retrieval. Our dataset, code and pre-trained models are available at https://cuberick-orion.github.io/CIRR/.","2380-7504","978-1-6654-2812-5","10.1109/ICCV48922.2021.00213","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9710082","Vision + language;Datasets and evaluation;Image and video retrieval;Representation learning","Visualization;Computer vision;Limiting;Codes;Image retrieval;Natural languages;Computer architecture","","","","19","","42","IEEE","28 Feb 2022","","","IEEE","IEEE Conferences"
"Unidentified Video Objects: A Benchmark for Dense, Open-World Segmentation","W. Wang; M. Feiszli; H. Wang; D. Tran",Facebook AI Research; Facebook AI Research; Facebook AI Research; Facebook AI Research,"2021 IEEE/CVF International Conference on Computer Vision (ICCV)","28 Feb 2022","2021","","","10756","10765","Current state-of-the-art object detection and segmentation methods work well under the closed-world assumption. This closed-world setting assumes that the list of object categories is available during training and deployment. However, many real-world applications require detecting or segmenting novel objects, i.e., object categories never seen during training. In this paper, we present, UVO (Unidentified Video Objects), a new benchmark for openworld class-agnostic object segmentation in videos. Besides shifting the focus to the open-world setup, UVO is significantly larger, providing approximately 6 times more videos compared with DAVIS, and 7 times more mask (instance) annotations per video compared with YouTube-VO(I)S. UVO is also more challenging as it includes many videos with crowded scenes and complex background motions. We also demonstrated that UVO can be used for other applications, such as object tracking and super-voxel segmentation. We believe that UVO is a versatile testbed for researchers to develop novel approaches for open-world class-agnostic object segmentation, and inspires new research directions towards a more comprehensive video understanding beyond classification and detection.","2380-7504","978-1-6654-2812-5","10.1109/ICCV48922.2021.01060","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9710887","Datasets and evaluation;Detection and localization in 2D and 3D;Motion and tracking;Segmentation;grouping and shape;Video analysis and understanding","Training;Computer vision;Annotations;Motion segmentation;Computational modeling;Object segmentation;Object detection","","","","18","","53","IEEE","28 Feb 2022","","","IEEE","IEEE Conferences"
"YouRefIt: Embodied Reference Understanding with Language and Gesture","Y. Chen; Q. Li; D. Kong; Y. L. Kei; S. -C. Zhu; T. Gao; Y. Zhu; S. Huang","University of California, Los Angeles; University of California, Los Angeles; University of California, Los Angeles; University of California, Los Angeles; Beijing Institute for General Artificial Intelligence; University of California, Los Angeles; Beijing Institute for General Artificial Intelligence; University of California, Los Angeles","2021 IEEE/CVF International Conference on Computer Vision (ICCV)","28 Feb 2022","2021","","","1365","1375","We study the machine’s understanding of embodied reference: One agent uses both language and gesture to refer to an object to another agent in a shared physical environment. Of note, this new visual task requires understanding multimodal cues with perspective-taking to identify which object is being referred to. To tackle this problem, we introduce YouRefIt, a new crowd-sourced dataset of embodied reference collected in various physical scenes; the dataset contains 4,195 unique reference clips in 432 indoor scenes. To the best of our knowledge, this is the first embodied reference dataset that allows us to study referring expressions in daily physical scenes to understand referential behavior, human communication, and human-robot interaction. We further devise two benchmarks for image-based and video-based embodied reference understanding. Comprehensive baselines and extensive experiments provide the very first result of machine perception on how the referring expressions and gestures affect the embodied reference understanding. Our results provide essential evidence that gestural cues are as critical as language cues in understanding the embodied reference.","2380-7504","978-1-6654-2812-5","10.1109/ICCV48922.2021.00142","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9711427","Vision + language;Scene analysis and understanding","Visualization;Computer vision;Human-robot interaction;Benchmark testing;Object recognition;Task analysis","","","","6","","69","IEEE","28 Feb 2022","","","IEEE","IEEE Conferences"
"CrowdDriven: A New Challenging Dataset for Outdoor Visual Localization","A. Jafarzadeh; M. L. Antequera; P. Gargallo; Y. Kuang; C. Toft; F. Kahl; T. Sattler",Chalmers University of Technology; Facebook; Facebook; Facebook; Chalmers University of Technology; Chalmers University of Technology; Czech Technical University in Prague,"2021 IEEE/CVF International Conference on Computer Vision (ICCV)","28 Feb 2022","2021","","","9825","9835","Visual localization is the problem of estimating the position and orientation from which a given image (or a sequence of images) is taken in a known scene. It is an important part of a wide range of computer vision and robotics applications, from self-driving cars to augmented/virtual reality systems. Visual localization techniques should work reliably and robustly under a wide range of conditions, including seasonal, weather, illumination and man-made changes. Recent benchmarking efforts model this by providing images under different conditions, and the community has made rapid progress on these datasets since their inception. However, they are limited to a few geographical regions and often recorded with a single device. We propose a new benchmark for visual localization in outdoor scenes, using crowd-sourced data to cover a wide range of geographical regions and camera devices with a focus on the failure cases of current algorithms. Experiments with state-of-the-art localization approaches show that our dataset is very challenging, with all evaluated methods failing on its hardest parts. As part of the dataset release, we provide the tooling used to generate it, enabling efficient and effective 2D correspondence annotation to obtain reference poses.","2380-7504","978-1-6654-2812-5","10.1109/ICCV48922.2021.00970","European Regional Development Fund; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9710297","Datasets and evaluation;Stereo;3D from multiview and other sensors;Vision for robotics and autonomous vehicles","Location awareness;Visualization;Computer vision;Annotations;Computational modeling;Pipelines;Lighting","","","","5","","86","IEEE","28 Feb 2022","","","IEEE","IEEE Conferences"
"A Machine Teaching Framework for Scalable Recognition","P. Wang; N. Vasconcelos","UC, San Diego; UC, San Diego","2021 IEEE/CVF International Conference on Computer Vision (ICCV)","28 Feb 2022","2021","","","4925","4934","We consider the scalable recognition problem in the fine-grained expert domain where large-scale data collection is easy whereas annotation is difficult. Existing solutions are typically based on semi-supervised or self-supervised learning. We propose an alternative new framework, MEMORABLE, based on machine teaching and online crowd-sourcing platforms. A small amount of data is first labeled by experts and then used to teach online annotators for the classes of interest, who finally label the entire dataset. Preliminary studies show that the accuracy of classifiers trained on the final dataset is a function of the accuracy of the student annotators. A new machine teaching algorithm, CMaxGrad, is then proposed to enhance this accuracy by introducing explanations in a state-of-the-art machine teaching algorithm. For this, CMaxGrad leverages counterfactual explanations, which take into account student predictions, thereby proving feedback that is student-specific, explicitly addresses the causes of student confusion, and adapts to the level of competence of the student. Experiments show that both MEMORABLE and CMaxGrad outperform existing solutions to their respective problems.","2380-7504","978-1-6654-2812-5","10.1109/ICCV48922.2021.00490","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9710974","Vision applications and systems;Explainable AI;Transfer/Low-shot/Semi/Unsupervised Learning","Crowdsourcing;Computer vision;Costs;Education;Pipelines;Data collection;Prediction algorithms","","","","4","","49","IEEE","28 Feb 2022","","","IEEE","IEEE Conferences"
"SDVTracker: Real-Time Multi-Sensor Association and Tracking for Self-Driving Vehicles","S. Gautam; G. P. Meyer; C. Vallespi-Gonzalez; B. C. Becker",Aurora Innovation Inc; Motional; Aurora Innovation Inc; Aurora Innovation Inc,"2021 IEEE/CVF International Conference on Computer Vision Workshops (ICCVW)","24 Nov 2021","2021","","","3012","3021","Accurate motion state estimation of Vulnerable Road Users (VRUs), is a critical requirement for autonomous vehicles that navigate in urban environments. Due to their computational efficiency, many traditional autonomy systems perform multi-object tracking using Kalman Filters which frequently rely on hand-engineered association. However, such methods fail to generalize to crowded scenes and multi-sensor modalities, often resulting in poor state estimates which cascade to inaccurate predictions. We present a practical and lightweight tracking system, SDV-Tracker, that uses a deep learned model for association and state estimation in conjunction with an Interacting Multiple Model (IMM) filter. The proposed tracking method is fast, robust and generalizes across multiple sensor modalities and different VRU classes. In this paper, we detail a model that jointly optimizes both association and state estimation with a novel loss, an algorithm for determining ground-truth supervision, and a training procedure. We show this system significantly outperforms hand-engineered methods on a real-world urban driving dataset while running in less than 2.5 ms on CPU for a scene with 100 actors, making it suitable for self-driving applications where low latency and high accuracy is critical.","2473-9944","978-1-6654-0191-3","10.1109/ICCVW54120.2021.00336","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9607432","","Training;Tracking;Navigation;Roads;Urban areas;Filtering algorithms;Real-time systems","deep learning (artificial intelligence);Kalman filters;mobile robots;object detection;object tracking;road safety;road vehicles;sensor fusion;state estimation","autonomous vehicles;urban environments;computational efficiency;multiobject tracking;kalman filters;hand-engineered association;multisensor modalities;lightweight tracking system;deep learned model;interacting multiple model filter;real-world urban driving dataset;real-time multisensor association;self-driving vehicles;motion state estimation;vulnerable road users","","3","","38","IEEE","24 Nov 2021","","","IEEE","IEEE Conferences"
"Body-Face Joint Detection via Embedding and Head Hook","J. Wan; J. Deng; X. Qiu; F. Zhou","Algorithm Research, Aibee Inc.; Algorithm Research, Aibee Inc.; Algorithm Research, Aibee Inc.; Algorithm Research, Aibee Inc.","2021 IEEE/CVF International Conference on Computer Vision (ICCV)","28 Feb 2022","2021","","","2939","2948","Detecting pedestrians and their associated faces jointly is a challenging task. On one hand, body or face could be absent because of occlusion or non-frontal human pose. On the other hand, the association becomes difficult or even miss-leading in crowded scenes due to the lack of strong correlational evidence. This paper proposes Body-Face Joint (BFJ) detector, a novel framework for detecting bodies and their faces with accurate correspondance. We follow the classical multi-class detector design by detecting body and face in parallel but with two key contributions. First, we propose an Embedding Matching Loss (EML) to learn an associative embedding for matching body and face of the same person. Second, we introduce a novel concept, ""head hook"", to bridge the gap of matching body and faces spatially. With the new semantical and geometrical sources of information, BFJ greatly reduces the difficulty of detecting body and face in pairs. Since the problem is unexplored yet, we design a new metric named log-average miss matching rate (mMR−2) to evaluate the association performance and extend the CrowdHuman and CityPersons benchmarks by annotating each face box. Experiments show that our BFJ detector can maintain state-of-the-art performance in pedestrian detection on both one-stage and two-stage structures while greatly outperform various body-face association strategies. Code will be available at: https://github.com/AibeeDetect/BFJDet.","2380-7504","978-1-6654-2812-5","10.1109/ICCV48922.2021.00295","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9711464","Detection and localization in 2D and 3D;Datasets and evaluation;Recognition and classification","Measurement;Computer vision;Codes;Detectors;Object detection;Benchmark testing;Automobiles","","","","1","","41","IEEE","28 Feb 2022","","","IEEE","IEEE Conferences"
"When Pigs Fly: Contextual Reasoning in Synthetic and Natural Scenes","P. Bomatter; M. Zhang; D. Karev; S. Madan; C. Tseng; G. Kreiman","ETH Zürich; Harvard Medical School, Children’s Hospital; Harvard College, Harvard University; Center for Brains, Minds and Machines; Harvard College, Harvard University; Harvard Medical School, Children’s Hospital","2021 IEEE/CVF International Conference on Computer Vision (ICCV)","28 Feb 2022","2021","","","255","264","Context is of fundamental importance to both human and machine vision; e.g., an object in the air is more likely to be an airplane than a pig. The rich notion of context incorporates several aspects including physics rules, statistical co-occurrences, and relative object sizes, among others. While previous work has focused on crowd-sourced out-of-context photographs from the web to study scene context, controlling the nature and extent of contextual violations has been a daunting task. Here we introduce a diverse, synthetic Out-of-Context Dataset (OCD) with fine-grained control over scene context. By leveraging a 3D simulation engine, we systematically control the gravity, object co-occurrences and relative sizes across 36 object categories in a virtual household environment. We conducted a series of experiments to gain insights into the impact of contextual cues on both human and machine vision using OCD. We conducted psychophysics experiments to establish a human benchmark for out-of-context recognition, and then compared it with state-of-the-art computer vision models to quantify the gap between the two. We propose a context-aware recognition transformer model, fusing object and contextual information via multi-head attention. Our model captures useful information for contextual reasoning, enabling human-level performance and better robustness in out-of-context conditions compared to baseline models across OCD and other out-of-context datasets. All source code and data are publicly available at https://github.com/kreimanlab/WhenPigsFlyContext","2380-7504","978-1-6654-2812-5","10.1109/ICCV48922.2021.00032","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9710269","Recognition and classification;Scene analysis and understanding","Visualization;Computer vision;Solid modeling;Three-dimensional displays;Computational modeling;Transformers;Cognition","","","","1","","39","IEEE","28 Feb 2022","","","IEEE","IEEE Conferences"
"UVStyle-Net: Unsupervised Few-shot Learning of 3D Style Similarity Measure for B-Reps","P. Meltzer; H. Shayani; A. Khasahmadi; P. K. Jayaraman; A. Sanghi; J. Lambourne",Autodesk AI Lab; Autodesk AI Lab; Autodesk AI Lab; Autodesk Research; Autodesk AI Lab; Autodesk AI Lab,"2021 IEEE/CVF International Conference on Computer Vision (ICCV)","28 Feb 2022","2021","","","9670","9679","Boundary Representations (B-Reps) are the industry standard in 3D Computer Aided Design/Manufacturing (CAD/CAM) and industrial design due to their fidelity in representing stylistic details. However, they have been ignored in the 3D style research. Existing 3D style metrics typically operate on meshes or point clouds, and fail to account for end-user subjectivity by adopting fixed definitions of style, either through crowd-sourcing for style labels or hand-crafted features. We propose UVStyle-Net, a style similarity measure for B-Reps that leverages the style signals in the second order statistics of the activations in a pre-trained (unsupervised) 3D encoder, and learns their relative importance to a subjective end-user through few-shot learning. Our approach differs from all existing data-driven 3D style methods since it may be used in completely unsupervised settings, which is desirable given the lack of publicly available labeled B-Rep datasets. More importantly, the few-shot learning accounts for the inherent subjectivity associated with style. We show quantitatively that our proposed method with B-Reps is able to capture stronger style signals than alternative methods on meshes and point clouds despite its significantly greater computational efficiency. We also show it is able to generate meaningful style gradients with respect to the input shape, and that few-shot learning with as few as two positive examples selected by an end-user is sufficient to significantly improve the style measure. Finally, we demonstrate its efficacy on a large unlabeled public dataset of CAD models. Source code and data are available at github.com/AutodeskAILab/UVStyle-Net.","2380-7504","978-1-6654-2812-5","10.1109/ICCV48922.2021.00955","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9710940","Representation learning;Transfer/Low-shot/Semi/Unsupervised Learning;Vision applications and systems","Point cloud compression;Industries;Solid modeling;Computer vision;Three-dimensional displays;Shape;Shape measurement","","","","","","38","IEEE","28 Feb 2022","","","IEEE","IEEE Conferences"
"LookOut: Diverse Multi-Future Prediction and Planning for Self-Driving","A. Cui; S. Casas; A. Sadat; R. Liao; R. Urtasun",Waabi; Waabi; Waabi; Google Brain; Waabi,"2021 IEEE/CVF International Conference on Computer Vision (ICCV)","28 Feb 2022","2021","","","16087","16096","In this paper, we present LOOKOUT, a novel autonomy system that perceives the environment, predicts a diverse set of futures of how the scene might unroll and estimates the trajectory of the SDV by optimizing a set of contingency plans over these future realizations. In particular, we learn a diverse joint distribution over multi-agent future trajectories in a traffic scene that covers a wide range of future modes with high sample efficiency while leveraging the expressive power of generative models. Unlike previous work in diverse motion forecasting, our diversity objective explicitly rewards sampling future scenarios that require distinct reactions from the self-driving vehicle for improved safety. Our contingency planner then finds comfortable and non-conservative trajectories that ensure safe reactions to a wide range of future scenarios. Through extensive evaluations, we show that our model demonstrates significantly more diverse and sample-efficient motion forecasting in a large-scale self-driving dataset as well as safer and less-conservative motion plans in long-term closed-loop simulations when compared to current state-of-the-art models.","2380-7504","978-1-6654-2812-5","10.1109/ICCV48922.2021.01580","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9710950","Vision for robotics and autonomous vehicles;Action and behavior recognition;Motion and tracking;Scene analysis and understanding;Vision applications and systems","Computer vision;Computational modeling;Decision making;Predictive models;Probability distribution;Trajectory;Planning","","","","35","","53","IEEE","28 Feb 2022","","","IEEE","IEEE Conferences"
"RAIN: Reinforced Hybrid Attention Inference Network for Motion Forecasting","J. Li; F. Yang; H. Ma; S. Malla; M. Tomizuka; C. Choi","Honda Research Institute, USA; Carnegie Mellon University; University of California, Berkeley; Honda Research Institute, USA; University of California, Berkeley; Honda Research Institute, USA","2021 IEEE/CVF International Conference on Computer Vision (ICCV)","28 Feb 2022","2021","","","16076","16086","Motion forecasting plays a significant role in various domains (e.g., autonomous driving, human-robot interaction), which aims to predict future motion sequences given a set of historical observations. However, the observed elements may be of different levels of importance. Some information may be irrelevant or even distracting to the forecasting in certain situations. To address this issue, we propose a generic motion forecasting framework (named RAIN) with dynamic key information selection and ranking based on a hybrid attention mechanism. The general framework is instantiated to handle multi-agent trajectory prediction and human motion forecasting tasks, respectively. In the former task, the model learns to recognize the relations between agents with a graph representation and to determine their relative significance. In the latter task, the model learns to capture the temporal proximity and dependency in long-term human motions. We also propose an effective double-stage training pipeline with an alternating training strategy to optimize the parameters in different modules of the framework. We validate the framework on both synthetic simulations and motion forecasting benchmarks in different domains, demonstrating that our method not only achieves state-of-the-art forecasting performance, but also provides interpretable and reasonable hybrid attention weights.","2380-7504","978-1-6654-2812-5","10.1109/ICCV48922.2021.01579","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9710777","Vision for robotics and autonomous vehicles;Motion and tracking;Scene analysis and understanding;Vision applications and systems","Training;Measurement;Rain;Pipelines;Human-robot interaction;Reinforcement learning;Predictive models","","","","16","","64","IEEE","28 Feb 2022","","","IEEE","IEEE Conferences"
