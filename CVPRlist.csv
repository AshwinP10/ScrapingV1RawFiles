"Document Title",Authors,"Author Affiliations","Publication Title",Date Added To Xplore,"Publication Year","Volume","Issue","Start Page","End Page","Abstract","ISSN",ISBNs,"DOI",Funding Information,PDF Link,"Author Keywords","IEEE Terms","INSPEC Controlled Terms","INSPEC Non-Controlled Terms","Mesh_Terms",Article Citation Count,Patent Citation Count,"Reference Count","License",Online Date,Issue Date,"Meeting Date","Publisher",Document Identifier
"Classifying Group Emotions for Socially-Aware Autonomous Vehicle Navigation","A. Bera; T. Randhavane; A. Wang; D. Manocha; E. Kubin; K. Gray","Department of Computer Science, University of North Carolina, Chapel Hill; Department of Computer Science, University of North Carolina, Chapel Hill; Department of Computer Science, University of North Carolina, Chapel Hill; Department of Computer Science, University of North Carolina, Chapel Hill; Department of Psychology and Neuroscience, University of North Carolina, Chapel Hill; Department of Psychology and Neuroscience, University of North Carolina, Chapel Hill","2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)","16 Dec 2018","2018","","","1152","11528","We present a real-time, data-driven algorithm to enhance the social-invisibility of autonomous robot navigation within crowds. Our approach is based on prior psychological research, which reveals that people notice and-importantly-react negatively to groups of social actors when they have negative group emotions or entitativity, moving in a tight group with similar appearances and trajectories. In order to evaluate that behavior, we performed a user study to develop navigational algorithms that minimize emotional reactions. This study establishes a mapping between emotional reactions and multi-robot trajectories and appearances and further generalizes the finding across various environmental conditions. We demonstrate the applicability of our approach for trajectory computation for active navigation and dynamic intervention in simulated autonomous robot-human interaction scenarios. Our approach empirically shows that various levels of emotional autonomous robots can be used to both avoid and influence pedestrians while not eliciting strong emotional reactions, giving multi-robot systems socially-invisibility.","2160-7516","978-1-5386-6100-0","10.1109/CVPRW.2018.00151","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8575305","","Navigation;Trajectory;Psychology;Computational modeling;Autonomous robots;Heuristic algorithms","human-robot interaction;mobile robots;multi-robot systems;path planning","simulated autonomous robot-human interaction scenarios;emotional autonomous robots;strong emotional reactions;multirobot systems socially-invisibility;socially-aware autonomous vehicle navigation;data-driven algorithm;social-invisibility;autonomous robot navigation;social actors;negative group emotions;tight group;similar appearances;user study;navigational algorithms;multirobot trajectories;trajectory computation;active navigation;psychological research","","6","","41","IEEE","16 Dec 2018","","","IEEE","IEEE Conferences"
"JRDB-Pose: A Large-Scale Dataset for Multi-Person Pose Estimation and Tracking","E. Vendrow; D. T. Le; J. Cai; H. Rezatofighi",Stanford University; Monash University; Monash University; Monash University,"2023 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)","22 Aug 2023","2023","","","4811","4820","Autonomous robotic systems operating in human environments must understand their surroundings to make accurate and safe decisions. In crowded human scenes with close-up human-robot interaction and robot navigation, a deep understanding of surrounding people requires reasoning about human motion and body dynamics over time with human body pose estimation and tracking. However, existing datasets captured from robot platforms either do not provide pose annotations or do not reflect the scene distribution of social robots. In this paper, we introduce JRDB-Pose, a large-scale dataset and benchmark for multi-person pose estimation and tracking. JRDB-Pose extends the existing JRDB which includes videos captured from a social navigation robot in a university campus environment, containing challenging scenes with crowded indoor and outdoor locations and a diverse range of scales and occlusion types. JRDB-Pose provides human pose annotations with per-keypoint occlusion labels and track IDs consistent across the scene and with existing annotations in JRDB. We conduct a thorough experimental study of state-of-the-art multi-person pose estimation and tracking methods on JRDB-Pose, showing that our dataset imposes new challenges for the existing methods. JRDB-Pose is available at https://jrdb.erc.monash.edu/.","2575-7075","979-8-3503-0129-8","10.1109/CVPR52729.2023.00466","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10204595","Humans: Face;body;pose;gesture;movement","Measurement;Computer vision;Annotations;Navigation;Tracking;Pose estimation;Social robots","human-robot interaction;mobile robots;path planning;pose estimation;video signal processing","autonomous robotic systems;body dynamics;crowded human scenes;human body;human environments;human motion;human-robot interaction;JRDB-Pose;large-scale dataset;multiperson pose estimation;robot navigation;social navigation robot","","","","57","IEEE","22 Aug 2023","","","IEEE","IEEE Conferences"
"Robust Neural Routing Through Space Partitions for Camera Relocalization in Dynamic Indoor Environments","S. Dong; Q. Fan; H. Wang; J. Shi; L. Yi; T. Funkhouser; B. Chen; L. Guibas",Shandong University; Stanford University; Stanford University; Peking University; Google Research; Google Research; Peking University; Stanford University,"2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)","2 Nov 2021","2021","","","8540","8550","Localizing the camera in a known indoor environment is a key building block for scene mapping, robot navigation, AR, etc. Recent advances estimate the camera pose via optimization over the 2D/3D-3D correspondences established between the coordinates in 2D/3D camera space and 3D world space. Such a mapping is estimated with either a convolution neural network or a decision tree using only the static input image sequence, which makes these approaches vulnerable to dynamic indoor environments that are quite common yet challenging in the real world. To address the aforementioned issues, in this paper, we propose a novel outlier-aware neural tree which bridges the two worlds, deep learning and decision tree approaches. It builds on three important blocks: (a) a hierarchical space partition over the indoor scene to construct the decision tree; (b) a neural routing function, implemented as a deep classification network, employed for better 3D scene understanding; and (c) an outlier rejection module used to filter out dynamic points during the hierarchical routing process. Our proposed algorithm is evaluated on the RIO-10 benchmark developed for camera relocalization in dynamic indoor environments. It achieves robust neural routing through space partitions and outperforms the state-of-the-art approaches by around 30% on camera pose accuracy, while running comparably fast for evaluation.","2575-7075","978-1-6654-4509-2","10.1109/CVPR46437.2021.00844","National Science Foundation; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9577932","","Three-dimensional displays;Heuristic algorithms;Robot kinematics;Robot vision systems;Cameras;Routing;Indoor environment","cameras;decision trees;feature extraction;image matching;image reconstruction;image sequences;learning (artificial intelligence);mobile robots;neural nets;object detection;optimisation;path planning;pattern classification;pose estimation;robot vision","robust neural routing;space partitions;camera relocalization;dynamic indoor environments;known indoor environment;scene mapping;convolution neural network;decision tree;static input image sequence;novel outlier-aware neural tree;hierarchical space partition;indoor scene;neural routing function;3D scene understanding;hierarchical routing process","","8","","58","IEEE","2 Nov 2021","","","IEEE","IEEE Conferences"
"Multi-Agent Tensor Fusion for Contextual Trajectory Prediction","T. Zhao; Y. Xu; M. Monfort; W. Choi; C. Baker; Y. Zhao; Y. Wang; Y. N. Wu",Peking University; ISEE.AI; ISEE.AI; ISEE.AI; ISEE.AI; ISEE.AI; Peking University; ISEE.AI,"2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)","9 Jan 2020","2019","","","12118","12126","Accurate prediction of others' trajectories is essential for autonomous driving. Trajectory prediction is challenging because it requires reasoning about agents' past movements, social interactions among varying numbers and kinds of agents, constraints from the scene context, and the stochasticity of human behavior. Our approach models these interactions and constraints jointly within a novel Multi-Agent Tensor Fusion (MATF) network. Specifically, the model encodes multiple agents' past trajectories and the scene context into a Multi-Agent Tensor, then applies convolutional fusion to capture multiagent interactions while retaining the spatial structure of agents and the scene context. The model decodes recurrently to multiple agents' future trajectories, using adversarial loss to learn stochastic predictions. Experiments on both highway driving and pedestrian crowd datasets show that the model achieves state-of-the-art prediction accuracy.","2575-7075","978-1-7281-3293-8","10.1109/CVPR.2019.01240","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8953520","Motion and Tracking;Deep Learning ; Robotics + Driving; Scene Analysis and Understanding; Vision Applications and System","","image capture;image fusion;intelligent transportation systems;learning (artificial intelligence);multi-agent systems;stochastic processes;tensors","contextual trajectory prediction;autonomous driving;social interactions;scene context;human behavior;convolutional fusion;multiagent interactions;stochastic predictions;pedestrian crowd datasets;prediction accuracy;multiagent tensor fusion network","","205","1","32","IEEE","9 Jan 2020","","","IEEE","IEEE Conferences"
"Boosting Detection in Crowd Analysis via Underutilized Output Features","S. Wu; F. Yang",Jilin University; University of Michigan,"2023 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)","22 Aug 2023","2023","","","15609","15618","Detection-based methods have been viewed unfavorably in crowd analysis due to their poor performance in dense crowds. However, we argue that the potential of these methods has been underestimated, as they offer crucial information for crowd analysis that is often ignored. Specifically, the area size and confidence score of output proposals and bounding boxes provide insight into the scale and density of the crowd. To leverage these underutilized features, we propose Crowd Hat, a plug-and-play module that can be easily integrated with existing detection models. This module uses a mixed 2D-1D compression technique to refine the output features and obtain the spatial and numerical distribution of crowd-specific information. Based on these features, we further propose region-adaptive NMS thresholds and a decouple-then-align paradigm that address the major limitations of detection-based methods. Our extensive evaluations on various crowd analysis tasks, including crowd counting, localization, and detection, demonstrate the effectiveness of utilizing output features and the potential of detection-based methods in crowd analysis. Our code is available at https://github.com/wskingdom/Crowd-Hat.","2575-7075","979-8-3503-0129-8","10.1109/CVPR52729.2023.01498","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10203588","Humans: Face;body;pose;gesture;movement","Location awareness;Computer vision;Codes;Computational modeling;Pipelines;Feature extraction;Pattern recognition","data compression;feature extraction;object detection","crowd analysis tasks;crowd counting;Crowd Hat;crowd-specific information;dense crowds;detection models;region-adaptive NMS thresholds;underutilized output features","","","","38","IEEE","22 Aug 2023","","","IEEE","IEEE Conferences"
"Tracking Pedestrian Heads in Dense Crowd","R. Sundararaman; C. De Almeida Braga; E. Marchand; J. Pettré","Univ Rennes, Inria, CNRS, Irisa, Rennes, France; Univ Rennes, Inria, CNRS, Irisa, Rennes, France; Univ Rennes, Inria, CNRS, Irisa, Rennes, France; Univ Rennes, Inria, CNRS, Irisa, Rennes, France","2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)","2 Nov 2021","2021","","","3864","3874","Tracking humans in crowded video sequences is an important constituent of visual scene understanding. Increasing crowd density challenges visibility of humans, limiting the scalability of existing pedestrian trackers to higher crowd densities. For that reason, we propose to revitalize head tracking with Crowd of Heads Dataset (CroHD), consisting of 9 sequences of 11,463 frames with over 2,276,838 heads and 5,230 tracks annotated in diverse scenes. For evaluation, we proposed a new metric, IDEucl, to measure an algorithm’s efficacy in preserving a unique identity for the longest stretch in image coordinate space, thus building a correspondence between pedestrian crowd motion and the performance of a tracking algorithm. Moreover, we also propose a new head detector, HeadHunter, which is designed for small head detection in crowded scenes. We extend HeadHunter with a Particle Filter and a color histogram based re-identification module for head tracking. To establish this as a strong baseline, we compare our tracker with existing state-of-the-art pedestrian trackers on CroHD and demonstrate superiority, especially in identity preserving tracking metrics. With a light-weight head detector and a tracker which is efficient at identity preservation, we believe our contributions will serve useful in advancement of pedestrian tracking in dense crowds. We make our dataset, code and models publicly available at https://project.inria.fr/crowdscience/project/dense-crowd-head-tracking/.","2575-7075","978-1-6654-4509-2","10.1109/CVPR46437.2021.00386","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9577483","","Visualization;Head;Tracking;Scalability;Video sequences;Detectors;Real-time systems","feature extraction;image colour analysis;image filtering;image motion analysis;image sequences;object detection;object tracking;particle filtering (numerical methods);pedestrians;traffic engineering computing;video signal processing","tracking algorithm;head detection;crowded scenes;color histogram based re-identification module;pedestrian trackers;CroHD;tracking metrics;identity preservation;dense crowds;crowded video sequences;visual scene understanding;crowd density;pedestrian crowd motion;pedestrian head tracking;lightweight head detector;human tracking;crowd of head dataset;diverse scene annotation;IDEucl metric;image coordinate space;HeadHunter;particle filter","","24","","70","IEEE","2 Nov 2021","","","IEEE","IEEE Conferences"
"StarCraftImage: A Dataset For Prototyping Spatial Reasoning Methods For Multi-Agent Environments","S. Kulinski; N. R. Waytowich; J. Z. Hare; D. I. Inouye","Purdue University; ARL, DEVCOM Army Research Laboratory; ARL, DEVCOM Army Research Laboratory; Purdue University","2023 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)","22 Aug 2023","2023","","","22004","22013","Spatial reasoning tasks in multi-agent environments such as event prediction, agent type identification, or missing data imputation are important for multiple applications (e.g., autonomous surveillance over sensor networks and subtasks for reinforcement learning (RL)). StarCraft II game replays encode intelligent (and adversarial) multiagent behavior and could provide a testbed for these tasks; however, extracting simple and standardized representations for prototyping these tasks is laborious and hinders reproducibility. In contrast, MNIST and CIFAR10, despite their extreme simplicity, have enabled rapid prototyping and reproducibility of ML methods. Following the simplicity of these datasets, we construct a benchmark spatial reasoning dataset based on StarCraft II replays that exhibit complex multi-agent behaviors, while still being as easy to use as MNIST and CIFAR10. Specifically, we carefully summarize a window of 255 consecutive game states to create 3.6 million summary images from 60,000 replays, including all relevant metadata such as game outcome and player races. We develop three formats of decreasing complexity: Hyperspectral images that include one channel for every unit type (similar to multispectral geospatial images), RGB images that mimic CIFAR10, and grayscale images that mimic MNIST. We show how this dataset can be used for prototyping spatial reasoning methods. All datasets, code for extraction, and code for dataset loading can be found at https://starcraftdata.davidinouye.com/.","2575-7075","979-8-3503-0129-8","10.1109/CVPR52729.2023.02107","NSF(grant numbers:IIS-2212097); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10204010","Datasets and evaluation","Codes;Loading;Games;Metadata;Gray-scale;Cognition;Spatial databases","computer games;hyperspectral imaging;meta data;multi-agent systems;reinforcement learning;spatial reasoning","255 consecutive game states;3.6 million summary images;agent type identification;autonomous surveillance over sensor networks;benchmark spatial reasoning dataset;dataset loading;encode intelligent multiagent behavior;event prediction;exhibit complex multiagent behaviors;game outcome;hinders reproducibility;mimic CIFAR10;missing data imputation;ML methods;MNIST;multiagent environments;player races;prototyping spatial reasoning methods;rapid prototyping;simple representations;spatial reasoning tasks;standardized representations;StarCraft II game;subtasks","","","","48","IEEE","22 Aug 2023","","","IEEE","IEEE Conferences"
"Progressive End-to-End Object Detection in Crowded Scenes","A. Zheng; Y. Zhang; X. Zhang; X. Qi; J. Sun",MEGVII Technology; Shanghai Jiao Tong University; MEGVII Technology; University of Hong Kong; MEGVII Technology,"2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)","27 Sep 2022","2022","","","847","856","In this paper, we propose a new query-based detection framework for crowd detection. Previous query-based detectors suffer from two drawbacks: first, multiple predictions will be inferred for a single object, typically in crowded scenes; second, the performance saturates as the depth of the decoding stage increases. Benefiting from the nature of the one-to-one label assignment rule, we propose a progressive predicting method to address the above issues. Specifically, we first select accepted queries prone to generate true positive predictions, then refine the rest noisy queries according to the previously accepted predictions. Experiments show that our method can significantly boost the performance of query-based detectors in crowded scenes. Equipped with our approach, Sparse RCNN achieves 92.0% AP, 41.4% MR−2 and 83.2% JI on the challenging CrowdHuman [35] dataset, outperforming the box-based method MIP [8] that specifies in handling crowded scenarios. Moreover, the proposed method, robust to crowdedness, can still obtain consistent improvements on moderately and slightly crowded datasets like CityPersons [47] and COCO [26]. Code will be made publicly available at https://github.com/megvii-model/Iter-E2EDET.","2575-7075","978-1-6654-6946-3","10.1109/CVPR52688.2022.00093","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9878585","Deep learning architectures and techniques; Recognition: detection;categorization;retrieval; Representation learning; Scene analysis and understanding; Vision applications and systems","Representation learning;Performance evaluation;Deep learning;Machine vision;Detectors;Prediction methods;Object detection","decoding;object detection;query processing;video surveillance","progressive end-to-end object detection;crowded scenes;query-based detection framework;crowd detection;previous query-based detectors;multiple predictions;decoding stage increases;label assignment rule;progressive predicting method;positive predictions;rest noisy queries;accepted predictions;box-based method MIP;crowded scenarios;crowded datasets","","11","","52","IEEE","27 Sep 2022","","","IEEE","IEEE Conferences"
"CrowdCLIP: Unsupervised Crowd Counting via Vision-Language Model","D. Liang; J. Xie; Z. Zou; X. Ye; W. Xu; X. Bai","Huazhong University of Science and Technology; Beijing University of Posts and Telecommunications; Baidu Inc., China; Baidu Inc., China; Beijing University of Posts and Telecommunications; Huazhong University of Science and Technology","2023 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)","22 Aug 2023","2023","","","2893","2903","Supervised crowd counting relies heavily on costly manual labeling, which is difficult and expensive, especially in dense scenes. To alleviate the problem, we propose a novel unsupervised framework for crowd counting, named CrowdCLIP. The core idea is built on two observations: 1) the recent contrastive pre-trained vision-language model (CLIP) has presented impressive performance on various downstream tasks; 2) there is a natural mapping between crowd patches and count text. To the best of our knowledge, CrowdCLIP is the first to investigate the vision-language knowledge to solve the counting problem. Specifically, in the training stage, we exploit the multi-modal ranking loss by constructing ranking text prompts to match the size-sorted crowd patches to guide the image encoder learning. In the testing stage, to deal with the diversity of image patches, we propose a simple yet effective progressive filtering strategy to first select the highly potential crowd patches and then map them into the language space with various counting intervals. Extensive experiments on five challenging datasets demonstrate that the proposed CrowdCLIP achieves superior performance compared to previous unsupervised state-of-the-art counting methods. Notably, CrowdCLIP even surpasses some pop-ular fully-supervised methods under the cross-dataset setting. The source code will be available at https://github.com/dk-liang/CrowdCLIP.","2575-7075","979-8-3503-0129-8","10.1109/CVPR52729.2023.00283","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10203327","Humans: Face;body;pose;gesture;movement","Training;Computer vision;Filtering;Source coding;Manuals;Pattern recognition;Labeling","computer vision;image representation;learning (artificial intelligence);object detection;supervised learning;unsupervised learning","costly manual labeling;count text;counting intervals;counting problem;dense scenes;downstream tasks;effective progressive filtering strategy;highly potential crowd patches;image encoder learning;image patches;language space;multimodal ranking loss;named CrowdCLIP;natural mapping;novel unsupervised framework;pop-ular fully-supervised methods;previous unsupervised state-of-the-art counting methods;ranking text;recent contrastive pre-trained vision-language model;size-sorted crowd patches;supervised crowd counting;testing stage;training stage;unsupervised crowd counting;vision-language knowledge","","","","65","IEEE","22 Aug 2023","","","IEEE","IEEE Conferences"
"IPCC-TP: Utilizing Incremental Pearson Correlation Coefficient for Joint Multi-Agent Trajectory Prediction","D. Zhu; G. Zhai; Y. Di; F. Manhardt; H. Berkemeyer; T. Tran; N. Navab; F. Tombari; B. Busam",Technical University of Munich; Technical University of Munich; Technical University of Munich; Google; Robert Bosch GmbH; Robert Bosch GmbH; Technical University of Munich; Technical University of Munich; Technical University of Munich,"2023 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)","22 Aug 2023","2023","","","5507","5516","Reliable multi-agent trajectory prediction is crucial for the safe planning and control of autonomous systems. Compared with single-agent cases, the major challenge in simultaneously processing multiple agents lies in modeling complex social interactions caused by various driving intentions and road conditions. Previous methods typically leverage graph-based message propagation or attention mechanism to encapsulate such interactions in the format of marginal probabilistic distributions. However, it is inherently sub-optimal. In this paper, we propose IPCC-TP, a novel relevance-aware module based on Incremental Pearson Correlation Coefficient to improve multi-agent interaction modeling. IPCC-TP learns pairwise joint Gaussian Distributions through the tightly-coupled estimation of the means and covariances according to interactive incremental movements. Our module can be conveniently embedded into existing multi-agent prediction methods to extend original motion distribution decoders. Extensive experiments on nuScenes and Argoverse 2 datasets demonstrate that IPCC-TP improves the performance of baselines by a large margin.","2575-7075","979-8-3503-0129-8","10.1109/CVPR52729.2023.00533","China Scholarship Council; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10203793","Autonomous driving","Correlation coefficient;Computational modeling;Roads;Predictive models;Gaussian distribution;Probabilistic logic;Probability distribution","control engineering computing;Gaussian distribution;graph theory;learning (artificial intelligence);multi-agent systems;road traffic control","autonomous systems;complex social interactions;graph-based message propagation;incremental pearson correlation coefficient;interactive incremental movements;IPCC-TP;joint Gaussian Distributions;joint multiagent trajectory prediction;marginal probabilistic distributions;multiagent interaction modeling;multiagent prediction methods;relevance-aware module;single-agent cases","","","","48","IEEE","22 Aug 2023","","","IEEE","IEEE Conferences"
"FJMP: Factorized Joint Multi-Agent Motion Prediction over Learned Directed Acyclic Interaction Graphs","L. Rowe; M. Ethier; E. -H. Dykhne; K. Czarnecki","School of Computer Science, University of Waterloo; School of Computer Science, University of Waterloo; School of Computer Science, University of Waterloo; School of Computer Science, University of Waterloo","2023 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)","22 Aug 2023","2023","","","13745","13755","Predicting the future motion of road agents is a critical task in an autonomous driving pipeline. In this work, we address the problem of generating a set of scene-level, or joint, future trajectory predictions in multi-agent driving scenarios. To this end, we propose FJMP, a Factorized Joint Motion Prediction framework for multi-agent interactive driving scenarios. FJMP models the future scene interaction dynamics as a sparse directed interaction graph, where edges denote explicit interactions between agents. We then prune the graph into a directed acyclic graph (DAG) and decompose the joint prediction task into a sequence of marginal and conditional predictions according to the partial ordering of the DAG, where joint future trajectories are decoded using a directed acyclic graph neural network (DAGNN). We conduct experiments on the INTERACTION and Argoverse 2 datasets and demonstrate that FJMP produces more accurate and scene-consistent joint trajectory predictions than non-factorized approaches, especially on the most interactive and kinematically interesting agents. FJMP ranks 1st on the multi-agent test leaderboard of the INTERACTION dataset.","2575-7075","979-8-3503-0129-8","10.1109/CVPR52729.2023.01321","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10204178","Autonomous driving","Directed acyclic graph;Roads;Dynamics;Pipelines;Neural networks;Predictive models;Trajectory","directed graphs;graph neural networks;learning (artificial intelligence);multi-agent systems;traffic engineering computing","autonomous driving pipeline;conditional predictions;DAGNN;directed acyclic graph neural network;factorized joint multiagent motion prediction;FJMP models;future trajectories;joint prediction task;joint trajectory predictions;kinematically interesting agents;learned directed acyclic interaction graphs;marginal predictions;multiagent interactive driving scenarios;multiagent test leaderboard;road agents;sparse directed interaction graph","","","","50","IEEE","22 Aug 2023","","","IEEE","IEEE Conferences"
"Context-Aware Crowd Counting","W. Liu; M. Salzmann; P. Fua","Computer Vision Laboratory, École Polytechnique Fédérale de Lausanne (EPFL); Computer Vision Laboratory, École Polytechnique Fédérale de Lausanne (EPFL); Computer Vision Laboratory, École Polytechnique Fédérale de Lausanne (EPFL)","2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)","9 Jan 2020","2019","","","5094","5103","State-of-the-art methods for counting people in crowded scenes rely on deep networks to estimate crowd density. They typically use the same filters over the whole image or over large image patches. Only then do they estimate local scale to compensate for perspective distortion. This is typically achieved by training an auxiliary classifier to select, for predefined image patches, the best kernel size among a limited set of choices. As such, these methods are not end-to-end trainable and restricted in the scope of context they can leverage. In this paper, we introduce an end-to-end trainable deep architecture that combines features obtained using multiple receptive field sizes and learns the importance of each such feature at each image location. In other words, our approach adaptively encodes the scale of the contextual information required to accurately predict crowd density. This yields an algorithm that outperforms state-of-the-art crowd counting methods, especially when perspective effects are strong.","2575-7075","978-1-7281-3293-8","10.1109/CVPR.2019.00524","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8954153","Recognition: Detection;Categorization;Retrieval;Deep Learning;Vision Applications and Systems;Visual Reasoning","Training;Computer vision;Deep architecture;Prediction algorithms;Distortion;Pattern recognition;Kernel","feature extraction;image classification;learning (artificial intelligence)","context-aware crowd counting;crowded scenes;deep networks;crowd density;local scale;perspective distortion;auxiliary classifier;predefined image patches;kernel size;end-to-end trainable deep architecture;multiple receptive field sizes;image location;state-of-the-art crowd counting methods","","325","1","43","IEEE","9 Jan 2020","","","IEEE","IEEE Conferences"
"A Generalized Loss Function for Crowd Counting and Localization","J. Wan; Z. Liu; A. B. Chan","Department of Computer Science, City University of Hong Kong; Department of Computer Science, City University of Hong Kong; Department of Computer Science, City University of Hong Kong","2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)","2 Nov 2021","2021","","","1974","1983","Previous work [40] shows that a better density map representation can improve the performance of crowd counting. In this paper, we investigate learning the density map representation through an unbalanced optimal transport problem, and propose a generalized loss function to learn density maps for crowd counting and localization. We prove that pixel-wise L2 loss and Bayesian loss [29] are special cases and suboptimal solutions to our proposed loss function. A perspective-guided transport cost function is further proposed to better handle the perspective transformation in crowd images. Since the predicted density will be pushed toward annotation positions, the density map prediction will be sparse and can naturally be used for localization. Finally, the proposed loss outperforms other losses on four large-scale datasets for counting, and achieves the best localization performance on NWPU-Crowd and UCF-QNRF.","2575-7075","978-1-6654-4509-2","10.1109/CVPR46437.2021.00201","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9578673","","Location awareness;Computer vision;Annotations;Computer architecture;Cost function;Bayes methods;Pattern recognition","Bayes methods;image annotation;image representation;learning (artificial intelligence);optimisation","generalized loss function;crowd counting;density map representation;unbalanced optimal transport;Bayesian loss;perspective-guided transport cost function;crowd images;density map prediction;crowd localization;NWPU-crowd;pixel-wise L2 loss;suboptimal solutions;annotation positions;UCF-QNRF","","78","","51","IEEE","2 Nov 2021","","","IEEE","IEEE Conferences"
"Leveraging Self-Supervision for Cross-Domain Crowd Counting","W. Liu; N. Durasov; P. Fua","Tencent AI Lab; CVLab, EPFL; CVLab, EPFL","2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)","27 Sep 2022","2022","","","5331","5342","State-of-the-art methods for counting people in crowded scenes rely on deep networks to estimate crowd density. While effective, these data-driven approaches rely on large amount of data annotation to achieve good performance, which stops these models from being deployed in emergencies during which data annotation is either too costly or cannot be obtained fast enough. One popular solution is to use synthetic data for training. Unfortunately, due to domain shift, the resulting models generalize poorly on real imagery. We remedy this shortcoming by training with both synthetic images, along with their associated labels, and unlabeled real images. To this end, we force our network to learn perspective-aware features by training it to recognize upside-down real images from regular ones and incorporate into it the ability to predict its own uncertainty so that it can generate useful pseudo labels for fine-tuning purposes. This yields an algorithm that consistently outperforms state-of-the-art cross-domain crowd counting ones without any extra computation at inference time. Code is publicly available at https://github.com/weizheliu/Cross-Domain-Crowd-Counting.","2575-7075","978-1-6654-6946-3","10.1109/CVPR52688.2022.00527","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9878515","Recognition: detection;categorization;retrieval; Scene analysis and understanding","Training;Computer vision;Uncertainty;Image recognition;Annotations;Force;Prediction algorithms","feature extraction;graph theory;learning (artificial intelligence)","synthetic images;associated labels;perspective-aware features;useful pseudolabels;state-of-the-art cross-domain crowd;Cross-Domain Crowd Counting;crowded scenes;deep networks;crowd density;data-driven approaches;data annotation;synthetic data;domain shift","","11","","115","IEEE","27 Sep 2022","","","IEEE","IEEE Conferences"
"Revisiting Perspective Information for Efficient Crowd Counting","M. Shi; Z. Yang; C. Xu; Q. Chen","Inria, CNRS, IRISA, Univ Rennes; Key Laboratory of Machine Perception, Peking University; Key Laboratory of Machine Perception, Peking University; Department of Control Science and Engineering, Tongji University","2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)","9 Jan 2020","2019","","","7271","7280","Crowd counting is the task of estimating people numbers in crowd images. Modern crowd counting methods employ deep neural networks to estimate crowd counts via crowd density regressions. A major challenge of this task lies in the perspective distortion, which results in drastic person scale change in an image. Density regression on the small person area is in general very hard. In this work, we propose a perspective-aware convolutional neural network (PACNN) for efficient crowd counting, which integrates the perspective information into density regression to provide additional knowledge of the person scale change in an image. Ground truth perspective maps are firstly generated for training; PACNN is then specifically designed to predict multi-scale perspective maps and encode them as perspective-aware weighting layers in the network to adaptively combine the outputs of multi-scale density maps. The weights are learned at every pixel of the maps such that the final density combination is robust to the perspective distortion. We conduct extensive experiments on the ShanghaiTech, WorldExpo'10, UCF_CC_50, and UCSD datasets, and demonstrate the effectiveness and efficiency of PACNN over the state-of-the-art.","2575-7075","978-1-7281-3293-8","10.1109/CVPR.2019.00745","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8953508","Recognition: Detection;Categorization;Retrieval","","convolutional neural nets;image processing;regression analysis","PACNN;perspective information;crowd images;modern crowd counting methods;deep neural networks;crowd density regressions;drastic person scale change;density regression;person area;perspective-aware convolutional neural network;ground truth perspective maps;multiscale perspective maps;perspective-aware weighting layers;multiscale density maps;final density combination;ShanghaiTech;UCSD datasets","","106","","48","IEEE","9 Jan 2020","","","IEEE","IEEE Conferences"
"Cross-Modal Collaborative Representation Learning and a Large-Scale RGBT Benchmark for Crowd Counting","L. Liu; J. Chen; H. Wu; G. Li; C. Li; L. Lin","School of Computer Science and Engineering, Sun Yat-sen University, China; School of Computer Science and Engineering, Sun Yat-sen University, China; School of Computer Science and Engineering, Sun Yat-sen University, China; School of Computer Science and Engineering, Sun Yat-sen University, China; Anhui University, China; School of Computer Science and Engineering, Sun Yat-sen University, China","2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)","2 Nov 2021","2021","","","4821","4831","Crowd counting is a fundamental yet challenging task, which desires rich information to generate pixel-wise crowd density maps. However, most previous methods only used the limited information of RGB images and cannot well discover potential pedestrians in unconstrained scenarios. In this work, we find that incorporating optical and thermal information can greatly help to recognize pedestrians. To promote future researches in this field, we introduce a large-scale RGBT Crowd Counting (RGBT-CC) benchmark, which contains 2,030 pairs of RGB-thermal images with 138,389 annotated people. Furthermore, to facilitate the multimodal crowd counting, we propose a cross-modal collaborative representation learning framework, which consists of multiple modality-specific branches, a modality-shared branch, and an Information Aggregation-Distribution Module (IADM) to capture the complementary information of different modalities fully. Specifically, our IADM incorporates two collaborative information transfers to dynamically enhance the modality-shared and modality-specific representations with a dual information propagation mechanism. Extensive experiments conducted on the RGBT-CC benchmark demonstrate the effectiveness of our framework for RGBT crowd counting. Moreover, the proposed approach is universal for multimodal crowd counting and is also capable to achieve superior performance on the ShanghaiTechRGBD [22] dataset. Finally, our source code and benchmark have been released at http://lingboliu.com/RGBT_Crowd_Counting.html.","2575-7075","978-1-6654-4509-2","10.1109/CVPR46437.2021.00479","National Natural Science Foundation of China; National Natural Science Foundation of China; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9578312","","Computer vision;Codes;Collaboration;Benchmark testing;Optical imaging;Pattern recognition;Task analysis","computer vision;groupware;image colour analysis;image representation;infrared imaging;learning (artificial intelligence);object detection;pedestrians","RGB-thermal images;multimodal crowd counting;modality-shared branch;information aggregation-distribution module;modality-specific representations;dual information propagation mechanism;pixel-wise crowd density maps;pedestrians;optical information;thermal information;large-scale RGBT crowd counting benchmark;cross-modal collaborative representation learning;IADM;large-scale RGBT-CC benchmark;ShanghaiTech RGBD dataset","","41","","67","IEEE","2 Nov 2021","","","IEEE","IEEE Conferences"
"Human Trajectory Prediction with Momentary Observation","J. Sun; Y. Li; L. Chai; H. -S. Fang; Y. -L. Li; C. Lu","Shanghai Jiao Tong University, China; Shanghai Jiao Tong University, China; Shanghai Jiao Tong University, China; Shanghai Jiao Tong University, China; Shanghai Jiao Tong University, China; Shanghai Jiao Tong University, China","2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)","27 Sep 2022","2022","","","6457","6466","Human trajectory prediction task aims to analyze human future movements given their past status, which is a crucial step for many autonomous systems such as self-driving cars and social robots. In real-world scenarios, it is unlikely to obtain sufficiently long observations at all times for prediction, considering inevitable factors such as tracking losses and sudden events. However, the problem of trajectory pre-diction with limited observations has not drawn much at-tention in previous work. In this paper, we study a task named momentary trajectory prediction, which reduces the observed history from a long time sequence to an extreme situation of two frames, one frame for social and scene contexts and both frames for the velocity of agents. We perform a rigorous study of existing state-of-the-art approaches in this challenging setting on two widely used benchmarks. We further propose a unified feature extractor, along with a novel pre-training mechanism, to capture effective infor-mation within the momentary observation. Our extractor can be adopted in existing prediction models and substan-tially boost their performance of momentary trajectory pre-diction. We hope our work will pave the way for more re-sponsive, precise and robust prediction approaches, an important step toward real-world autonomous systems.","2575-7075","978-1-6654-6946-3","10.1109/CVPR52688.2022.00636","National Natural Science Foundation of China(grant numbers:72192821,72192820); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9878712","Motion and tracking; Navigation and autonomous driving","Computer vision;Tracking;Navigation;Social robots;Predictive models;Feature extraction;Trajectory","feature extraction;learning (artificial intelligence);mobile robots","momentary observation;human trajectory prediction task;human future movements;self-driving cars;social robots;sufficiently long observations;inevitable factors;tracking losses;sudden events;momentary trajectory prediction;observed history;long time sequence;social scene contexts;rigorous study;existing state-of-the-art approaches;unified feature extractor;novel pre-training mechanism;prediction models;momentary trajectory pre-diction;robust prediction approaches;real-world autonomous systems","","1","","42","IEEE","27 Sep 2022","","","IEEE","IEEE Conferences"
"Learning From Synthetic Data for Crowd Counting in the Wild","Q. Wang; J. Gao; W. Lin; Y. Yuan","School of Computer Science and Center for OPTical IMagery Analysis and Learning (OPTIMAL), Northwestern Polytechnical University, Xi’an, Shaanxi, P. R. China; School of Computer Science and Center for OPTical IMagery Analysis and Learning (OPTIMAL), Northwestern Polytechnical University, Xi’an, Shaanxi, P. R. China; School of Computer Science and Center for OPTical IMagery Analysis and Learning (OPTIMAL), Northwestern Polytechnical University, Xi’an, Shaanxi, P. R. China; School of Computer Science and Center for OPTical IMagery Analysis and Learning (OPTIMAL), Northwestern Polytechnical University, Xi’an, Shaanxi, P. R. China","2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)","9 Jan 2020","2019","","","8190","8199","Recently, counting the number of people for crowd scenes is a hot topic because of its widespread applications (e.g. video surveillance, public security). It is a difficult task in the wild: changeable environment, large-range number of people cause the current methods can not work well. In addition, due to the scarce data, many methods suffer from over-fitting to a different extent. To remedy the above two problems, firstly, we develop a data collector and labeler, which can generate the synthetic crowd scenes and simultaneously annotate them without any manpower. Based on it, we build a large-scale, diverse synthetic dataset. Secondly, we propose two schemes that exploit the synthetic data to boost the performance of crowd counting in the wild: 1) pretrain a crowd counter on the synthetic data, then finetune it using the real data, which significantly prompts the model's performance on real data; 2) propose a crowd counting method via domain adaptation, which can free humans from heavy data annotations. Extensive experiments show that the first method achieves the state-of-the-art performance on four real datasets, and the second outperforms our baselines. The dataset and source code are available at https://gjy3035.github.io/GCC-CL/.","2575-7075","978-1-7281-3293-8","10.1109/CVPR.2019.00839","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8953868","Scene Analysis and Understanding;Datasets and Evaluation;Image and Video Synthesis;Others;Representation Learning;Vision Applicat","Computer vision;Codes;Annotations;Supervised learning;Feature extraction;Video surveillance;Data models","feature extraction;learning (artificial intelligence);object detection;video surveillance","crowd counting method;data annotations;synthetic data;data collector;synthetic crowd scenes;diverse synthetic dataset;crowd counter","","277","","44","IEEE","9 Jan 2020","","","IEEE","IEEE Conferences"
"ADCrowdNet: An Attention-Injective Deformable Convolutional Network for Crowd Understanding","N. Liu; Y. Long; C. Zou; Q. Niu; L. Pan; H. Wu","School of Data and Computer Science, Sun Yat-sen University; School of Data and Computer Science, Sun Yat-sen University; University of Maryland; School of Data and Computer Science, Sun Yat-sen University; Shanghai Jiao Tong University; School of Data and Computer Science, Sun Yat-sen University","2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)","9 Jan 2020","2019","","","3220","3229","We propose an attention-injective deformable convolutional network called ADCrowdNet for crowd understanding that can address the accuracy degradation problem of highly congested noisy scenes. ADCrowdNet contains two concatenated networks. An attention-aware network called Attention Map Generator (AMG) first detects crowd regions in images and computes the congestion degree of these regions. Based on detected crowd regions and congestion priors, a multi-scale deformable network called Density Map Estimator (DME) then generates high-quality density maps. With the attention-aware training scheme and multi-scale deformable convolutional scheme, the proposed ADCrowdNet achieves the capability of being more effective to capture the crowd features and more resistant to various noises. We have evaluated our method on four popular crowd counting datasets (ShanghaiTech, UCF_CC_50, WorldEXPO'10, and UCSD) and an extra vehicle counting dataset TRANCOS, and our approach beats existing state-of-the-art approaches on all of these datasets.","2575-7075","978-1-7281-3293-8","10.1109/CVPR.2019.00334","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8953548","Statistical Learning;Deep Learning","","computer vision;convolutional neural nets;estimation theory;feature extraction;image motion analysis;object detection;traffic engineering computing","multiscale deformable network;Density Map Estimator;high-quality density maps;attention-aware training scheme;multiscale deformable convolutional scheme;ADCrowdNet;crowd features;attention-injective deformable convolutional network;crowd understanding;attention-aware network;Attention Map Generator;crowd regions detection;vehicle counting dataset","","175","","38","IEEE","9 Jan 2020","","","IEEE","IEEE Conferences"
"Wide-Area Crowd Counting via Ground-Plane Density Maps and Multi-View Fusion CNNs","Q. Zhang; A. B. Chan","Department of Computer Science, City University of Hong Kong; Department of Computer Science, City University of Hong Kong","2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)","9 Jan 2020","2019","","","8289","8298","Crowd counting in single-view images has achieved outstanding performance on existing counting datasets. However, single-view counting is not applicable to large and wide scenes (e.g., public parks, long subway platforms, or event spaces) because a single camera cannot capture the whole scene in adequate detail for counting, e.g., when the scene is too large to fit into the field-of-view of the camera, too long so that the resolution is too low on faraway crowds, or when there are too many large objects that occlude large portions of the crowd. Therefore, to solve the wide-area counting task requires multiple cameras with overlapping fields-of-view. In this paper, we propose a deep neural network framework for multi-view crowd counting, which fuses information from multiple camera views to predict a scene-level density map on the ground-plane of the 3D world. We consider 3 versions of the fusion framework: the late fusion model fuses camera-view density map; the naive early fusion model fuses camera-view feature maps; and the multi-view multi-scale early fusion model favors that features aligned to the same ground-plane point have consistent scales. We test our 3 fusion models on 3 multi-view counting datasets, PETS2009, DukeMTMC, and a newly collected multi-view counting dataset containing a crowded street intersection. Our methods achieve state-of-the-art results compared to other multi-view counting baselines.","2575-7075","978-1-7281-3293-8","10.1109/CVPR.2019.00849","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8953461","Scene Analysis and Understanding;Motion and Tracking; Recognition: Detection;Categorization;Retrieval; Segmentation;Grouping and S","","cameras;convolutional neural nets;feature extraction;image fusion;image motion analysis;object detection","field-of-view;wide-area counting task;deep neural network framework;multiview crowd counting;scene-level density map;fusion framework;ground-plane point;crowded street intersection;wide-area crowd counting;ground-plane density maps;camera-view feature maps;multiview fusion CNNs","","55","","52","IEEE","9 Jan 2020","","","IEEE","IEEE Conferences"
"Learning to Estimate Robust 3D Human Mesh from In-the-Wild Crowded Scenes","H. Choi; G. Moon; J. Park; K. M. Lee","Dept. of ECE & ASRI, Seoul National University, Korea; Dept. of ECE & ASRI, Seoul National University, Korea; Dept. of ECE & ASRI, Seoul National University, Korea; Dept. of ECE & ASRI, Seoul National University, Korea","2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)","27 Sep 2022","2022","","","1465","1474","We consider the problem of recovering a single person's 3D human mesh from in-the-wild crowded scenes. While much progress has been in 3D human mesh estimation, existing methods struggle when test input has crowded scenes. The first reason for the failure is a domain gap between training and testing data. A motion capture dataset, which provides accurate 3D labels for training, lacks crowd data and impedes a network from learning crowded scene-robust image features of a target person. The second reason is a feature processing that spatially averages the feature map of a localized bounding box containing multiple people. Averaging the whole feature map makes a target person's feature indistinguishable from others. We present 3DCrowdNet that firstly explicitly targets in-the-wild crowded scenes and estimates a robust 3D human mesh by addressing the above issues. First, we leverage 2D human pose estimation that does not require a motion capture dataset with 3D labels for training and does not suffer from the domain gap. Second, we propose a joint-based regressor that distinguishes a target person's feature from others. Our joint-based regressor preserves the spatial activation of a target by sampling features from the target's joint locations and regresses human model parameters. As a result, 3DCrowdNet learns target-focused features and effectively excludes the irrelevant features of nearby persons. We conduct experiments on various benchmarks and prove the robustness of 3D CrowdNet to the in-the-wild crowded scenes both quantitatively and qualitatively. Codes are available here 11https://github.com/hongsukchoi/3DCrowdNet_RELEASE.","2575-7075","978-1-6654-6946-3","10.1109/CVPR52688.2022.00153","IITP(grant numbers:2021-0-01343); Seoul National University; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9879767","3D from single images; Face and gestures","Training;Computer vision;Three-dimensional displays;Codes;Face recognition;Pose estimation;Training data","computer vision;feature extraction;image motion analysis;image reconstruction;learning (artificial intelligence);object detection;pose estimation;regression analysis","2D human;motion capture dataset;domain gap;target person;human model parameters;target-focused features;3D CrowdNet;in-the-wild crowded scenes;robust 3D human mesh;single person;3D human mesh estimation;accurate 3D labels;crowd data;crowded scene-robust image features;feature processing;feature map","","19","","53","IEEE","27 Sep 2022","","","IEEE","IEEE Conferences"
"HiVT: Hierarchical Vector Transformer for Multi-Agent Motion Prediction","Z. Zhou; L. Ye; J. Wang; K. Wu; K. Lu",City University of Hong Kong; City University of Hong Kong; City University of Hong Kong; University of Victoria; University of Puerto Rico at Mayaguez,"2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)","27 Sep 2022","2022","","","8813","8823","Accurately predicting the future motions of surrounding traffic agents is critical for the safety of autonomous ve-hicles. Recently, vectorized approaches have dominated the motion prediction community due to their capability of capturing complex interactions in traffic scenes. How-ever, existing methods neglect the symmetries of the prob-lem and suffer from the expensive computational cost, facing the challenge of making real-time multi-agent motion prediction without sacrificing the prediction performance. To tackle this challenge, we propose Hierarchical Vector Transformer (HiVT) for fast and accurate multi-agent motion prediction. By decomposing the problem into local con-text extraction and global interaction modeling, our method can effectively and efficiently model a large number of agents in the scene. Meanwhile, we propose a translation-invariant scene representation and rotation-invariant spa-tial learning modules, which extract features robust to the geometric transformations of the scene and enable the model to make accurate predictions for multiple agents in a single forward pass. Experiments show that HiVT achieves the state-of-the-art performance on the Argoverse motion forecasting benchmark with a small model size and can make fast multi-agent motion prediction.","2575-7075","978-1-6654-6946-3","10.1109/CVPR52688.2022.00862","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9878832","Motion and tracking; Deep learning architectures and techniques; Navigation and autonomous driving","Tracking;Computational modeling;Computer architecture;Predictive models;Benchmark testing;Transformers;Feature extraction","feature extraction;image representation;learning (artificial intelligence);multi-agent systems;object detection;traffic engineering computing","HiVT;Hierarchical Vector Transformer;future motions;traffic agents;vectorized approaches;motion prediction community;traffic scenes;prediction performance;accurate multiagent motion prediction;translation-invariant scene representation;rotation-invariant spa-tial learning modules;multiple agents;Argoverse motion forecasting benchmark;fast multiagent motion prediction","","21","","52","IEEE","27 Sep 2022","","","IEEE","IEEE Conferences"
"Deep Decision Trees for Discriminative Dictionary Learning with Adversarial Multi-agent Trajectories","T. Fernando; S. Sridharan; C. Fookes; S. Denman","SAIVT, Queensland University of Technology (QUT), Australia; SAIVT, Queensland University of Technology (QUT), Australia; SAIVT, Queensland University of Technology (QUT), Australia; SAIVT, Queensland University of Technology (QUT), Australia","2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)","16 Dec 2018","2018","","","1803","180309","With the explosion in the availability of spatio-temporal tracking data in modern sports, there is an enormous opportunity to better analyse, learn and predict important events in adversarial group environments. In this paper, we propose a deep decision tree architecture for discriminative dictionary learning from adversarial multi-agent trajectories. We first build up a hierarchy for the tree structure by adding each layer and performing feature weight based clustering in the forward pass. We then fine tune the player role weights using back propagation. The hierarchical architecture ensures the interpretability and the integrity of the group representation. The resulting architecture is a decision tree, with leaf-nodes capturing a dictionary of multi-agent group interactions. Due to the ample volume of data available, we focus on soccer tracking data, although our approach can be used in any adversarial multi-agent domain. We present applications of proposed method for simulating soccer games as well as evaluating and quantifying team strategies.","2160-7516","978-1-5386-6100-0","10.1109/CVPRW.2018.00224","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8575387","","Trajectory;Sports;Dictionaries;Machine learning;Task analysis;Decision trees;Predictive models","decision trees;learning (artificial intelligence);multi-agent systems;pattern clustering;sport","dictionary learning;soccer games;feature weight based clustering;back propagation;multiagent group interactions;deep decision tree architecture;spatio-temporal tracking data;adversarial multiagent trajectories","","1","","37","IEEE","16 Dec 2018","","","IEEE","IEEE Conferences"
"Adaptive NMS: Refining Pedestrian Detection in a Crowd","S. Liu; D. Huang; Y. Wang","Beijing Advanced Innovation Center for Big Data and Brain Computing, Beihang University; Beijing Advanced Innovation Center for Big Data and Brain Computing, Beihang University; Beijing Advanced Innovation Center for Big Data and Brain Computing, Beihang University","2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)","9 Jan 2020","2019","","","6452","6461","Pedestrian detection in a crowd is a very challenging issue. This paper addresses this problem by a novel Non-Maximum Suppression (NMS) algorithm to better refine the bounding boxes given by detectors. The contributions are threefold: (1) we propose adaptive-NMS, which applies a dynamic suppression threshold to an instance, according to the target density; (2) we design an efficient subnetwork to learn density scores, which can be conveniently embedded into both the single-stage and two-stage detectors; and (3) we achieve state of the art results on the CityPersons and CrowdHuman benchmarks.","2575-7075","978-1-7281-3293-8","10.1109/CVPR.2019.00662","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8953514","Recognition: Detection;Categorization;Retrieval;Face;Gesture;and Body Pose","","image segmentation;learning (artificial intelligence);object detection;pedestrians","pedestrian detection;nonmaximum suppression algorithm;bounding boxes;adaptive-NMS;dynamic suppression threshold;density scores;two-stage detectors;single-stage detectors;CityPersons benchmarks;CrowdHuman benchmarks","","184","","52","IEEE","9 Jan 2020","","","IEEE","IEEE Conferences"
"MonoRec: Semi-Supervised Dense Reconstruction in Dynamic Environments from a Single Moving Camera","F. Wimbauer; N. Yang; L. von Stumberg; N. Zeller; D. Cremers",Technical University of Munich; Technical University of Munich; Technical University of Munich; Technical University of Munich; Technical University of Munich,"2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)","2 Nov 2021","2021","","","6108","6118","In this paper, we propose MonoRec, a semi-supervised monocular dense reconstruction architecture that predicts depth maps from a single moving camera in dynamic environments. MonoRec is based on a multi-view stereo setting which encodes the information of multiple consecutive images in a cost volume. To deal with dynamic objects in the scene, we introduce a MaskModule that predicts moving object masks by leveraging the photometric inconsistencies encoded in the cost volumes. Unlike other multi-view stereo methods, MonoRec is able to reconstruct both static and moving objects by leveraging the predicted masks. Furthermore, we present a novel multi-stage training scheme with a semi-supervised loss formulation that does not require LiDAR depth values. We carefully evaluate MonoRec on the KITTI dataset and show that it achieves state-of-theart performance compared to both multi-view and singleview methods. With the model trained on KITTI, we furthermore demonstrate that MonoRec is able to generalize well to both the Oxford RobotCar dataset and the more challenging TUM-Mono dataset recorded by a handheld camera. Code and related materials are available at https://vision.in.tum.de/research/monorec.","2575-7075","978-1-6654-4509-2","10.1109/CVPR46437.2021.00605","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9578522","","Training;Costs;Three-dimensional displays;Volume measurement;Robot vision systems;Prediction methods;Lead","cameras;feature extraction;image classification;image fusion;image motion analysis;image reconstruction;image resolution;image sampling;image segmentation;image sensors;image sequences;learning (artificial intelligence);object detection;object recognition;optical radar;robot vision;SLAM (robots);stereo image processing;video signal processing","single moving camera;monocular dense reconstruction architecture;dynamic environments;multiview stereo setting;cost volume;dynamic objects;object masks;multiview stereo methods;predicted masks;novel multistage training scheme;semisupervised loss formulation;semisupervised dense reconstruction","","27","","67","IEEE","2 Nov 2021","","","IEEE","IEEE Conferences"
"Recurrent Attentive Zooming for Joint Crowd Counting and Precise Localization","C. Liu; X. Weng; Y. Mu","Peking University, Beijing, China; Peking University, Beijing, China; Peking University, Beijing, China","2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)","9 Jan 2020","2019","","","1217","1226","Crowd counting is a new frontier in computer vision with far-reaching applications particularly in social safety management. A majority of existing works adopt a methodology that first estimates a person-density map and then calculates integral over this map to obtain the final count. As noticed by several prior investigations, the learned density map can significantly deviate from the true person density even though the final reported count is precise. This implies that the density map is unreliable for localizing crowd. To address this issue, this work proposes a novel framework that simultaneously solving two inherently related tasks - crowd counting and localization. The contributions are several-fold. First, our formulation is based on a crucial observation that localization tends to be inaccurate at high-density regions, and increasing the resolution is an effective albeit simple solution for improving localization. We thus propose Recurrent Attentive Zooming Network, which recurrently detects ambiguous image region and zooms it into high resolution for re-inspection. Second, the two tasks of counting and localization mutually reinforce each other. We propose an adaptive fusion scheme that effectively elevates the performance. Finally, a well-defined evaluation metric is proposed for the rarely-explored localization task. We conduct comprehensive evaluations on several crowd benchmarks, including the newly-developed large-scale UCF-QNRF dataset and demonstrate superior advantages over state-of-the-art methods.","2575-7075","978-1-7281-3293-8","10.1109/CVPR.2019.00131","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8953396","Video Analytics;Recognition: Detection;Categorization;Retrieval","","computer vision;crowdsourcing;image fusion;image resolution;learning (artificial intelligence);recurrent neural nets","Recurrent Attentive Zooming Network;image region detection;joint crowd counting;computer vision;social safety management;person-density map;crowd counting;crowd localization;precise localization;UCF-QNRF dataset;adaptive fusion","","86","","52","IEEE","9 Jan 2020","","","IEEE","IEEE Conferences"
"Density Map Regression Guided Detection Network for RGB-D Crowd Counting and Localization","D. Lian; J. Li; J. Zheng; W. Luo; S. Gao",ShanghaiTech University; ShanghaiTech University; ShanghaiTech University; ShanghaiTech University; ShanghaiTech University,"2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)","9 Jan 2020","2019","","","1821","1830","To simultaneously estimate head counts and localize heads with bounding boxes, a regression guided detection network (RDNet) is proposed for RGB-D crowd counting. Specifically, to improve the robustness of detection-based approaches for small/tiny heads, we leverage density map to improve the head/non-head classification in detection network where density map serves as the probability of a pixel being a head. A depth-adaptive kernel that considers the variances in head sizes is also introduced to generate high-fidelity density map for more robust density map regression. Further, a depth-aware anchor is designed for better initialization of anchor sizes in detection framework. Then we use the bounding boxes whose sizes are estimated with depth to train our RDNet. The existing RGB-D datasets are too small and not suitable for performance evaluation on data-driven based approaches, we collect a large-scale RGB-D crowd counting dataset. Experiments on both our RGB-D dataset and the MICC RGB-D counting dataset show that our method achieves the best performance for RGB-D crowd counting and localization. Further, our method can be readily extended to RGB image based crowd counting and achieves comparable performance on the ShanghaiTech Part\_B dataset for both counting and localization.","2575-7075","978-1-7281-3293-8","10.1109/CVPR.2019.00192","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8954179","Scene Analysis and Understanding;RGBD sensors and analytics","","image classification;image colour analysis;object detection;probability;regression analysis;SLAM (robots)","bounding boxes;RGB-D crowd counting;depth-adaptive kernel;high-fidelity density map;depth-aware anchor;RGB image based crowd counting;density map regression guided detection network;head counts estimation;nonhead classification;RGB-D crowd localization","","72","","37","IEEE","9 Jan 2020","","","IEEE","IEEE Conferences"
"Boosting Crowd Counting via Multifaceted Attention","H. Lin; Z. Ma; R. Ji; Y. Wang; X. Hong","School of Cyber Science and Engineering, Xi'an Jiaotong University; Shenzhen Institute of Advanced Technology, Chinese Academy of Science; Xiamen University; Peng Cheng Laboratory; School of Cyber Science and Engineering, Xi'an Jiaotong University","2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)","27 Sep 2022","2022","","","19596","19605","This paper focuses on the challenging crowd counting task. As large-scale variations often exist within crowd images, neither fixed-size convolution kernel of CNN nor fixed-size attention of recent vision transformers can well handle this kind of variations. To address this problem, we propose a Multifaceted Attention Network (MAN) to improve transformer models in local spatial relation encoding. MAN incorporates global attention from vanilla transformer, learnable local attention, and instance attention into a counting model. Firstly, the local Learnable Region Attention (LRA) is proposed to assign attention exclusive for each feature location dynamically. Secondly, we design the Local Attention Regularization to supervise the training of LRA by minimizing the deviation among the attention for different feature locations. Finally, we provide an Instance Attention mechanism to focus on the most important instances dynamically during training. Extensive experiments on four challenging crowd counting datasets namely ShanghaiTech, UCF-QNRF, JHU++, and NWPU have validated the proposed method. Code: https://github.com/LoraLinH/Boosting-Crowd-Counting-via-Multifaceted-Attention.","2575-7075","978-1-6654-6946-3","10.1109/CVPR52688.2022.01901","National Key Research and Development Project of China(grant numbers:2019YFB1312000); National Natural Science Foundation of China(grant numbers:62076195,U20B2052); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9880163","Scene analysis and understanding; Deep learning architectures and techniques; Representation learning","Training;Computer vision;Convolution;Computational modeling;Pipelines;Transformers;Encoding","computer vision;feature extraction;image classification;learning (artificial intelligence)","Boosting Crowd Counting;crowd counting task;large-scale variations;crowd images;fixed-size convolution kernel;CNN nor fixed-size attention;vision transformers;Multifaceted Attention Network;transformer models;local spatial relation encoding;global attention;vanilla transformer;learnable local attention;counting model;local Learnable Region Attention;LRA;feature location;Local Attention Regularization;different feature locations;Instance Attention mechanism","","34","","54","IEEE","27 Sep 2022","","","IEEE","IEEE Conferences"
"TrafficSim: Learning to Simulate Realistic Multi-Agent Behaviors","S. Suo; S. Regalado; S. Casas; R. Urtasun",Uber ATG; University of Waterloo; Uber ATG; Uber ATG,"2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)","2 Nov 2021","2021","","","10395","10404","Simulation has the potential to massively scale evaluation of self-driving systems, enabling rapid development as well as safe deployment. Bridging the gap between simulation and the real world requires realistic multi-agent behaviors. Existing simulation environments rely on heuristic-based models that directly encode traffic rules, which cannot capture irregular maneuvers (e.g., nudging, U-turns) and complex interactions (e.g., yielding, merging). In contrast, we leverage real-world data to learn directly from human demonstration, and thus capture more naturalistic driving behaviors. To this end, we propose TrafficSim, a multi-agent behavior model for realistic traffic simulation. In particular, we parameterize the policy with an implicit la-tent variable model that generates socially-consistent plans for all actors in the scene jointly. To learn a robust policy amenable for long horizon simulation, we unroll the policy in training and optimize through the fully differentiable simulation across time. Our learning objective incorporates both human demonstrations as well as common sense. We show TrafficSim generates significantly more realistic traffic scenarios as compared to a diverse set of baselines. Notably, we can exploit trajectories generated by TrafficSim as effective data augmentation for training better motion planner.","2575-7075","978-1-6654-4509-2","10.1109/CVPR46437.2021.01026","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9577667","","Training;Computer vision;Merging;Traffic control;Trajectory;Safety;Pattern recognition","behavioural sciences computing;driver information systems;learning (artificial intelligence);multi-agent systems;road traffic","TrafficSim;traffic scenarios;simulation environments;heuristic-based models;traffic rules;naturalistic driving behaviors;multiagent behavior model;realistic traffic simulation;implicit latent variable model;long horizon simulation;fully differentiable simulation;learning objective;realistic multiagent behavior simulation;data augmentation;motion planner","","50","","43","IEEE","2 Nov 2021","","","IEEE","IEEE Conferences"
"Multi-Agent Automated Machine Learning","Z. Wang; K. Su; J. Zhang; H. Jia; Q. Ye; X. Xie; Z. Lu",Peking University; Peking University; Huawei; Peking University; Peng Cheng Lab; Peking University; Peking University,"2023 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)","22 Aug 2023","2023","","","11960","11969","In this paper, we propose multi-agent automated machine learning (MA2ML) with the aim to effectively handle joint optimization of modules in automated machine learning (AutoML). MA2ML takes each machine learning module, such as data augmentation (AUG), neural architecture search (NAS), or hyper-parameters (HPO), as an agent and the final performance as the reward, to formulate a multi-agent reinforcement learning problem. MA2ML explicitly assigns credit to each agent according to its marginal contribution to enhance cooperation among modules, and incorporates off-policy learning to improve search efficiency. Theoretically, MA2ML guarantees monotonic improvement of joint optimization. Extensive experiments show that MA2ML yields the state-of-the-art top-1 accuracy on ImageNet under constraints of computational cost, e.g., 79.7%/80.5% with FLOPs fewer than 600M/800M. Exten\sive ablation studies verify the benefits of credit assignment and off-policy learning of MA2ML.","2575-7075","979-8-3503-0129-8","10.1109/CVPR52729.2023.01151","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10205095","Deep learning architectures and techniques","Training;Costs;Semantic segmentation;Pipelines;Transforms;Reinforcement learning;Search problems","data augmentation;learning (artificial intelligence);multi-agent systems;neural net architecture;reinforcement learning;search problems","joint optimization;MA2ML guarantees monotonic improvement;MA2ML yields;multiagent automated machine learning;multiagent reinforcement learning problem;off-policy learning","","","","54","IEEE","22 Aug 2023","","","IEEE","IEEE Conferences"
"Diverse Generation for Multi-Agent Sports Games","R. A. Yeh; A. G. Schwing; J. Huang; K. Murphy",University of Illinois at Urbana-Champaign; University of Illinois at Urbana-Champaign; University of Illinois at Urbana-Champaign; University of Illinois at Urbana-Champaign,"2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)","9 Jan 2020","2019","","","4605","4614","In this paper, we propose a new generative model for multi-agent trajectory data, focusing on the case of multi-player sports games. Our model leverages graph neural networks (GNNs) and variational recurrent neural networks (VRNNs) to achieve a permutation equivariant model suitable for sports. On two challenging datasets (basketball and soccer), we show that we are able to produce more accurate forecasts than previous methods. We assess accuracy using various metrics, such as log-likelihood and ""best of N"" loss, based on N different samples of the future. We also measure the distribution of statistics of interest, such as player location or velocity, and show that the distribution induced by our generative model better matches the empirical distribution of the test set. Finally, we show that our model can perform conditional prediction, which lets us answer counterfactual questions such as “how will the players move differently if A passes the ball to B instead of C?”","2575-7075","978-1-7281-3293-8","10.1109/CVPR.2019.00474","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8953867","Motion and Tracking;Deep Learning","","graph theory;learning (artificial intelligence);multi-agent systems;recurrent neural nets;sport;statistical analysis","basketball;soccer;empirical distribution;multiagent sports games;multiagent trajectory data;multiplayer sports games;graph neural networks;variational recurrent neural networks;permutation equivariant model","","32","","44","IEEE","9 Jan 2020","","","IEEE","IEEE Conferences"
"STCrowd: A Multimodal Dataset for Pedestrian Perception in Crowded Scenes","P. Cong; X. Zhu; F. Qiao; Y. Ren; X. Peng; Y. Hou; L. Xu; R. Yang; D. Manocha; Y. Ma",ShanghaiTech University; The Chinese University of Hong Kong; RWTH Aachen University; ShanghaiTech University; ShanghaiTech University; Shanghai AI Laboratory; ShanghaiTech University; University of Kentucky; University of Maryland at College Park; ShanghaiTech University,"2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)","27 Sep 2022","2022","","","19576","19585","Accurately detecting and tracking pedestrians in 3D space is challenging due to large variations in rotations, poses and scales. The situation becomes even worse for dense crowds with severe occlusions. However, existing benchmarks either only provide 2D annotations, or have limited 3D annotations with low-density pedestrian distribution, making it difficult to build a reliable pedestrian perception system especially in crowded scenes. To better evaluate pedestrian perception algorithms in crowded scenarios, we introduce a large-scale multimodal dataset, STCrowd. Specifically, in STCrowd, there are a total of 219 K pedestrian instances and 20 persons per frame on average, with various levels of occlusion. We provide synchronized LiDAR point clouds and camera images as well as their corresponding 3D labels and joint IDs. STCrowd can be used for various tasks, including LiDAR-only, image-only, and sensor-fusion based pedestrian detection and tracking. We provide baselines for most of the tasks. In addition, considering the property of sparse global distribution and density-varying local distribution of pedestrians, we further propose a novel method, Density-aware Hierarchical heatmap Aggregation (DHA), to enhance pedestrian perception in crowded scenes. Extensive experiments show that our new method achieves state-of-the-art performance for pedestrian detection on various datasets. https://github.com/4DVLab/STCrowd.git.","2575-7075","978-1-6654-6946-3","10.1109/CVPR52688.2022.01899","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9880203","Scene analysis and understanding; 3D from multi-view and sensors; Datasets and evaluation; Recognition: detection;categorization;retrieval; RGBD sensors and analytics","Point cloud compression;Three-dimensional displays;Annotations;Benchmark testing;Sensors;Pattern recognition;Synchronization","cameras;image fusion;image motion analysis;object detection;optical radar;pedestrians;traffic engineering computing","dense crowds;low-density pedestrian distribution;pedestrian perception algorithms;crowded scenarios;large-scale multimodal dataset;pedestrian instances;LiDAR point clouds;3D labels;sensor-fusion based pedestrian detection;sparse global distribution;density-varying local distribution;STCrowd;density-aware hierarchical heatmap aggregation;DHA","","4","","62","IEEE","27 Sep 2022","","","IEEE","IEEE Conferences"
"Optimal Transport Minimization: Crowd Localization on Density Maps for Semi-Supervised Counting","W. Lin; A. B. Chan","Department of Computer Science, City University of Hong Kong; Department of Computer Science, City University of Hong Kong","2023 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)","22 Aug 2023","2023","","","21663","21673","The accuracy of crowd counting in images has improved greatly in recent years due to the development of deep neural networks for predicting crowd density maps. However, most methods do not further explore the ability to localize people in the density map, with those few works adopting simple methods, like finding the local peaks in the density map. In this paper, we propose the optimal transport minimization (OT-M) algorithm for crowd localization with density maps. The objective of OT-M is to find a target point map that has the minimal Sinkhorn distance with the input density map, and we propose an iterative algorithm to compute the solution. We then apply OT-M to generate hard pseudo-labels (point maps) for semi-supervised counting, rather than the soft pseudo-labels (density maps) used in previous methods. Our hard pseudo-labels provide stronger supervision, and also enable the use of recent density-to-point loss functions for training. We also propose a confidence weighting strategy to give higher weight to the more reliable unlabeled data. Extensive experiments show that our methods achieve outstanding performance on both crowd localization and semi-supervised counting. Code is available at https://github.com/Elin24/OT-M.","2575-7075","979-8-3503-0129-8","10.1109/CVPR52729.2023.02075","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10205094","Scene analysis and understanding","Location awareness;Training;Computer vision;Image analysis;Minimization;Prediction algorithms;Pattern recognition","deep learning (artificial intelligence);iterative methods;learning (artificial intelligence);supervised learning","crowd density maps;crowd localization;input density map;optimal transport minimization algorithm;point maps;recent density-to-point loss;semisupervised counting;soft pseudolabels;target point map","","1","","68","IEEE","22 Aug 2023","","","IEEE","IEEE Conferences"
"Cross-View Cross-Scene Multi-View Crowd Counting","Q. Zhang; W. Lin; A. B. Chan","Department of Computer Science, City University of Hong Kong, Hong Kong SAR, China; School of Computer Science and School of Artificial Intelligence, Northwestern Polytechnical University, Xi’an, China; Department of Computer Science, City University of Hong Kong, Hong Kong SAR, China","2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)","2 Nov 2021","2021","","","557","567","Multi-view crowd counting has been previously proposed to utilize multi-cameras to extend the field-of-view of a single camera, capturing more people in the scene, and improve counting performance for occluded people or those in low resolution. However, the current multi-view paradigm trains and tests on the same single scene and camera-views, which limits its practical application. In this paper, we propose a cross-view cross-scene (CVCS) multi-view crowd counting paradigm, where the training and testing occur on different scenes with arbitrary camera layouts. To dynamically handle the challenge of optimal view fusion under scene and camera layout change and non-correspondence noise due to camera calibration errors or erroneous features, we propose a CVCS model that attentively selects and fuses multiple views together using camera layout geometry, and a noise view regularization method to train the model to handle non-correspondence errors. We also generate a large synthetic multi-camera crowd counting dataset with a large number of scenes and camera views to capture many possible variations, which avoids the difficulty of collecting and annotating such a large real dataset. We then test our trained CVCS model on real multi-view counting datasets, by using unsupervised domain transfer. The proposed CVCS model trained on synthetic data outperforms the same model trained only on real data, and achieves promising performance compared to fully supervised methods that train and test on the same single scene.","2575-7075","978-1-6654-4509-2","10.1109/CVPR46437.2021.00062","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9577502","","Training;Geometry;Adaptation models;Fuses;Layout;Cameras;Data models","cameras;feature extraction;image capture;image denoising;image fusion;image resolution;unsupervised learning","field-of-view;camera-views;optimal view fusion;camera layout;fuses multiple views;noise view regularization method;camera views;multiview counting datasets;cross-view cross-scene multiview crowd counting;multicameras;synthetic multicamera crowd counting dataset;CVCS model;unsupervised domain transfer","","17","","61","IEEE","2 Nov 2021","","","IEEE","IEEE Conferences"
"Bi-level Alignment for Cross-Domain Crowd Counting","S. Gong; S. Zhang; J. Yang; D. Dai; B. Schiele","PCA Lab, Key Lab of Intelligent Perception and Systems for High-Dimensional Information of Ministry of Education, and Jiangsu Key Lab of Image and Video Understanding for Social Security, School of Computer Science and Engineering, Nanjing University of Science and Technology; PCA Lab, Key Lab of Intelligent Perception and Systems for High-Dimensional Information of Ministry of Education, and Jiangsu Key Lab of Image and Video Understanding for Social Security, School of Computer Science and Engineering, Nanjing University of Science and Technology; PCA Lab, Key Lab of Intelligent Perception and Systems for High-Dimensional Information of Ministry of Education, and Jiangsu Key Lab of Image and Video Understanding for Social Security, School of Computer Science and Engineering, Nanjing University of Science and Technology; MPI Informatics; MPI Informatics","2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)","27 Sep 2022","2022","","","7532","7540","Recently, crowd density estimation has received increasing attention. The main challenge for this task is to achieve high-quality manual annotations on a large amount of training data. To avoid reliance on such annotations, previous works apply unsupervised domain adaptation (UDA) techniques by transferring knowledge learned from easily accessible synthetic data to real-world datasets. However, current state-of-the-art methods either rely on external data for training an auxiliary task or apply an expensive coarse-to-fine estimation. In this work, we aim to develop a new adversarial learning based method, which is simple and efficient to apply. To reduce the domain gap between the synthetic and real data, we design a bi-level alignment framework (BLA) consisting of (1) task-driven data alignment and (2) fine-grained feature alignment. In contrast to previous domain augmentation methods, we introduce AutoML to search for an optimal transform on source, which well serves for the downstream task. On the other hand, we do fine-grained alignment for foreground and background separately to alleviate the alignment difficulty. We evaluate our approach on five real-world crowd counting benchmarks, where we outperform existing approaches by a large margin. Also, our approach is simple, easy to implement and efficient to apply. The code is publicly available at https://github.com/Yankeegsj/BLA.","2575-7075","978-1-6654-6946-3","10.1109/CVPR52688.2022.00739","Fundamental Research Funds for the Central Universities(grant numbers:30920032201); National Natural Science Foundation of China(grant numbers:62172225); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9878759","Recognition: detection;categorization;retrieval; Transfer/low-shot/long-tail learning","Training;Computer vision;Annotations;Estimation;Training data;Transforms;Manuals","feature extraction;image recognition;neural nets;unsupervised learning","cross-domain crowd counting;crowd density estimation;training data;coarse-to-fine estimation;bi-level alignment;fine-grained feature alignment;unsupervised domain adaptation;adversarial learning;task-driven data alignment;BLA;AutoML;UDA;knowledge transfer","","5","","32","IEEE","27 Sep 2022","","","IEEE","IEEE Conferences"
"Towards Professional Level Crowd Annotation of Expert Domain Data","P. Wang; N. Vasconcelos",UC San Diego; UC San Diego,"2023 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)","22 Aug 2023","2023","","","3166","3175","Image recognition on expert domains is usually fine-grained and requires expert labeling, which is costly. This limits dataset sizes and the accuracy of learning systems. To address this challenge, we consider annotating expert data with crowdsourcing. This is denoted as PrOfeSsional lEvel cRowd (POSER) annotation. A new approach, based on semi-supervised learning (SSL) and denoted as SSL with human filtering (SSL-HF) is proposed. It is a human-in-the-loop SSL method, where crowd-source workers act as filters of pseudo-labels, replacing the unreliable confidence thresholding used by state-of-the-art SSL methods. To enable annotation by non-experts, classes are specified implicitly, via positive and negative sets of examples and augmented with deliberative explanations, which highlight regions of class ambiguity. In this way, SSL-HF leverages the strong low-shot learning and confidence estimation ability of humans to create an intuitive but effective labeling experience. Experiments show that SSL-HF significantly outperforms various alternative approaches in several benchmarks.","2575-7075","979-8-3503-0129-8","10.1109/CVPR52729.2023.00309","NSF(grant numbers:IIS-1924937,IIS-2041009); NVIDIA; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10204827","Efficient and scalable vision","Training;Deep learning;Image recognition;Annotations;Filtering;Estimation;Semisupervised learning","crowdsourcing;deep learning (artificial intelligence);image annotation;image recognition;semi-supervised learning (artificial intelligence)","class ambiguity;confidence estimation ability;crowd-source workers;dataset sizes;expert data;expert domain data annotation;expert labeling;human filtering;human-in-the-loop SSL method;image recognition;learning systems;low-shot learning;POSER;professional level crowd annotation;pseudolabels;semisupervised learning;SSL-HF;unreliable confidence thresholding","","","","57","IEEE","22 Aug 2023","","","IEEE","IEEE Conferences"
"LaPred: Lane-Aware Prediction of Multi-Modal Future Trajectories of Dynamic Agents","B. Kim; S. H. Park; S. Lee; E. Khoshimjonov; D. Kum; J. Kim; J. S. Kim; J. W. Choi",Hanyang University; Hanyang University; Hanyang University; Hanyang University; Korea Advanced Institute of Science and Technology; Hyundai Motor Company; Hyundai Motor Company; Hanyang University,"2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)","2 Nov 2021","2021","","","14631","14640","In this paper, we address the problem of predicting the future motion of a dynamic agent (called a target agent) given its current and past states as well as the information on its environment. It is paramount to develop a prediction model that can exploit the contextual information in both static and dynamic environments surrounding the target agent and generate diverse trajectory samples that are meaningful in a traffic context. We propose a novel prediction model, referred to as the lane-aware prediction (LaPred) network, which uses the instance-level lane entities extracted from a semantic map to predict the multi-modal future trajectories. For each lane candidate found in the neighborhood of the target agent, LaPred extracts the joint features relating the lane and the trajectories of the neighboring agents. Then, the features for all lane candidates are fused with the attention weights learned through a self-supervised learning task that identifies the lane candidate likely to be followed by the target agent. Using the instance-level lane information, LaPred can produce the trajectories compliant with the surroundings better than 2D raster image-based methods and generate the diverse future trajectories given multiple lane candidates. The experiments conducted on the public nuScenes dataset and Argo- verse dataset demonstrate that the proposed LaPred method significantly outperforms the existing prediction models, achieving state-of-the-art performance in the benchmarks.","2575-7075","978-1-6654-4509-2","10.1109/CVPR46437.2021.01440","National Research Foundation; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9578925","","Measurement;Dynamics;Semantics;Predictive models;Feature extraction;Trajectory;Pattern recognition","image classification;image reconstruction;supervised learning;traffic engineering computing","argo- verse dataset demonstrate;2D raster image-based methods;self-supervised learning task;traffic context;multiple lane candidates;diverse future trajectories;instance-level lane information;neighboring agents;lane-aware prediction network;diverse trajectory samples;dynamic environments;static environments;target agent;future motion;dynamic agent;multimodal future trajectories;LaPred","","30","","34","IEEE","2 Nov 2021","","","IEEE","IEEE Conferences"
"NeRF-DS: Neural Radiance Fields for Dynamic Specular Objects","Z. Yan; C. Li; G. H. Lee","Department of Computer Science, National University of Singapore; Department of Computer Science, National University of Singapore; Department of Computer Science, National University of Singapore","2023 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)","22 Aug 2023","2023","","","8285","8295","Dynamic Neural Radiance Field (NeRF) is a powerful algorithm capable of rendering photo-realistic novel view images from a monocular RGB video of a dynamic scene. Although it warps moving points across frames from the observation spaces to a common canonical space for rendering, dynamic NeRF does not model the change of the reflected color during the warping. As a result, this approach often fails drastically on challenging specular objects in motion. We address this limitation by reformulating the neural radiance field function to be conditioned on surface position and orientation in the observation space. This allows the specular surface at different poses to keep the different reflected colors when mapped to the common canonical space. Additionally, we add the mask of moving objects to guide the deformation field. As the specular surface changes color during motion, the mask mitigates the problem of failure to find temporal correspondences with only RGB supervision. We evaluate our model based on the novel view synthesis quality with a self-collected dataset of different moving specular objects in realistic environments. The experimental results demonstrate that our method significantly improves the reconstruction quality of moving specular objects from monocular RGB videos compared to the existing NeRF models. Our code and data are available at the project website11https://github.com/JokerYan/NeRF-DS.","2575-7075","979-8-3503-0129-8","10.1109/CVPR52729.2023.00801","National Research Foundation(grant numbers:AISG2-RP-2021-024); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10204790","3D from multi-view and sensors","Surface reconstruction;Computer vision;Image color analysis;Deformation;Heuristic algorithms;Computational modeling;Dynamics","feature extraction;image colour analysis;image reconstruction;image representation;image sensors;rendering (computer graphics)","common canonical space;deformation field;different moving specular objects;different poses;different reflected colors;dynamic NeRF;Dynamic Neural Radiance Field;dynamic scene;dynamic specular objects;existing NeRF models;monocular RGB video;neural radiance field function;Neural Radiance fields;novel view synthesis quality;observation space;powerful algorithm;realistic environments;reflected color;rendering photo-realistic novel view images;RGB supervision;specular surface changes color;surface position;warping","","","","59","IEEE","22 Aug 2023","","","IEEE","IEEE Conferences"
"Crowd Activity Change Point Detection in Videos via Graph Stream Mining","M. Yang; L. Rashidi; S. Rajasegarar; C. Leckie; A. S. Rao; M. Palaniswami","School of Computing and Information Systems, The University of Melbourne; School of Computing and Information Systems, The University of Melbourne; Department of Electrical and Electronic Engineering, The University of Melbourne; School of Computing and Information Systems, The University of Melbourne; Department of Electrical and Electronic Engineering, The University of Melbourne; Department of Electrical and Electronic Engineering, The University of Melbourne","2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)","16 Dec 2018","2018","","","328","3288","In recent years, there has been a growing interest in detecting anomalous behavioral patterns in video. In this work, we address this task by proposing a novel activity change point detection method to identify crowd movement anomalies for video surveillance. In our proposed novel framework, a hyperspherical clustering algorithm is utilized for the automatic identification of interesting regions, then the density of pedestrian flows between every pair of interesting regions over consecutive time intervals is monitored and represented as a sequence of adjacency matrices where the direction and density of flows are captured through a directed graph. Finally, we use graph edit distance as well as a cumulative sum test to detect change points in the graph sequence. We conduct experiments on four real-world video datasets: Dublin, New Orleans, Abbey Road and MCG Datasets. We observe that our proposed approach achieves a high F-measure, i.e., in the range [0.7, 1], for these datasets. The evaluation reveals that our proposed method can successfully detect the change points in all datasets at both global and local levels. Our results also demonstrate the efficiency and effectiveness of our proposed algorithm for change point detection and segmentation tasks.","2160-7516","978-1-5386-6100-0","10.1109/CVPRW.2018.00059","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8575520","","Videos;Trajectory;Clustering algorithms;Monitoring;Object detection;Task analysis;Video sequences","data mining;directed graphs;graph theory;image segmentation;object detection;pattern clustering;video signal processing;video surveillance","change points;segmentation tasks;crowd activity change point detection;graph stream mining;growing interest;anomalous behavioral patterns;novel activity change;crowd movement anomalies;video surveillance;hyperspherical clustering algorithm;consecutive time intervals;directed graph;graph edit distance;cumulative sum test;graph sequence;real-world video datasets;MCG Datasets;Dublin dataset;New Orleans dataset;Abbey Road dataset;MCG dataset","","5","","22","IEEE","16 Dec 2018","","","IEEE","IEEE Conferences"
"MotionDiffuser: Controllable Multi-Agent Motion Prediction Using Diffusion","C. “. Jiang; A. Cornman; C. Park; B. Sapp; Y. Zhou; D. Anguelov",Waymo LLC; Waymo LLC; Waymo LLC; Waymo LLC; Waymo LLC; Waymo LLC,"2023 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)","22 Aug 2023","2023","","","9644","9653","We present MotionDiffuser, a diffusion based representation for the joint distribution of future trajectories over multiple agents. Such representation has several key advantages: first, our model learns a highly multimodal distribution that captures diverse future outcomes. Second, the simple predictor design requires only a single L2 loss training objective, and does not depend on trajectory anchors. Third, our model is capable of learning the joint distribution for the motion of multiple agents in a permutation-invariant manner. Furthermore, we utilize a compressed trajectory representation via PCA, which improves model performance and allows for efficient computation of the exact sample log probability. Subsequently, we propose a general constrained sampling framework that enables controlled trajectory sampling based on differentiable cost functions. This strategy enables a host of applications such as enforcing rules and physical priors, or creating tailored simulation scenarios. MotionDiffuser can be combined with existing backbone architectures to achieve top motion forecasting results. We obtain state-of-the-art results for multi-agent motion prediction on the Waymo Open Motion Dataset.","2575-7075","979-8-3503-0129-8","10.1109/CVPR52729.2023.00930","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10203706","Autonomous driving","Training;Computer vision;Computational modeling;Computer architecture;Cost function;Trajectory;Pattern recognition","learning (artificial intelligence);multi-agent systems;probability","compressed trajectory representation;controllable multiagent motion prediction;differentiable cost functions;diffusion based representation;diverse future outcomes;exact sample log probability;general constrained sampling framework;joint distribution;motion forecasting;MotionDiffuser;multimodal distribution;multiple agents;PCA;permutation-invariant manner;simple predictor design;single L2 loss training objective;trajectory anchors;trajectory sampling;Waymo open motion dataset","","","","53","IEEE","22 Aug 2023","","","IEEE","IEEE Conferences"
"OcTr: Octree-Based Transformer for 3D Object Detection","C. Zhou; Y. Zhang; J. Chen; D. Huang","State Key Laboratory of Software Development Environment, Beihang University, Beijing, China; State Key Laboratory of Software Development Environment, Beihang University, Beijing, China; School of Computer Science and Engineering, Beihang University, Beijing, China; State Key Laboratory of Software Development Environment, Beihang University, Beijing, China","2023 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)","22 Aug 2023","2023","","","5166","5175","A key challenge for LiDAR-based 3D object detection is to capture sufficient features from large scale 3D scenes especially for distant or/and occluded objects. Albeit recent efforts made by Transformers with the long sequence modeling capability, they fail to properly balance the accuracy and efficiency, suffering from inadequate receptive fields or coarse-grained holistic correlations. In this paper, we propose an Octree-based Transformer, named OcTr, to address this issue. It first constructs a dynamic octree on the hierarchical feature pyramid through conducting self-attention on the top level and then recursively propagates to the level below restricted by the octants, which captures rich global context in a coarse-to-fine manner while maintaining the computational complexity under control. Furthermore, for enhanced foreground perception, we propose a hybrid positional embedding, composed of the semantic-aware positional embedding and attention mask, to fully exploit semantic and geometry clues. Extensive experiments are conducted on the Waymo Open Dataset and KITTI Dataset, and OcTr reaches newly state-of-the-art results.","2575-7075","979-8-3503-0129-8","10.1109/CVPR52729.2023.00500","National Natural Science Foundation of China(grant numbers:62022011,62202034); Research Program of State Key Laboratory of Software Development Environment(grant numbers:SKLSDE-2021ZX-04); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10203760","Autonomous driving","Geometry;Three-dimensional displays;Costs;Correlation;Semantics;Octrees;Object detection","computational complexity;deep learning (artificial intelligence);feature extraction;object detection;octrees;optical radar","albeit recent efforts;attention mask;coarse-grained holistic correlations;coarse-to-fine manner;distant objects;dynamic octree;hierarchical feature pyramid;inadequate receptive fields;LiDAR-based 3D object detection;long sequence modeling capability;named OcTr;occluded objects;Octree-based Transformer;scale 3D scenes;semantic-aware positional embedding;sufficient features;Transformers","","1","","63","IEEE","22 Aug 2023","","","IEEE","IEEE Conferences"
"Point in, Box Out: Beyond Counting Persons in Crowds","Y. Liu; M. Shi; Q. Zhao; X. Wang","College of Computer Science, Sichuan University; Univ Rennes, Inria, CNRS, IRISA; College of Computer Science, Sichuan University; Univ Rennes, Inria, CNRS, IRISA","2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)","9 Jan 2020","2019","","","6462","6471","Modern crowd counting methods usually employ deep neural networks (DNN) to estimate crowd counts via density regression. Despite their significant improvements, the regression-based methods are incapable of providing the detection of individuals in crowds. The detection-based methods, on the other hand, have not been largely explored in recent trends of crowd counting due to the needs for expensive bounding box annotations. In this work, we instead propose a new deep detection network with only point supervision required. It can simultaneously detect the size and location of human heads and count them in crowds. We first mine useful person size information from point-level annotations and initialize the pseudo ground truth bounding boxes. An online updating scheme is introduced to refine the pseudo ground truth during training; while a locally-constrained regression loss is designed to provide additional constraints on the size of the predicted boxes in a local neighborhood. In the end, we propose a curriculum learning strategy to train the network from images of relatively accurate and easy pseudo ground truth first. Extensive experiments are conducted in both detection and counting tasks on several standard benchmarks, e.g. ShanghaiTech, UCF_CC_50, WiderFace, and TRANCOS datasets, and the results show the superiority of our method over the state-of-the-art.","2575-7075","978-1-7281-3293-8","10.1109/CVPR.2019.00663","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8953871","Recognition: Detection;Categorization;Retrieval","","feature extraction;image classification;learning (artificial intelligence);neural nets;object detection;regression analysis","deep neural networks;density regression;regression-based methods;detection-based methods;crowd counting;bounding box annotations;deep detection network;point supervision;person size information;point-level annotations;pseudoground truth bounding boxes;regression loss;counting tasks;curriculum learning strategy","","100","","49","IEEE","9 Jan 2020","","","IEEE","IEEE Conferences"
"SLOPER4D: A Scene-Aware Dataset for Global 4D Human Pose Estimation in Urban Environments","Y. Dai; Y. Lin; X. Lin; C. Wen; L. Xu; H. Yi; S. Shen; Y. Ma; C. Wang","Fujian Key Laboratory of Sensing and Computing for Smart Cities, Xiamen University; Fujian Key Laboratory of Sensing and Computing for Smart Cities, Xiamen University; Fujian Key Laboratory of Sensing and Computing for Smart Cities, Xiamen University; Fujian Key Laboratory of Sensing and Computing for Smart Cities, Xiamen University; ShanghaiTech University, China; Max Planck Institute for Intelligent Systems, Germany; Fujian Key Laboratory of Sensing and Computing for Smart Cities, Xiamen University; ShanghaiTech University, China; Fujian Key Laboratory of Sensing and Computing for Smart Cities, Xiamen University","2023 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)","22 Aug 2023","2023","","","682","692","We present SLOPER4D, a novel scene-aware dataset collected in large urban environments to facilitate the research of global human pose estimation (GHPE) with human-scene interaction in the wild. Employing a head-mounted device integrated with a LiDAR and camera, we record 12 human subjects' activities over 10 diverse urban scenes from an egocentric view. Frame-wise annotations for 2D key points, 3D pose parameters, and global translations are provided, together with reconstructed scene point clouds. To obtain accurate 3D ground truth in such large dynamic scenes, we propose a joint optimization method to fit local SMPL meshes to the scene and fine-tune the camera calibration during dynamic motions frame by frame, resulting in plausible and scene-natural 3D human poses. Even-tually, SLOPER4D consists of 15 sequences of human motions, each of which has a trajectory length of more than 200 meters (up to 1,300 meters) and covers an area of more than 200 m2 (up to 30,000 m2), including more than 100k LiDAR frames, 300k video frames, and 500k IMU-based motion frames. With SLOPER4D, we provide a detailed and thorough analysis of two critical tasks, including camera-based 3D HPE and LiDAR-based 3D HPE in urban environments, and benchmark a new task, GHPE. The in-depth analysis demonstrates SLOPER4D poses significant challenges to existing methods and produces great research opportunities. The dataset and code are released at http://www.lidarhumanmotion.net/sloper4d/.","2575-7075","979-8-3503-0129-8","10.1109/CVPR52729.2023.00073","Natural Science Foundation of China(grant numbers:62171393); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10204011","Humans: Face;body;pose;gesture;movement","Meters;Three-dimensional displays;Laser radar;Annotations;Urban areas;Pose estimation;Dynamics","calibration;cameras;image reconstruction;optical radar;pose estimation;solid modelling","10 diverse urban scenes;100k LiDAR frames;2D key points;300k video frames;3D pose parameters;accurate 3D ground truth;camera-based 3D HPE;dynamic motions frame;dynamic scenes;frame-wise annotations;global 4D human pose estimation;global human;human motions;human-scene interaction;IMU-based motion frames;LiDAR-based 3D HPE;novel scene-aware dataset;reconstructed scene point clouds;record 12 human subjects;scene-natural;urban environments","","4","","68","IEEE","22 Aug 2023","","","IEEE","IEEE Conferences"
"EqMotion: Equivariant Multi-Agent Motion Prediction with Invariant Interaction Reasoning","C. Xu; R. T. Tan; Y. Tan; S. Chen; Y. G. Wang; X. Wang; Y. Wang",Shanghai Jiao Tong University; National University of Singapore; Shanghai Jiao Tong University; Shanghai Jiao Tong University; Shanghai Jiao Tong University; National University of Singapore; Shanghai AI Laboratory,"2023 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)","22 Aug 2023","2023","","","1410","1420","Learning to predict agent motions with relationship reasoning is important for many applications. In motion prediction tasks, maintaining motion equivariance under Euclidean geometric transformations and invariance of agent interaction is a critical and fundamental principle. However, such equivariance and invariance properties are overlooked by most existing methods. To fill this gap, we propose Eq-Motion, an efficient equivariant motion prediction model with invariant interaction reasoning. To achieve motion equivariance, we propose an equivariant geometric feature learning module to learn a Euclidean transformable feature through dedicated designs of equivariant operations. To reason agent's interactions, we propose an invariant interaction reasoning module to achieve a more stable interaction modeling. To further promote more comprehensive motion features, we propose an invariant pattern feature learning module to learn an invariant pattern feature, which cooperates with the equivariant geometric feature to enhance network expressiveness. We conduct experiments for the proposed model on four distinct scenarios: particle dynamics, molecule dynamics, human skeleton motion prediction and pedestrian trajectory prediction. Experimental results show that our method is not only generally applicable, but also achieves state-of-the-art prediction performances on all the four tasks, improving by 24.0/30.1/8.6/9.2%. Code is available at https://github.com/MediaBrain-SJTU/EqMotion.","2575-7075","979-8-3503-0129-8","10.1109/CVPR52729.2023.00142","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10205349","Video: Low-level analysis;motion;tracking","Representation learning;Computer vision;Pedestrians;Dynamics;Predictive models;Cognition;Skeleton","feature extraction;image motion analysis;inference mechanisms;learning (artificial intelligence);multi-agent systems;pedestrians","agent interaction;agent motions;comprehensive motion features;critical principle;efficient equivariant motion prediction model;Eq-Motion;equivariant geometric feature;equivariant multiagent Motion prediction;equivariant operations;Euclidean transformable feature;fundamental principle;invariance properties;invariant interaction reasoning module;invariant pattern feature;motion equivariance;motion prediction tasks;pedestrian trajectory prediction;reason agent;relationship reasoning;stable interaction modeling;state-of-the-art prediction performances","","1","","77","IEEE","22 Aug 2023","","","IEEE","IEEE Conferences"
"Residual Regression With Semantic Prior for Crowd Counting","J. Wan; W. Luo; B. Wu; A. B. Chan; W. Liu","Department of Computer Science, City University of Hong Kong; Tencent AI Lab; Tencent AI Lab; Department of Computer Science, City University of Hong Kong; Tencent AI Lab","2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)","9 Jan 2020","2019","","","4031","4040","Crowd counting is a challenging task due to factors such as large variations in crowdedness and severe occlusions. Although recent deep learning based counting algorithms have achieved a great progress, the correlation knowledge among samples and the semantic prior have not yet been fully exploited. In this paper, a residual regression framework is proposed for crowd counting utilizing the correlation information among samples. By incorporating such information into our network, we discover that more intrinsic characteristics can be learned by the network which thus generalizes better to unseen scenarios. Besides, we show how to effectively leverage the semantic prior to improve the performance of crowd counting. We also observe that the adversarial loss can be used to improve the quality of predicted density maps, thus leading to an improvement in crowd counting. Experiments on public datasets demonstrate the effectiveness and generalization ability of the proposed method.","2575-7075","978-1-7281-3293-8","10.1109/CVPR.2019.00416","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8954128","Scene Analysis and Understanding;Vision Applications and Systems","","learning (artificial intelligence);pose estimation;regression analysis","residual regression framework;counting algorithms;crowd counting","","60","","36","IEEE","9 Jan 2020","","","IEEE","IEEE Conferences"
"DynamicStereo: Consistent Dynamic Depth from Stereo Videos","N. Karaev; I. Rocco; B. Graham; N. Neverova; A. Vedaldi; C. Rupprecht","Meta AI; Meta AI; Meta AI; Meta AI; Meta AI; Visual Geometry Group, University of Oxford","2023 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)","22 Aug 2023","2023","","","13229","13239","We consider the problem of reconstructing a dynamic scene observed from a stereo camera. Most existing meth-ods for depth from stereo treat different stereo frames in-dependently, leading to temporally inconsistent depth pre-dictions. Temporal consistency is especially important for immersive AR or VR scenarios, where flickering greatly di-minishes the user experience. We propose DynamicStereo, a novel transformer-based architecture to estimate dispar-ity for stereo videos. The network learns to pool information from neighboring frames to improve the temporal consistency of its predictions. Our architecture is designed to process stereo videos efficiently through divided attention layers. We also introduce Dynamic Replica, a new bench-mark dataset containing synthetic videos of people and ani-mals in scanned environments, which provides complemen-tary training and evaluation data for dynamic stereo closer to real applications than existing datasets. Training with this dataset further improves the quality of predictions of our proposed DynamicStereo as well as prior methods. Finally, it acts as a benchmark for consistent stereo methods. Project page: https://dynamic-stereo.github.io/","2575-7075","979-8-3503-0129-8","10.1109/CVPR52729.2023.01271","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10203742","3D from multi-view and sensors","Training;Solid modeling;Three-dimensional displays;Estimation;Computer architecture;Transformers;User experience","cameras;learning (artificial intelligence);stereo image processing;video signal processing;virtual reality","consistent Dynamic depth;consistent stereo methods;Dynamic Replica;dynamic scene;dynamic stereo;DynamicStereo;existing meth-ods;novel transformer-based architecture;stereo camera;stereo treat different stereo frames;stereo videos;temporal consistency;temporally inconsistent depth pre-dictions","","","","56","IEEE","22 Aug 2023","","","IEEE","IEEE Conferences"
"TRACE: 5D Temporal Regression of Avatars with Dynamic Cameras in 3D Environments","Y. Sun; Q. Bao; W. Liu; T. Mei; M. J. Black",Harbin Institute of Technology; Explore Academy of JD.com; Explore Academy of JD.com; HiDream.ai Inc.; Max Planck Institute for Intelligent Systems,"2023 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)","22 Aug 2023","2023","","","8856","8866","Although the estimation of 3D human pose and shape (HPS) is rapidly progressing, current methods still cannot reliably estimate moving humans in global coordinates, which is critical for many applications. This is particularly challenging when the camera is also moving, entangling human and camera motion. To address these issues, we adopt a novel 5D representation (space, time, and identity) that enables end-to-end reasoning about people in scenes. Our method, called TRACE, introduces several novel architectural components. Most importantly, it uses two new “maps” to reason about the 3D trajectory of people over time in camera, and world, coordinates. An additional memory unit enables persistent tracking of people even during long occlusions. TRACE is the first one-stage method to jointly recover and track 3D humans in global coordinates from dynamic cameras. By training it end-to-end, and using full image information, TRACE achieves state-of-the-art performance on tracking and HPS benchmarks. The code11https://www.yusun.work/TRACE/TRACE.html and dataset22https://github.com/Arthur151/DynaCam are released for research purposes.","2575-7075","979-8-3503-0129-8","10.1109/CVPR52729.2023.00855","National Key R&D Program of China(grant numbers:2020AAA0103800); Beijing Nova Program(grant numbers:20220484063); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10204771","Humans: Face;body;pose;gesture;movement","Training;Three-dimensional displays;Tracking;Shape;Estimation;Training data;Benchmark testing","avatars;cameras;data analysis;image motion analysis;learning (artificial intelligence);natural language processing;object detection;pose estimation;regression analysis","3D human pose;3D trajectory;additional memory unit;architectural components;called TRACE;dynamic cameras;end-to-end reasoning;entangling human;global coordinates;HPS benchmarks;one-stage method;persistent tracking","","5","","65","IEEE","22 Aug 2023","","","IEEE","IEEE Conferences"
"CrowdPose: Efficient Crowded Scenes Pose Estimation and a New Benchmark","J. Li; C. Wang; H. Zhu; Y. Mao; H. -S. Fang; C. Lu",Shanghai Jiao Tong University; Shanghai Jiao Tong University; Shanghai Jiao Tong University; Tsinghua University; Shanghai Jiao Tong University; Shanghai Jiao Tong University,"2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)","9 Jan 2020","2019","","","10855","10864","Multi-person pose estimation is fundamental to many computer vision tasks and has made significant progress in recent years. However, few previous methods explored the problem of pose estimation in crowded scenes while it remains challenging and inevitable in many scenarios. Moreover, current benchmarks cannot provide an appropriate evaluation for such cases. In this paper, we propose a novel and efficient method to tackle the problem of pose estimation in the crowd and a new dataset to better evaluate algorithms. Our model consists of two key components: joint-candidate single person pose estimation (SPPE) and global maximum joints association. With multi-peak prediction for each joint and global association using the graph model, our method is robust to inevitable interference in crowded scenes and very efficient in inference. The proposed method surpasses the state-of-the-art methods on CrowdPose dataset by 5.2 mAP and results on MSCOCO dataset demonstrate the generalization ability of our method.","2575-7075","978-1-7281-3293-8","10.1109/CVPR.2019.01112","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8954341","Face;Gesture;and Body Pose;Deep Learning;Recognition: Detection;Categorization;Retrieval","Computer vision;Pose estimation;Transforms;Interference;Benchmark testing;Predictive models;Prediction algorithms","computer vision;natural scenes;pose estimation","generalization ability;MSCOCO dataset;graph model;crowded scenes pose estimation;joint-candidate SPPE;joint-candidate single person pose estimation;CrowdPose dataset;multipeak prediction;global maximum joints association;computer vision tasks;multiperson pose estimation","","224","","43","IEEE","9 Jan 2020","","","IEEE","IEEE Conferences"
"Crowd Counting and Density Estimation by Trellis Encoder-Decoder Networks","X. Jiang; Z. Xiao; B. Zhang; X. Zhen; X. Cao; D. Doermann; L. Shao","School of Electronic and Information Engineering, Beihang University, Beijing, China; School of Electronic and Information Engineering, Beihang University, Beijing, China; School of Automation Science and Electrical Engineering, Beihang University, Beijing, China; Inception Institute of Artificial Intelligence, UAE; School of Electronic and Information Engineering, Beihang University, Beijing, China; Department of Computer Science and Engineering, University at Buffalo, New York, USA; Inception Institute of Artificial Intelligence, UAE","2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)","9 Jan 2020","2019","","","6126","6135","Crowd counting has recently attracted increasing interest in computer vision but remains a challenging problem. In this paper, we propose a trellis encoder-decoder network (TEDnet) for crowd counting, which focuses on generating high-quality density estimation maps. The major contributions are four-fold. First, we develop a new trellis architecture that incorporates multiple decoding paths to hierarchically aggregate features at different encoding stages, which improves the representative capability of convolutional features for large variations in objects. Second, we employ dense skip connections interleaved across paths to facilitate sufficient multi-scale feature fusions, which also helps TEDnet to absorb the supervision information. Third, we propose a new combinatorial loss to enforce similarities in local coherence and spatial correlation between maps. By distributedly imposing this combinatorial loss on intermediate outputs, TEDnet can improve the back-propagation process and alleviate the gradient vanishing problem. Finally, on four widely-used benchmarks, our TEDnet achieves the best overall performance in terms of both density map quality and counting accuracy, with an improvement up to 14% in MAE metric. These results validate the effectiveness of TEDnet for crowd counting.","2575-7075","978-1-7281-3293-8","10.1109/CVPR.2019.00629","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8954254","Scene Analysis and Understanding;Deep Learning;Recognition: Detection;Categorization;Retrieval;Vision Applications and Systems","Measurement;Location awareness;Deep learning;Computer vision;Estimation;Computer architecture;Benchmark testing","backpropagation;computer vision;convolution;image coding;trellis codes","computer vision;gradient vanishing problem;back-propagation process;density estimation maps;multiscale feature fusions;convolutional features;TEDnet;trellis encoder-decoder network;crowd counting","","209","","54","IEEE","9 Jan 2020","","","IEEE","IEEE Conferences"
"Leveraging Heterogeneous Auxiliary Tasks to Assist Crowd Counting","M. Zhao; J. Zhang; C. Zhang; W. Zhang","School of Electronic Information and Electrical Engineering, Shanghai Jiao Tong University; University of Technology, Sydney; School of Electronic Information and Electrical Engineering, Shanghai Jiao Tong University; School of Electronic Information and Electrical Engineering, Shanghai Jiao Tong University","2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)","9 Jan 2020","2019","","","12728","12737","Crowd counting is a challenging task in the presence of drastic scale variations, the clutter background, and severe occlusions, etc. Existing CNN-based counting methods tackle these challenges mainly by fusing either multi-scale or multi-context features to generate robust representations. In this paper, we propose to address these issues by leveraging the heterogeneous attributes compounded in the density map. We identify three geometric/semantic/numeric attributes essentially important to the density estimation, and demonstrate how to effectively utilize these heterogeneous attributes to assist the crowd counting by formulating them into multiple auxiliary tasks. With the multi-fold regularization effects induced by the auxiliary tasks, the backbone CNN model is driven to embed desired properties explicitly and thus gains robust representations towards more accurate density estimation. Extensive experiments on three challenging crowd counting datasets have demonstrated the effectiveness of the proposed approach.","2575-7075","978-1-7281-3293-8","10.1109/CVPR.2019.01302","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8953875","Vision Applications and Systems;Deep Learning;Representation Learning;Scene Analysis and Understanding","Geometry;Computer vision;Computational modeling;Semantics;Estimation;Pattern recognition;Computational efficiency","convolutional neural nets;geometry;image fusion;image representation;object detection","heterogeneous auxiliary tasks;CNN;heterogeneous attributes;numeric attribute;semantic attribute;geometric attribute;multicontext feature fusion;multiscale feature fusion;crowd counting;density estimation;robust representations;multiple auxiliary tasks;density map","","78","","38","IEEE","9 Jan 2020","","","IEEE","IEEE Conferences"
"Crowd Counting in the Frequency Domain","W. Shu; J. Wan; K. C. Tan; S. Kwong; A. B. Chan","Dept. of Computer Science, City University of Hong Kong; Dept. of Computer Science, City University of Hong Kong; Dept. of Computing, The Hong Kong Polytechnic University; Dept. of Computer Science, City University of Hong Kong; Dept. of Computer Science, City University of Hong Kong","2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)","27 Sep 2022","2022","","","19586","19595","This paper investigates crowd counting in the frequency domain, which is a novel direction compared to the traditional view in the spatial domain. By transforming the density map into the frequency domain and using the properties of the characteristic function, we propose a novel method that is simple, effective, and efficient. The solid theoretical analysis ends up as an implementation-friendly loss function, which requires only standard tensor operations in the training process. We prove that our loss function is an upper bound of the pseudo sup norm metric between the ground truth and the prediction density map (over all of their sub-regions), and demonstrate its efficacy and efficiency versus other loss functions. The experimental results also show its competitiveness to the state-of-the-art on five benchmark data sets: ShanghaiTech A & B, UCF-QNRF, JHU++, and NWPU. Our codes will be available at: wb-shu/Crowd_Couniing_in_the_Frequency_Domain","2575-7075","978-1-6654-6946-3","10.1109/CVPR52688.2022.01900","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9879332","Scene analysis and understanding; Vision applications and systems","Training;Upper bound;Tensors;Image analysis;Frequency-domain analysis;Machine vision;Design methodology","frequency-domain analysis;image processing;learning (artificial intelligence);prediction theory;tensors","frequency domain;loss function;prediction density map;crowd counting;tensor operations;pseudo sup norm metric","","13","","48","IEEE","27 Sep 2022","","","IEEE","IEEE Conferences"
"Action4D: Online Action Recognition in the Crowd and Clutter","Q. You; H. Jiang","Microsoft Cloud & AI, Redmond, WA; Microsoft Cloud & AI, Redmond, WA","2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)","9 Jan 2020","2019","","","11849","11858","Recognizing every person's action in a crowded and cluttered environment is a challenging task in computer vision. We propose to tackle this challenging problem using a holistic 4D ``scan'' of a cluttered scene to include every detail about the people and environment. This leads to a new problem, i.e., recognizing multiple people's actions in the cluttered 4D representation. At the first step, we propose a new method to track people in 4D, which can reliably detect and follow each person in real time. Then, we build a new deep neural network, the Action4DNet, to recognize the action of each tracked person. Such a model gives reliable and accurate results in the real-world settings. We also design an adaptive 3D convolution layer and a novel discriminative temporal feature learning objective to further improve the performance of our model. Our method is invariant to camera view angles, resistant to clutter and able to handle crowd. The experimental results show that the proposed method is fast, reliable and accurate. Our method paves the way to action recognition in the real-world applications and is ready to be deployed to enable smart homes, smart factories and smart stores.","2575-7075","978-1-7281-3293-8","10.1109/CVPR.2019.01213","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8954340","RGBD sensors and analytics;Action Recognition;Deep Learning","Representation learning;Adaptation models;Computer vision;Solid modeling;Three-dimensional displays;Convolution;Computer network reliability","computer vision;convolutional neural nets;feature extraction;image motion analysis;image representation;learning (artificial intelligence);object detection;video signal processing","online action recognition;crowded environment;computer vision;cluttered scene;cluttered 4D representation;deep neural network;Action4DNet;tracked person;adaptive 3D convolution layer;discriminative temporal feature","","6","","30","IEEE","9 Jan 2020","","","IEEE","IEEE Conferences"
"What do navigation agents learn about their environment?","K. Dwivedi; G. Roig; A. Kembhavi; R. Mottaghi",Goethe University Frankfurt; Goethe University Frankfurt; PRIOR @ Allen Institute for AI; PRIOR @ Allen Institute for AI,"2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)","27 Sep 2022","2022","","","10266","10275","Today's state of the art visual navigation agents typically consist of large deep learning models trained end to end. Such models offer little to no interpretability about the learned skills or the actions of the agent taken in response to its environment. While past works have explored interpreting deep learning models, little attention has been devoted to interpreting embodied AI systems, which often involve reasoning about the structure of the environment, target characteristics and the outcome of one's actions. In this paper, we introduce the Interpretability System for Embodied agEnts (iSEE) for Point Goal and Object Goal navigation agents. We use iSEE to probe the dynamic representations produced by these agents for the presence of information about the agent as well as the environment. We demonstrate interesting insights about navigation agents using iSEE, including the ability to encode reachable locations (to avoid obstacles), visibility of the target, progress from the initial spawn location as well as the dramatic effect on the behaviors of agents when we mask out critical individual neurons.","2575-7075","978-1-6654-6946-3","10.1109/CVPR52688.2022.01003","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9880337","Explainable computer vision; Navigation and autonomous driving; Robot vision","Deep learning;Visualization;Navigation;Neurons;Virtual environments;Metadata;Pattern recognition","artificial intelligence;cognitive systems;collision avoidance;computer aided instruction;distance learning;inference mechanisms;knowledge acquisition;learning (artificial intelligence);problem solving","art visual navigation agents;deep learning models;learned skills;interpreting embodied AI systems;Interpretability System;Embodied agEnts;iSEE;Object Goal navigation agents","","2","","52","IEEE","27 Sep 2022","","","IEEE","IEEE Conferences"
"Detection, Tracking, and Counting Meets Drones in Crowds: A Benchmark","L. Wen; D. Du; P. Zhu; Q. Hu; Q. Wang; L. Bo; S. Lyu","JD Finance America Corporation, Mountain View, CA, USA; University at Albany, Albany, NY, USA; Tianjin University, Tianjin, China; Tianjin University, Tianjin, China; Tianjin University, Tianjin, China; JD Finance America Corporation, Mountain View, CA, USA; University at Albany, Albany, NY, USA","2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)","2 Nov 2021","2021","","","7808","7817","To promote the developments of object detection, tracking and counting algorithms in drone-captured videos, we construct a benchmark with a new drone-captured large-scale dataset, named as DroneCrowd, formed by 112 video clips with 33, 600 HD frames in various scenarios. Notably, we annotate 20, 800 people trajectories with 4.8 million heads and several video-level attributes. Meanwhile, we design the Space-Time Neighbor-Aware Network (STNNet) as a strong baseline to solve object detection, tracking and counting jointly in dense crowds. STNNet is formed by the feature extraction module, followed by the density map estimation heads, and localization and association subnets. To exploit the context information of neighboring objects, we design the neighboring context loss to guide the association subnet training, which enforces consistent relative position of nearby objects in temporal domain. Extensive experiments on our DroneCrowd dataset demonstrate that STNNet performs favorably against the state-of-the-arts.","2575-7075","978-1-6654-4509-2","10.1109/CVPR46437.2021.00772","National Key Research and Development Program of China; National Natural Science Foundation of China; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9578305","","Location awareness;Training;Target tracking;Estimation;Object detection;Benchmark testing;Trajectory","autonomous aerial vehicles;data analysis;feature extraction;object detection;object tracking;video signal processing;video surveillance","association subnets;neighboring objects;neighboring context loss;association subnet training;nearby objects;DroneCrowd dataset;STNNet performs;counting meets drones;object detection;drone-captured videos;drone-captured large-scale dataset;video clips;HD frames;people trajectories;million heads;video-level attributes;strong baseline;dense crowds;feature extraction module;density map estimation heads;localization;space-time neighbor-aware network;object tracking;object counting algorithms","","35","","46","IEEE","2 Nov 2021","","","IEEE","IEEE Conferences"
"Interpretable Social Anchors for Human Trajectory Forecasting in Crowds","P. Kothari; B. Sifringer; A. Alahi","EPFL VITA Lab, Lausanne; EPFL VITA Lab, Lausanne; EPFL VITA Lab, Lausanne","2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)","2 Nov 2021","2021","","","15551","15561","Human trajectory forecasting in crowds, at its core, is a sequence prediction problem with specific challenges of capturing inter-sequence dependencies (social interactions) and consequently predicting socially-compliant multimodal distributions. In recent years, neural network-based methods have been shown to outperform hand-crafted methods on distance-based metrics. However, these data-driven methods still suffer from one crucial limitation: lack of interpretability. To overcome this limitation, we leverage the power of discrete choice models to learn interpretable rule-based intents, and subsequently utilise the expressibility of neural networks to model scene-specific residual. Extensive experimentation on the interaction-centric benchmark TrajNet++ demonstrates the effectiveness of our proposed architecture to explain its predictions without compromising the accuracy.","2575-7075","978-1-6654-4509-2","10.1109/CVPR46437.2021.01530","National Science Foundation; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9578212","","Measurement;Computer vision;Neural networks;Knowledge based systems;Predictive models;Data models;Trajectory","knowledge based systems;learning (artificial intelligence);neural nets;social sciences","distance-based metrics;data-driven methods;crucial limitation;discrete choice models;interpretable rule-based intents;neural networks;scene-specific residual;interpretable social anchors;sequence prediction problem;inter-sequence dependencies;social interactions;socially-compliant multimodal distributions;crowd human trajectory forecasting;interaction-centric benchmark;TrajNet++","","19","","66","IEEE","2 Nov 2021","","","IEEE","IEEE Conferences"
"Ambiance in Social Media Venues: Visual Cue Interpretation by Machines and Crowds","G. Can; Y. Benkhedda; D. Gatica-Perez","Fédérale de Lausanne (EPFL), Idiap Research Institute and École Polytechnique, Switzerland; Fédérale de Lausanne (EPFL), Idiap Research Institute and École Polytechnique, Switzerland; Fédérale de Lausanne (EPFL), Idiap Research Institute and École Polytechnique, Switzerland","2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)","16 Dec 2018","2018","","","2426","242609","We study the perception of ambiance of places captured in social media images by both machines and crowdworkers. This task is challenging due to the subjective nature of the ambiance construct as well as the large variety in layout, style, and visual characteristics of venues. For machine recognition of ambiance, we use state-of-the-art Residual Deep Convolutional Neural Networks (ResNets), followed by gradient-weighted class activation mapping (Grad-CAM) visualizations. This form of visual explanation obtained from the trained ResNet-50 models were assessed by crowdworkers based on a carefully designed crowdsourcing task, in which both visual ambiance cues of venues and subjective assessment of Grad-CAM results were collected and analyzed. The results show that paintings, photos, and decorative items are strong cues for artsy ambiance, whereas type of utensils, type of lamps and presence of flowers may indicate formal ambiance. Layout and design-related cues such as type of chairs, type of tables/tablecloth and type of windows are noted to have impact for both ambiances. Overall, the ambiance visual cue recognition results are promising, and the crowd-based assessment approach may motivate other studies on subjective perception of place attributes.","2160-7516","978-1-5386-6100-0","10.1109/CVPRW.2018.00313","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8575486","","Visualization;Task analysis;Crowdsourcing;Social network services;Image color analysis;Layout;Urban areas","convolutional neural nets;crowdsourcing;data visualisation;feature extraction;image capture;image recognition;Internet;social networking (online)","design-related cues;social media venues;visual cue interpretation;social media images;machine recognition;gradient-weighted class activation mapping visualizations;ResNet-50 models;ambiance visual cue recognition;crowd-based assessment;residual deep convolutional neural networks;crowdsourcing;Grad-CAM visualizations","","7","","39","IEEE","16 Dec 2018","","","IEEE","IEEE Conferences"
"Query-Centric Trajectory Prediction","Z. Zhou; J. Wang; Y. Li; Y. Huang",City University of Hong Kong; City University of Hong Kong; Hon Hai Research Institute; Carnegie Mellon University,"2023 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)","22 Aug 2023","2023","","","17863","17873","Predicting the future trajectories of surrounding agents is essential for autonomous vehicles to operate safely. This paper presents QCNet, a modeling framework toward pushing the boundaries of trajectory prediction. First, we identify that the agent-centric modeling scheme used by existing approaches requires re-normalizing and re-encoding the input whenever the observation window slides forward, leading to redundant computations during online prediction. To overcome this limitation and achieve faster inference, we introduce a query-centric paradigm for scene encoding, which enables the reuse of past computations by learning representations independent of the global spacetime coordinate system. Sharing the invariant scene features among all target agents further allows the parallelism of multi-agent trajectory decoding. Second, even given rich encodings of the scene, existing decoding strategies struggle to capture the multimodality inherent in agents' future behavior, especially when the prediction horizon is long. To tackle this challenge, we first employ anchor-free queries to generate trajectory proposals in a recurrent fashion, which allows the model to utilize different scene contexts when decoding waypoints at different horizons. A refinement module then takes the trajectory proposals as anchors and leverages anchor-based queries to refine the trajectories further. By supplying adaptive and high-quality anchors to the refinement module, our query-based decoder can better deal with the multimodality in the output of trajectory prediction. Our approach ranks 1st on Argoverse 1 and Argoverse 2 motion forecasting benchmarks, outperforming all methods on all main metrics by a large margin. Meanwhile, our model can achieve streaming scene encoding and parallel multi-agent decoding thanks to the query-centric design ethos.","2575-7075","979-8-3503-0129-8","10.1109/CVPR52729.2023.01713","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10203873","Autonomous driving","Adaptation models;Computational modeling;Predictive models;Parallel processing;Encoding;Trajectory;Decoding","image retrieval;learning (artificial intelligence);mobile robots;multi-agent systems;road traffic control;road vehicles;robot vision;trajectory control","agent-centric modeling scheme;anchor-based queries;anchor-free queries;Argoverse 1 motion forecasting benchmark;Argoverse 2 motion forecasting benchmark;autonomous vehicles;invariant scene features;learning representations;multiagent decoding;multiagent trajectory decoding;QCNet;query-based decoder;query-centric trajectory prediction;refinement module;scene encoding","","1","","56","IEEE","22 Aug 2023","","","IEEE","IEEE Conferences"
"Large-scale Localization Datasets in Crowded Indoor Spaces","D. Lee; S. Ryu; S. Yeon; Y. Lee; D. Kim; C. Han; Y. Cabon; P. Weinzaepfel; N. Guérin; G. Csurka; M. Humenberger",Naver Labs; Naver Labs; Naver Labs; Naver Labs; Naver Labs; Naver Labs; Naver Labs Europe; Naver Labs Europe; Naver Labs Europe; Naver Labs Europe; Naver Labs Europe,"2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)","2 Nov 2021","2021","","","3226","3235","Estimating the precise location of a camera using visual localization enables interesting applications such as augmented reality or robot navigation. This is particularly useful in indoor environments where other localization technologies, such as GNSS, fail. Indoor spaces impose interesting challenges on visual localization algorithms: occlusions due to people, textureless surfaces, large viewpoint changes, low light, repetitive textures, etc. Existing indoor datasets are either comparably small or do only cover a subset of the mentioned challenges. In this paper, we introduce 5 new indoor datasets for visual localization in challenging real-world environments. They were captured in a large shopping mall and a large metro station in Seoul, South Korea, using a dedicated mapping platform consisting of 10 cameras and 2 laser scanners. In order to obtain accurate ground truth camera poses, we developed a robust LiDAR SLAM which provides initial poses that are then refined using a novel structure-from-motion based optimization. We present a benchmark of modern visual localization algorithms on these challenging datasets showing superior performance of structure-based methods using robust image features. The datasets are available at: https://naverlabs.com/datasets","2575-7075","978-1-6654-4509-2","10.1109/CVPR46437.2021.00324","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9578849","","Location awareness;Visualization;Laser radar;Simultaneous localization and mapping;Wheels;Benchmark testing;Cameras","augmented reality;cameras;feature extraction;image motion analysis;image sensors;image texture;mobile robots;navigation;object detection;optical radar;optical scanners;path planning;pose estimation;robot vision;SLAM (robots);solid modelling","indoor environments;localization technologies;interesting challenges;textureless surfaces;viewpoint changes;low light;indoor datasets;mentioned challenges;real-world environments;accurate ground truth camera;modern visual localization algorithms;challenging datasets;scale localization datasets;crowded indoor spaces;precise location;augmented reality","","19","","65","IEEE","2 Nov 2021","","","IEEE","IEEE Conferences"
"Crowd3D: Towards Hundreds of People Reconstruction from a Single Image","H. Wen; J. Huang; H. Cui; H. Lin; Y. -K. Lai; L. Fang; K. Li","Tianjin University, China; Tianjin University, China; Tianjin University, China; Tsinghua University, China; Cardiff University, United Kingdom; Tsinghua University, China; Tianjin University, China","2023 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)","22 Aug 2023","2023","","","8937","8946","Image-based multi-person reconstruction in wide-field large scenes is critical for crowd analysis and security alert. However, existing methods cannot deal with large scenes containing hundreds of people, which encounter the challenges of large number of people, large variations in human scale, and complex spatial distribution. In this paper, we propose Crowd3D, the first framework to reconstruct the 3D poses, shapes and locations of hundreds of people with global consistency from a single large-scene image. The core of our approach is to convert the problem of complex crowd localization into pixel localization with the help of our newly defined concept, Human-scene Virtual Interaction Point (HVIP). To reconstruct the crowd with global consistency, we propose a progressive reconstruction network based on HVIP by pre-estimating a scene-level camera and a ground plane. To deal with a large number of persons and various human sizes, we also design an adaptive human-centric cropping scheme. Besides, we contribute a benchmark dataset, LargeCrowd, for crowd reconstruction in a large scene. Experimental results demonstrate the effectiveness of the proposed method. The code and the dataset are available at http://cic.tju.edu.cn/faculty/likun/projects/Crowd3D.","2575-7075","979-8-3503-0129-8","10.1109/CVPR52729.2023.00863","National Natural Science Foundation of China(grant numbers:62122058,62125106,61860206003,62171317); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10203526","3D from single images","Location awareness;Computer vision;Three-dimensional displays;Graphical models;Codes;Shape;Pattern recognition","cameras;image reconstruction;image representation;image resolution;virtual reality","complex crowd localization;complex spatial distribution;crowd analysis;crowd reconstruction;global consistency;human scale;human sizes;human-centric cropping scheme;Human-scene Virtual Interaction Point;HVIP;multiperson reconstruction;newly defined concept;people reconstruction;pixel localization;progressive reconstruction network;scene-level camera;security alert;single image;single large-scene image;towards hundreds","","1","","46","IEEE","22 Aug 2023","","","IEEE","IEEE Conferences"
"Introvert: Human Trajectory Prediction via Conditional 3D Attention","N. Shafiee; T. Padir; E. Elhamifar",Northeastern University; Northeastern University; Northeastern University,"2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)","2 Nov 2021","2021","","","16810","16820","Predicting human trajectories is an important component of autonomous moving platforms, such as social robots and self-driving cars. Human trajectories are affected by both the physical features of the environment and social interactions with other humans. Despite recent surge of studies on human path prediction, most works focus on static scene information, therefore, cannot leverage the rich dynamic visual information of the scene. In this work, we propose Introvert, a model which predicts human path based on his/her observed trajectory and the dynamic scene context, captured via a conditional 3D visual attention mechanism working on the input video. Introvert infers both environment constraints and social interactions through observing the dynamic scene instead of communicating with other humans, hence, its computational cost is independent of how crowded the surrounding of a target human is. In addition, to focus on relevant interactions and constraints for each human, Introvert conditions its 3D attention model on the observed trajectory of the target human to extract and focus on relevant spatiotemporal primitives. Our experiments on five publicly available datasets show that the Introvert improves the prediction errors of the state of the art.","2575-7075","978-1-6654-4509-2","10.1109/CVPR46437.2021.01654","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9577353","","Visualization;Solid modeling;Three-dimensional displays;Computational modeling;Predictive models;Trajectory;Computational efficiency","computer vision;data visualisation;feature extraction;human-robot interaction;image motion analysis;mobile robots;object detection;object tracking;robot vision;spatiotemporal phenomena;video signal processing;visual perception","human trajectory prediction;human trajectories;autonomous moving platforms;social robots;social interactions;human path prediction;static scene information;rich dynamic visual information;observed trajectory;dynamic scene context;conditional 3D visual attention mechanism;environment constraints;target human;relevant interactions;human Introvert conditions its 3D attention model;prediction errors","","17","","56","IEEE","2 Nov 2021","","","IEEE","IEEE Conferences"
"Rawgment: Noise-Accounted RAW Augmentation Enables Recognition in a Wide Variety of Environments","M. Yoshimura; J. Otsuka; A. Irie; T. Ohashi",Sony Group Corporation; Sony Group Corporation; Sony Group Corporation; Sony Group Corporation,"2023 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)","22 Aug 2023","2023","","","14007","14017","Image recognition models that work in challenging environments (e.g., extremely dark, blurry, or high dynamic range conditions) must be useful. However, creating training datasets for such environments is expensive and hard due to the difficulties of data collection and annotation. It is desirable if we could get a robust model without the need for hard-to-obtain datasets. One simple approach is to apply data augmentation such as color jitter and blur to standard RGB (sRGB) images in simple scenes. Unfortunately, this approach struggles to yield realistic images in terms of pixel intensity and noise distribution due to not considering the non-linearity of Image Signal Processors (ISPs) and noise characteristics of image sensors. Instead, we propose a noise-accounted RAW image augmentation method. In essence, color jitter and blur augmentation are applied to a RAW image before applying non-linear ISP, resulting in realistic intensity. Furthermore, we introduce a noise amount alignment method that calibrates the domain gap in the noise property caused by the augmentation. We show that our proposed noise-accounted RAW augmentation method doubles the image recognition accuracy in challenging environments only with simple training data.","2575-7075","979-8-3503-0129-8","10.1109/CVPR52729.2023.01346","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10204594","Computational imaging","Training;Computer vision;Image recognition;Program processors;Image color analysis;Training data;Jitter","image colour analysis;image denoising;image recognition;jitter;learning (artificial intelligence)","blur augmentation;color jitter;data augmentation;data collection;high dynamic range conditions;image recognition accuracy;Image recognition models;image sensors;Image Signal Processors;noise amount alignment method;noise characteristics;noise distribution;noise property;noise-accounted RAW augmentation method;noise-accounted RAW image augmentation method;nonlinear ISP;pixel intensity;Rawgment;realistic images;realistic intensity;simple training data;sRGB images;standard RGB images;training datasets","","","","54","IEEE","22 Aug 2023","","","IEEE","IEEE Conferences"
"Pseudo-Labels for Supervised Learning on Dynamic Vision Sensor Data, Applied to Object Detection Under Ego-Motion","N. F. Y. Chen","DSO National Laboratories, Singapore","2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)","16 Dec 2018","2018","","","757","75709","In recent years, dynamic vision sensors (DVS), also known as event-based cameras or neuromorphic sensors, have seen increased use due to various advantages over conventional frame-based cameras. Using principles inspired by the retina, its high temporal resolution overcomes motion blurring, its high dynamic range overcomes extreme illumination conditions and its low power consumption makes it ideal for embedded systems on platforms such as drones and self-driving cars. However, event-based data sets are scarce and labels are even rarer for tasks such as object detection. We transferred discriminative knowledge from a state-of-the-art frame-based convolutional neural network (CNN) to the event-based modality via intermediate pseudo-labels, which are used as targets for supervised learning. We show, for the first time, event-based car detection under ego-motion in a real environment at 100 frames per second with a test average precision of 40.3% relative to our annotated ground truth. The event-based car detector handles motion blur and poor illumination conditions despite not explicitly trained to do so, and even complements frame-based CNN detectors, suggesting that it has learnt generalized visual representations.","2160-7516","978-1-5386-6100-0","10.1109/CVPRW.2018.00107","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8575257","","Vision sensors;Cameras;Vehicle dynamics;Object detection;Voltage control;Supervised learning;Automobiles","cameras;computer vision;convolutional neural nets;embedded systems;image motion analysis;image sensors;object detection;object recognition;supervised learning","neuromorphic sensors;high temporal resolution;motion blurring;low power consumption;embedded systems;self-driving cars;object detection;discriminative knowledge;intermediate pseudolabels;supervised learning;event-based car detection;ego-motion;dynamic vision sensor data;DVS;event-based cameras;frame-based cameras;convolutional neural network;illumination conditions;frame-based CNN detectors","","33","1","30","IEEE","16 Dec 2018","","","IEEE","IEEE Conferences"
"Stochastic Trajectory Prediction via Motion Indeterminacy Diffusion","T. Gu; G. Chen; J. Li; C. Lin; Y. Rao; J. Zhou; J. Lu","University of California, Los Angeles; MBZUAI; Tsinghua University; SenseTime Research; Tsinghua University; Tsinghua University; Tsinghua University","2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)","27 Sep 2022","2022","","","17092","17101","Human behavior has the nature of indeterminacy, which requires the pedestrian trajectory prediction system to model the multi-modality of future motion states. Unlike existing stochastic trajectory prediction methods which usually use a latent variable to represent multi-modality, we explicitly simulate the process of human motion variation from indeterminate to determinate. In this paper, we present a new framework to formulate the trajectory prediction task as a reverse process of motion indeterminacy diffusion (MID), in which we progressively discard indeterminacy from all the walkable areas until reaching the desired trajectory. This process is learned with a parameterized Markov chain conditioned by the observed trajectories. We can adjust the length of the chain to control the degree of indeterminacy and balance the diversity and determinacy of the predictions. Specifically, we encode the history behavior information and the social interactions as a state embedding and devise a Transformer-based diffusion model to capture the temporal dependencies of trajectories. Extensive experiments on the human trajectory prediction benchmarks including the Stanford Drone and ETH/UCY datasets demonstrate the superiority of our method. Code is available at https://github.com/gutianpei/MID.","2575-7075","978-1-6654-6946-3","10.1109/CVPR52688.2022.01660","National Natural Science Foundation of China(grant numbers:62125603,U1813218); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9878855","Navigation and autonomous driving","Navigation;Predictive models;Markov processes;Transformers;Trajectory;Behavioral sciences;Pattern recognition","image motion analysis;learning (artificial intelligence);Markov processes;network theory (graphs);stochastic processes","stochastic trajectory prediction methods;multimodality;human motion variation;trajectory prediction task;reverse process;motion indeterminacy diffusion;parameterized Markov chain;observed trajectories;history behavior information;Transformer-based diffusion model;human trajectory prediction benchmarks;human behavior;pedestrian trajectory prediction system;future motion states","","19","","55","IEEE","27 Sep 2022","","","IEEE","IEEE Conferences"
"Robust Test-Time Adaptation in Dynamic Scenarios","L. Yuan; B. Xie; S. Li","School of Computer Science and Technology, Beijing Institute of Technology; School of Computer Science and Technology, Beijing Institute of Technology; School of Computer Science and Technology, Beijing Institute of Technology","2023 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)","22 Aug 2023","2023","","","15922","15932","Test-time adaptation (TTA) intends to adapt the pretrained model to test distributions with only unlabeled test data streams. Most of the previous TTA methods have achieved great success on simple test data streams such as independently sampled data from single or multiple distributions. However, these attempts may fail in dynamic scenarios of real-world applications like autonomous driving, where the environments gradually change and the test data is sampled correlatively over time. In this work, we explore such practical test data streams to deploy the model on the fly, namely practical test-time adaptation (PTTA). To do so, we elaborate a Robust Test-Time Adaptation (RoTTA) method against the complex data stream in PTTA. More specifically, we present a robust batch normalization scheme to estimate the normalization statistics. Meanwhile, a memory bank is utilized to sample category-balanced data with consideration of timeliness and uncertainty. Further, to stabilize the training procedure, we develop a time-aware reweighting strategy with a teacher-student model. Extensive experiments prove that RoTTA enables continual test-time adaptation on the correlatively sampled data streams. Our method is easy to implement, making it a good choice for rapid deployment. The code is publicly available at https://github.com/BIT-DA/RoTTA","2575-7075","979-8-3503-0129-8","10.1109/CVPR52729.2023.01528","National Key R&D Program of China(grant numbers:2021YFB3301503); National Natural Science Foundation of China(grant numbers:61902028); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10204405","Transfer;meta;low-shot;continual;or long-tail learning","Training;Adaptation models;Computer vision;Uncertainty;Codes;Computational modeling;Data models","data handling","complex data stream;continual test-time adaptation;correlatively sampled data streams;dynamic scenarios;independently sampled data;practical test data streams;practical test-time adaptation;pretrained model;PTTA;robust batch normalization scheme;robust test-time adaptation method;RoTTA;sample category-balanced data;simple test data streams;teacher-student model;time-aware reweighting strategy;TTA methods;unlabeled test data streams","","","","85","IEEE","22 Aug 2023","","","IEEE","IEEE Conferences"
"MERLOT RESERVE: Neural Script Knowledge through Vision and Language and Sound","R. Zellers; J. Lu; X. Lu; Y. Yu; Y. Zhao; M. Salehi; A. Kusupati; J. Hessel; A. Farhadi; Y. Choi","Paul G. Allen School of Computer Science & Engineering, University of Washington; Allen Institute for Artificial Intelligence; Allen Institute for Artificial Intelligence; Allen Institute for Artificial Intelligence; Allen Institute for Artificial Intelligence; University of Edinburgh; Paul G. Allen School of Computer Science & Engineering, University of Washington; Allen Institute for Artificial Intelligence; Paul G. Allen School of Computer Science & Engineering, University of Washington; Allen Institute for Artificial Intelligence","2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)","27 Sep 2022","2022","","","16354","16366","As humans, we navigate a multimodal world, building a holistic understanding from all our senses. We introduce @MERLOT RESERVE, a model that represents videos jointly over time - through a new training objective that learns from audio, subtitles, and video frames. Given a video, we replace snippets of text and audio with a MASK token; the model learns by choosing the correct masked-out snippet. Our objective learns faster than alternatives, and performs well at scale: we pretrain on 20 million YouTube videos. Empirical results show that @MERLOT RESERVE learns strong multimodal representations. When finetuned, it sets state-of-the-art on Visual Commonsense Reasoning (VCR), TVQA, and Kinetics-600; outperforming prior work by 5%, 7%, and 1.5% respectively. Ablations show that these tasks benefit from audio pretraining - even VCR, a QA task centered around images (without sound). Moreover, our objective enables out-of-the-box prediction, revealing strong multimodal commonsense understanding. In a fully zero-shot setting, our model obtains competitive results on four video tasks, even outperforming supervised approaches on the recently proposed Situated Reasoning (STAR) benchmark. We analyze why audio enables better vision-language representations, suggesting significant opportunities for future research. We conclude by discussing ethical and societal implications of multimodal pretraining.","2575-7075","978-1-6654-6946-3","10.1109/CVPR52688.2022.01589","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9879062","Vision + language; Representation learning; Video analysis and understanding; Visual reasoning","Training;Representation learning;Visualization;Ethics;Video on demand;Navigation;Stars","computer aided instruction;inference mechanisms;learning (artificial intelligence);natural language processing;social networking (online);text analysis;video signal processing","neural script knowledge;multimodal world;holistic understanding;@MERLOT RESERVE;training objective;video frames;snippet;MASK token;objective learns;20 million YouTube videos;strong multimodal representations;VCR;outperforming prior work;audio pretraining;strong multimodal commonsense understanding;video tasks;vision-language representations;multimodal pretraining","","25","","134","IEEE","27 Sep 2022","","","IEEE","IEEE Conferences"
"DR.VIC: Decomposition and Reasoning for Video Individual Counting","T. Han; L. Bai; J. Gao; Q. Wang; W. Ouyang","Northwestern Polytechnical University, Xi'an, P.R. China; SenseTime Computer Vision Group, The University of Sydney, Australia; Northwestern Polytechnical University, Xi'an, P.R. China; Northwestern Polytechnical University, Xi'an, P.R. China; SenseTime Computer Vision Group, The University of Sydney, Australia","2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)","27 Sep 2022","2022","","","3073","3082","Pedestrian counting is a fundamental tool for under-standing pedestrian patterns and crowd flow analysis. Existing works (e.g., image-level pedestrian counting, cross-line crowd counting et al.) either only focus on the image-level counting or are constrained to the manual annotation of lines. In this work, we propose to conduct the pedes-trian counting from a new perspective - Video Individual Counting (VIC), which counts the total number of individual pedestrians in the given video (a person is only counted once). Instead of relying on the Multiple Object Tracking (MOT) techniques, we propose to solve the problem by decomposing all pedestrians into the initial pedestrians who existed in the first frame and the new pedestrians with separate identities in each following frame. Then, an end-to-end Decomposition and Reasoning Network (DRNet) is designed to predict the initial pedestrian count with the density estimation method and reason the new pedestrian's count of each frame with the differentiable optimal transport. Extensive experiments are conducted on two datasets with congested pedestrians and diverse scenes, demonstrating the effectiveness of our method over baselines with great superiority in counting the individual pedestrians. Code: https://github.com/taohan10200/DRNet.","2575-7075","978-1-6654-6946-3","10.1109/CVPR52688.2022.00309","National Natural Science Foundation of China(grant numbers:U21B2041,U1864204); Australian Research Council(grant numbers:DP200103223); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9879966","Video analysis and understanding; Scene analysis and understanding","Computer vision;Codes;Annotations;Estimation;Manuals;Cognition;Pattern recognition","image sequences;object detection;object tracking;optimisation;pedestrians;traffic engineering computing;video signal processing;video surveillance","perspective - Video Individual Counting;individual pedestrians;Multiple Object Tracking techniques;initial pedestrians;initial pedestrian count;congested pedestrians;DR.VIC;pedestrian patterns;crowd flow analysis;image-level pedestrian counting;cross-line crowd;image-level counting;pedes-trian counting","","2","","64","IEEE","27 Sep 2022","","","IEEE","IEEE Conferences"
"PoseExaminer: Automated Testing of Out-of-Distribution Robustness in Human Pose and Shape Estimation","Q. Liu; A. Kortylewski; A. Yuille",Johns Hopkins University; Max Planck Institute for Informatics; Johns Hopkins University,"2023 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)","22 Aug 2023","2023","","","672","681","Human pose and shape (HPS) estimation methods achieve remarkable results. However, current HPS bench-marks are mostly designed to test models in scenarios that are similar to the training data. This can lead to critical situations in real-world applications when the observed data differs significantly from the training data and hence is out-of-distribution (OOD). It is therefore important to test and improve the OOD robustness of HPS methods. To address this fundamental problem, we develop a simulator that can be controlled in a fine-grained manner using interpretable parameters to explore the manifold of images of human pose, e.g. by varying poses, shapes, and clothes. We introduce a learning-based testing method, termed PoseExaminer, that automatically diagnoses HPS algorithms by searching over the parameter space of human pose images to find the failure modes. Our strategy for exploring this high-dimensional parameter space is a multiagent reinforcement learning system, in which the agents collaborate to explore different parts of the parameter space. We show that our PoseExaminer discovers a variety of limitations in current state-of-the-art models that are relevant in real-world scenarios but are missed by current benchmarks. For example, it finds large regions of realistic human poses that are not predicted correctly, as well as reduced performance for humans with skinny and corpulent body shapes. In addition, we show that fine-tuning HPS methods by exploiting the failure modes found by PoseExaminer improve their robustness and even their performance on standard benchmarks by a significant margin. The code are available for research purposes at https://github.com/qihao0671PoseExaminer.","2575-7075","979-8-3503-0129-8","10.1109/CVPR52729.2023.00072","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10205124","Humans: Face;body;pose;gesture;movement","Manifolds;Shape;Training data;Estimation;Reinforcement learning;Benchmark testing;Robustness","learning (artificial intelligence);multi-agent systems;pose estimation;reinforcement learning","automated testing;corpulent body shapes;critical situations;current benchmarks;current HPS bench-marks;current state-of-the-art models;failure modes;fine-grained manner;high-dimensional parameter space;HPS algorithms;HPS methods;human pose images;interpretable parameters;learning-based;multiagent reinforcement learning system;observed data differs;OOD robustness;out-of-distribution robustness;PoseExaminer discovers;real-world applications;realistic human poses;shape estimation methods;skinny body shapes;termed PoseExaminer;training data;varying poses","","","","58","IEEE","22 Aug 2023","","","IEEE","IEEE Conferences"
"PhyIR: Physics-based Inverse Rendering for Panoramic Indoor Images","Z. Li; L. Wang; X. Huang; C. Pan; J. Yang",Realsee; Realsee; Realsee; Realsee; Northwestern Polytechnical University,"2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)","27 Sep 2022","2022","","","12703","12713","Inverse rendering of complex material such as glossy, metal and mirror material is a long-standing ill-posed problem in this area, which has not been well solved. Previous approaches cannot tackle them well due to simplified BRDF and unsuitable illumination representations. In this paper, we present PhyIR, a neural inverse rendering method with a more completed SVBRDF representation and a physics-based in-network rendering layer, which can handle complex material and incorporate physical constraints by re-rendering realistic and detailed specular reflectance. Our framework estimates geometry, material and Spatially-Coherent (SC) illumination from a single indoor panorama. Due to the lack of panoramic datasets with completed SVBRDF and full-spherical light probes, we introduce an artist-designed dataset named FutureHouse with high-quality geometry, SVBRDF and per-pixel Spatially-Varying (SV) lighting. To ensure the coherence of SV lighting, a novel SC loss is proposed. Extensive experiments on both synthetic and real-world data show that the proposed method outperforms the state-of-the-arts quantitatively and qualitatively, and is able to produce photorealistic results for a number of applications such as dynamic virtual object insertion.","2575-7075","978-1-6654-6946-3","10.1109/CVPR52688.2022.01238","National Natural Science Foundation of China(grant numbers:62002295); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9879361","Physics-based vision and shape-from-X; 3D from single images; Datasets and evaluation; Deep learning architectures and techniques; Scene analysis and understanding; Vision + graphics","Geometry;Reflectivity;Three-dimensional displays;Lighting;Metals;Computer architecture;Rendering (computer graphics)","geometry;image representation;indoor environment;lighting;rendering (computer graphics)","high-quality geometry;PhyIR;physics-based inverse rendering;panoramic indoor;complex material;mirror material;simplified BRDF;illumination representations;neural inverse rendering method;physics-based in-network rendering layer;physical constraints;re-rendering;specular reflectance;spatially-coherent illumination;single indoor panorama;panoramic datasets;full-spherical light probes;artist-designed dataset;FutureHouse;SVBRDF representation;per-pixel spatially-varying lighting;dynamic virtual object insertion","","5","","57","IEEE","27 Sep 2022","","","IEEE","IEEE Conferences"
"Two Body Problem: Collaborative Visual Task Completion","U. Jain; L. Weihs; E. Kolve; M. Rastegari; S. Lazebnik; A. Farhadi; A. G. Schwing; A. Kembhavi",UIUC; PRIOR @ Allen Institute for AI; PRIOR @ Allen Institute for AI; PRIOR @ Allen Institute for AI; UIUC; PRIOR @ Allen Institute for AI; UIUC; PRIOR @ Allen Institute for AI,"2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)","9 Jan 2020","2019","","","6682","6692","Collaboration is a necessary skill to perform tasks that are beyond one agent's capabilities. Addressed extensively in both conventional and modern AI, multi-agent collaboration has often been studied in the context of simple grid worlds. We argue that there are inherently visual aspects to collaboration which should be studied in visually rich environments. A key element in collaboration is communication that can be either explicit, through messages, or implicit, through perception of the other agents and the visual world. Learning to collaborate in a visual environment entails learning (1) to perform the task, (2) when and what to communicate, and (3) how to act based on these communications and the perception of the visual world. In this paper we study the problem of learning to collaborate directly from pixels in AI2-THOR and demonstrate the benefits of explicit and implicit modes of communication to perform visual tasks. Refer to our project page for more details: https://prior.allenai.org/projects/two-body-problem.","2575-7075","978-1-7281-3293-8","10.1109/CVPR.2019.00685","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8953359","Visual Reasoning;Recognition: Detection;Categorization;Retrieval; Scene Analysis and Understanding","","data visualisation;learning (artificial intelligence);multi-agent systems","collaborative visual task completion;conventional AI;modern AI;multiagent collaboration;inherently visual aspects;visual world;visual environment;AI2-THOR;explicit modes;implicit modes;visual tasks","","21","","92","IEEE","9 Jan 2020","","","IEEE","IEEE Conferences"
"Divide-and-Conquer for Lane-Aware Diverse Trajectory Prediction","S. Narayanan; R. Moslemi; F. Pittaluga; B. Liu; M. Chandraker",NEC Labs America; NEC Labs America; NEC Labs America; NEC Labs America; NEC Labs America,"2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)","2 Nov 2021","2021","","","15794","15803","Trajectory prediction is a safety-critical tool for autonomous vehicles to plan and execute actions. Our work addresses two key challenges in trajectory prediction, learning multimodal outputs, and better predictions by imposing constraints using driving knowledge. Recent methods have achieved strong performances using Multi-Choice Learning objectives like winner-takes-all (WTA) or best-of-many. But the impact of those methods in learning diverse hypotheses is under-studied as such objectives highly depend on their initialization for diversity. As our first contribution, we propose a novel Divide-And-Conquer (DAC) approach that acts as a better initialization technique to WTA objective, resulting in diverse outputs without any spurious modes. Our second contribution is a novel trajectory prediction framework called ALAN that uses existing lane centerlines as anchors to provide trajectories constrained to the input lanes. Our framework provides multi-agent trajectory outputs in a forward pass by capturing interactions through hypercolumn descriptors and incorporating scene information in the form of rasterized images and per-agent lane anchors. Experiments on synthetic and real data show that the proposed DAC captures the data distribution better compare to other WTA family of objectives. Further, we show that our ALAN approach provides on par or better performance with SOTA methods evaluated on Nuscenes urban driving benchmark.","2575-7075","978-1-6654-4509-2","10.1109/CVPR46437.2021.01554","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9577563","","Couplings;Context-aware services;Computer vision;Semantics;Tools;Benchmark testing;Trajectory","divide and conquer methods;learning (artificial intelligence);mobile robots;multi-agent systems;object detection;road vehicles;traffic engineering computing;trajectory control","lane centerlines;input lanes;multiagent trajectory;per-agent lane anchors;lane-aware diverse trajectory prediction;safety-critical tool;autonomous vehicles;multimodal outputs;driving knowledge;diverse hypotheses;initialization technique;WTA objective;diverse outputs;trajectory prediction framework;multichoice learning objectives;DAC;ALAN approach;SOTA methods;Nuscenes urban driving benchmark","","17","","46","IEEE","2 Nov 2021","","","IEEE","IEEE Conferences"
"Whose Track Is It Anyway? Improving Robustness to Tracking Errors with Affinity-based Trajectory Prediction","X. Weng; B. Ivanovic; K. Kitani; M. Pavone","Robotics Institute, Carnegie Mellon University; NVIDIA Research; Robotics Institute, Carnegie Mellon University; Department of Aeronautics and Astronautics, Stanford University","2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)","27 Sep 2022","2022","","","6563","6572","Multi-agent trajectory prediction is critical for planning and decision-making in human-interactive autonomous systems, such as self-driving cars. However, most prediction models are developed separately from their upstream perception (detection and tracking) modules, assuming ground truth past trajectories as inputs. As a result, their performance degrades significantly when using real-world noisy tracking results as inputs. This is typically caused by the propagation of errors from tracking to prediction, such as noisy tracks, fragments and identity switches. To alleviate this propagation of errors, we propose a new prediction paradigm that uses detections and their affinity matrices across frames as inputs, removing the need for error- prone data association during tracking. Since affinity matrices contain “soft” information about the similarity and identity of detections across frames, making prediction directly from affinity matrices retains strictly more information than making prediction from the tracklets generated by data association. Experiments on large-scale, real-world autonomous driving datasets show that our affinity-based prediction scheme 11Our project website is at https://www.xinshuoweng.com/projects/Affinipred. reduces overall prediction errors by up to 57.9%, in comparison to standard prediction pipelines that use tracklets as inputs, with even more significant error reduction (up to 88.6%) if restricting the evaluation to challenging scenarios with tracking errors.","2575-7075","978-1-6654-6946-3","10.1109/CVPR52688.2022.00646","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9879091","Motion and tracking; Navigation and autonomous driving","Tracking;Navigation;Pipelines;Predictive models;Robustness;Trajectory;Planning","decision making;feature extraction;mobile robots;multi-agent systems;object detection;object tracking;pattern clustering;road vehicles;sensor fusion;target tracking;tracking;trajectory control;video signal processing","prediction models;ground truth past trajectories;real-world noisy tracking results;noisy tracks;prediction paradigm;affinity matrices;error- prone data association;real-world autonomous driving datasets;affinity-based prediction scheme;prediction errors;standard prediction pipelines;significant error reduction;tracking errors;affinity-based trajectory prediction;multiagent trajectory prediction;decision-making;human-interactive autonomous systems;self-driving cars","","5","","50","IEEE","27 Sep 2022","","","IEEE","IEEE Conferences"
"Collaboration Helps Camera Overtake LiDAR in 3D Detection","Y. Hu; Y. Lu; R. Xu; W. Xie; S. Chen; Y. Wang","Cooperative Medianet Innovation Center, Shanghai Jiao Tong University; Cooperative Medianet Innovation Center, Shanghai Jiao Tong University; University of California, Los Angeles; Cooperative Medianet Innovation Center, Shanghai Jiao Tong University; Cooperative Medianet Innovation Center, Shanghai Jiao Tong University; Shanghai AI Laboratory","2023 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)","22 Aug 2023","2023","","","9243","9252","Camera-only 3D detection provides an economical solution with a simple configuration for localizing objects in 3D space compared to LiDAR-based detection systems. However, a major challenge lies in precise depth estimation due to the lack of direct 3D measurements in the input. Many previous methods attempt to improve depth estimation through network designs, e.g., deformable layers and larger receptive fields. This work proposes an orthogonal direction, improving the camera-only 3D detection by introducing multi-agent collaborations. Our proposed collaborative camera-only 3D detection (CoCa3D) enables agents to share complementary information with each other through communication. Meanwhile, we optimize communication efficiency by selecting the most informative cues. The shared messages from multiple view-points disambiguate the single-agent estimated depth and complement the occluded and long-range regions in the single-agent view. We evaluate CoCa3D in one real-world dataset and two new simulation datasets. Results show that CoCa3D improves previous SOTA performances by 44.21% on DAIR-V2X, 30.60% on OPV2V+, 12.59% on CoPerception-UAVs+ for AP@70. Our preliminary results show a potential that with sufficient collaboration, the camera might overtake LiDAR in some practical scenarios. We released the dataset and code.","2575-7075","979-8-3503-0129-8","10.1109/CVPR52729.2023.00892","National Key RD Program of China(grant numbers:2021ZD0112801); NSFC(grant numbers:62171276); Science and Technology Commission of Shanghai Municipal(grant numbers:21511100900,22DZ2229005); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10203708","3D from multi-view and sensors","Computer vision;Three-dimensional displays;Laser radar;Codes;Collaboration;Estimation;Cameras","autonomous aerial vehicles;cameras;feature extraction;multi-agent systems;object detection;optical radar;traffic engineering computing","camera-only 3D detection;CoCa3D;collaboration helps camera overtake LiDAR;collaborative camera-only;complement;detection systems;direct 3D measurements;multiagent collaborations;precise depth estimation;single-agent estimated depth;single-agent view","","","","45","IEEE","22 Aug 2023","","","IEEE","IEEE Conferences"
"Safe Local Motion Planning with Self-Supervised Freespace Forecasting","P. Hu; A. Huang; J. Dolan; D. Held; D. Ramanan","Robotics Institute, Carnegie Mellon University; Robotics Institute, Carnegie Mellon University; Robotics Institute, Carnegie Mellon University; Robotics Institute, Carnegie Mellon University; Robotics Institute, Carnegie Mellon University","2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)","2 Nov 2021","2021","","","12727","12736","Safe local motion planning for autonomous driving in dynamic environments requires forecasting how the scene evolves. Practical autonomy stacks adopt a semantic object-centric representation of a dynamic scene and build object detection, tracking, and prediction modules to solve forecasting. However, training these modules comes at an enormous human cost of manually annotated objects across frames. In this work, we explore future freespace as an alternative representation to support motion planning. Our key intuition is that it is important to avoid straying into occupied space regardless of what is occupying it. Importantly, computing ground-truth future freespace is annotation-free. First, we explore freespace forecasting as a self-supervised learning task. We then demonstrate how to use forecasted freespace to identify collision-prone plans from off-the-shelf motion planners. Finally, we propose future freespace as an additional source of annotation-free supervision. We demonstrate how to integrate such supervision into the learning-based planners. Experimental results on nuScenes and CARLA suggest both approaches lead to a significant reduction in collision rates.1","2575-7075","978-1-6654-4509-2","10.1109/CVPR46437.2021.01254","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9577522","","Training;Costs;Tracking;Dynamics;Semantics;Object detection;Planning","control engineering computing;object detection;path planning;road traffic control;supervised learning;traffic engineering computing","object detection;prediction modules;human cost;manually annotated objects;ground-truth future freespace;self-supervised learning task;collision-prone plans;off-the-shelf motion planners;annotation-free supervision;safe local motion planning;self-supervised freespace forecasting;dynamic environments;autonomy stacks;semantic object-centric representation;dynamic scene representation;object tracking;CARLA;nuScenes","","10","","47","IEEE","2 Nov 2021","","","IEEE","IEEE Conferences"
"The Wisdom of Crowds: Temporal Progressive Attention for Early Action Prediction","A. Stergiou; D. Damen","Vrije University of Brussels, Belgium; University of Bristol, UK","2023 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)","22 Aug 2023","2023","","","14709","14719","Early action prediction deals with inferring the ongoing action from partially-observed videos, typically at the outset of the video. We propose a bottleneck-based attention model that captures the evolution of the action, through progressive sampling over fine-to-coarse scales. Our proposed Temporal Progressive (TemPr) model is composed of multiple attention towers, one for each scale. The predicted action label is based on the collective agreement considering confidences of these towers. Extensive experiments over four video datasets showcase state-of-the-art performance on the task of Early Action Prediction across a range of encoder architectures. We demonstrate the effectiveness and consistency of TemPr through detailed ablations.††Code is available at: https://tinyurl.com/temprog","2575-7075","979-8-3503-0129-8","10.1109/CVPR52729.2023.01413","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10205036","Video: Action and event understanding","Computer vision;Aggregates;Poles and towers;Computer architecture;Probability distribution;Pattern recognition;Task analysis","deep learning (artificial intelligence);human activity recognition;video signal processing","action label;bottleneck-based attention model;early action prediction;encoder architectures;fine-to-coarse scales;multiple attention towers;ongoing action;partially-observed videos;progressive sampling;temporal progressive attention model;TemPr;video datasets","","","","73","IEEE","22 Aug 2023","","","IEEE","IEEE Conferences"
"“Seeing” Electric Network Frequency from Events","L. Xu; G. Hua; H. Zhang; L. Yu; N. Qiao","Wuhan University, Wuhan, China; Institute for Infocomm Research (I2R), A *STAR, Singapore; Wuhan University, Wuhan, China; Wuhan University, Wuhan, China; SynSense Tech. Co. Ltd., Chengdu, China","2023 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)","22 Aug 2023","2023","","","18022","18031","Most of the artificial lights fluctuate in response to the grid's alternating current and exhibit subtle variations in terms of both intensity and spectrum, providing the potential to estimate the Electric Network Frequency (ENF)from conventional frame-based videos. Nevertheless, the performance of Video-based ENF (V-ENF) estimation largely re-lies on the imaging quality and thus may suffer from significant interference caused by non-ideal sampling, motion, and extreme lighting conditions. In this paper, we show that the ENF can be extracted without the above limitations from a new modality provided by the so-called event camera, a neuromorphic sensor that encodes the light intensity variations and asynchronously emits events with extremely high temporal resolution and high dynamic range. Specifically, we first formulate and validate the physical mechanism for the ENF captured in events, and then propose a simple yet robust Event-based ENF (E-ENF) estimation method through mode filtering and harmonic enhancement. Furthermore, we build an Event-Video ENF Dataset (EV-ENFD) that records both events and videos in diverse scenes. Extensive experiments on EV-ENFD demonstrate that our proposed E-ENF method can extract more accurate ENF traces, outperforming the conventional V-ENF by a large margin, especially in challenging environments with object motions and extreme lighting conditions. The code and dataset are available at https://github.com/x1x-creater/E-ENF.","2575-7075","979-8-3503-0129-8","10.1109/CVPR52729.2023.01728","National Natural Science Foundation of China(grant numbers:62271354); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10203338","Computational imaging","Neuromorphics;Estimation;Interference;Cameras;High dynamic range;Recording;Pattern recognition","cameras;feature extraction;image enhancement;image filtering;image motion analysis;image resolution;image sampling;image sensors;object detection;power engineering computing;power grids;video signal processing","accurate ENF traces;artificial lights;conventional frame-based videos;conventional V-ENF;E-ENF method;EV-ENFD;event camera;Event-Video ENF Dataset;extreme lighting conditions;extremely high temporal resolution;grid alternating current;harmonic enhancement;high dynamic range;imaging quality;light intensity variations;mode filtering;neuromorphic sensor;nonideal sampling;object motions;robust event-based ENF estimation method;seeing electric network frequency;Video-based ENF estimation","","1","","35","IEEE","22 Aug 2023","","","IEEE","IEEE Conferences"
"Multiple Object Tracking with Correlation Learning","Q. Wang; Y. Zheng; P. Pan; Y. Xu","Machine Intelligence Technology Lab, Alibaba Group; Machine Intelligence Technology Lab, Alibaba Group; Machine Intelligence Technology Lab, Alibaba Group; Machine Intelligence Technology Lab, Alibaba Group","2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)","2 Nov 2021","2021","","","3875","3885","Recent works have shown that convolutional networks have substantially improved the performance of multiple object tracking by simultaneously learning detection and appearance features. However, due to the local perception of the convolutional network structure itself, the long-range dependencies in both the spatial and temporal cannot be obtained efficiently. To incorporate the spatial layout, we propose to exploit the local correlation module to model the topological relationship between targets and their surrounding environment, which can enhance the discriminative power of our model in crowded scenes. Specifically, we establish dense correspondences of each spatial location and its context, and explicitly constrain the correlation volumes through self-supervised learning. To exploit the temporal context, existing approaches generally utilize two or more adjacent frames to construct an enhanced feature representation, but the dynamic motion scene is inherently difficult to depict via CNNs. Instead, our paper proposes a learnable correlation operator to establish frame-to-frame matches over convolutional feature maps in the different layers to align and propagate temporal context. With extensive experimental results on the MOT datasets, our approach demonstrates the effectiveness of correlation learning with the superior performance and obtains state-of-the-art MOTA of 76.5% and IDF1 of 73.6% on MOT17.","2575-7075","978-1-6654-4509-2","10.1109/CVPR46437.2021.00387","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9578419","","Computer vision;Correlation;Target tracking;Computational modeling;Layout;Dynamics;Feature extraction","convolutional neural nets;feature extraction;image matching;image motion analysis;image representation;object detection;object tracking;supervised learning","object tracking;correlation learning;appearance features;local perception;convolutional network structure;long-range dependencies;spatial layout;local correlation module;topological relationship;crowded scenes;spatial location;correlation volumes;self-supervised learning;feature representation;dynamic motion scene;learnable correlation operator;frame-to-frame matches;convolutional feature maps;dense correspondences;temporal context;adjacent frames;CNN;MOT datasets","","61","","66","IEEE","2 Nov 2021","","","IEEE","IEEE Conferences"
"ChiTransformer: Towards Reliable Stereo from Cues","Q. Su; S. Ji",Georgia State University; Georgia State University,"2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)","27 Sep 2022","2022","","","1929","1939","Current stereo matching techniques are challenged by restricted searching space, occluded regions and sheer size. While single image depth estimation is spared from these challenges and can achieve satisfactory results with the extracted monocular cues, the lack of stereoscopic relationship renders the monocular prediction less reliable on its own especially in highly dynamic or cluttered environments. To address these issues in both scenarios, we present an optic-chiasm-inspired self-supervised binocular depth estimation method, wherein vision transformer (ViT) with a gated positional cross-attention (GPCA) layer is designed to enable feature-sensitive pattern retrieval between views, while retaining the extensive context information aggregated through self-attentions. Monocular cues from a single view are thereafter conditionally rectified by a blending layer with the retrieved pattern pairs. This crossover design is biologically analogous to the optic-chasma structure in human visual system and hence the name, Chi-Transformer. Our experiments show that this architecture yields substantial improvements over state-of-the-art self-supervised stereo approaches by 11%, and can be used on both rectilinear and non-rectilinear (e.g., fisheye) images.11https://github.com/ISL-CV/ChiTransformer.git","2575-7075","978-1-6654-6946-3","10.1109/CVPR52688.2022.00198","Army Research Laboratory(grant numbers:W911NF-22-2-0025); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9879564","Low-level vision; Deep learning architectures and techniques; Self-& semi-& meta- & unsupervised learning","Optical polarization;Biomedical optical imaging;Optical design;Stereo image processing;Estimation;Visual systems;Optical imaging","computer vision;image matching;stereo image processing","self-attentions;blending layer;retrieved pattern pairs;crossover design;optic-chasma structure;human visual system;Chi-Transformer;architecture yields substantial improvements;state-of-the-art self-supervised stereo approaches;towards reliable stereo;current stereo matching techniques;restricted searching space;occluded regions;sheer size;single image depth estimation;extracted monocular cues;stereoscopic relationship;monocular prediction;highly dynamic environments;cluttered environments;optic-chiasm-inspired self;binocular depth estimation method;vision transformer;ViT;gated positional cross-attention layer;GPCA;feature-sensitive pattern retrieval;extensive context information","","1","","76","IEEE","27 Sep 2022","","","IEEE","IEEE Conferences"
"Tracking Through Containers and Occluders in the Wild","B. Van Hoorick; P. Tokmakov; S. Stent; J. Li; C. Vondrick",Columbia University; Toyota Research Institute; Woven Planet; Toyota Research Institute; Columbia University,"2023 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)","22 Aug 2023","2023","","","13802","13812","Tracking objects with persistence in cluttered and dynamic environments remains a difficult challenge for computer vision systems. In this paper, we introduce TCOW, a new benchmark and model for visual tracking through heavy occlusion and containment. We set up a task where the goal is to, given a video sequence, segment both the projected extent of the target object, as well as the surrounding container or occluder whenever one exists. To study this task, we create a mixture of synthetic and annotated real datasets to support both supervised learning and structured evaluation of model performance under various forms of task variation, such as moving or nested containment. We evaluate two recent transformer-based video models and find that while they can be surprisingly capable of tracking targets under certain settings of task variation, there remains a considerable performance gap before we can claim a tracking model to have acquired a true notion of object permanence.","2575-7075","979-8-3503-0129-8","10.1109/CVPR52729.2023.01326","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10204496","Video: Low-level analysis;motion;tracking","Computer vision;Visualization;Target tracking;Computational modeling;Video sequences;Supervised learning;Containers","computer vision;image sequences;object detection;object tracking;supervised learning;target tracking;video signal processing","annotated real datasets;cluttered environments;computer vision systems;considerable performance gap;dynamic environments;heavy occlusion;moving nested containment;object permanence;projected extent;recent transformer-based video models;structured evaluation;supervised learning;surrounding container;synthetic datasets;target object;task variation;TCOW;tracking model;video sequence;visual tracking;wild","","","","72","IEEE","22 Aug 2023","","","IEEE","IEEE Conferences"
"Non-Probability Sampling Network for Stochastic Human Trajectory Prediction","I. Bae; J. -H. Park; H. -G. Jeon","AI Graduate School, GIST, South Korea; AI Graduate School, GIST, South Korea; AI Graduate School, GIST, South Korea","2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)","27 Sep 2022","2022","","","6467","6477","Capturing multimodal natures is essential for stochastic pedestrian trajectory prediction, to infer a finite set of future trajectories. The inferred trajectories are based on observation paths and the latent vectors of potential decisions of pedestrians in the inference step. However, stochastic approaches provide varying results for the same data and parameter settings, due to the random sampling of the latent vector. In this paper, we analyze the problem by reconstructing and comparing probabilistic distributions from prediction samples and socially-acceptable paths, respectively. Through this analysis, we observe that the inferences of all stochastic models are biased toward the random sampling, and fail to generate a set of realistic paths from finite samples. The problem cannot be resolved unless an infinite number of samples is available, which is infeasible in practice. We introduce that the Quasi-Monte Carlo (QMC) method, ensuring uniform coverage on the sampling space, as an alternative to the conventional random sampling. With the same finite number of samples, the QMC improves all the multimodal prediction results. We take an additional step ahead by incorporating a learnable sampling network into the existing networks for trajectory prediction. For this purpose, we propose the Non-Probability Sampling Network (NPSN), a very small network (~5K parameters) that generates purposive sample sequences using the past paths of pedestrians and their social interactions. Extensive experiments confirm that NPSN can significantly improve both the prediction accuracy (up to 60%) and reliability of the public pedestrian trajectory prediction benchmark. Code is publicly available at https://github.com/inhwanbae/NPSN.","2575-7075","978-1-6654-6946-3","10.1109/CVPR52688.2022.00637","National Research Foundation of Korea (NRF); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9879462","Motion and tracking; Explainable computer vision; Navigation and autonomous driving; Robot vision","Computer vision;Codes;Computational modeling;Computer network reliability;Stochastic processes;Benchmark testing;Probabilistic logic","Monte Carlo methods;probability;sampling methods;stochastic processes","stochastic human trajectory prediction;multimodal natures;stochastic pedestrian trajectory prediction;observation paths;latent vector;inference step;stochastic approaches;parameter settings;prediction samples;socially-acceptable paths;stochastic models;QuasiMonte Carlo method;sampling space;conventional random sampling;multimodal prediction results;learnable sampling network;sample sequences;prediction accuracy;public pedestrian trajectory prediction benchmark;nonprobability sampling network;social interactions;random sampling;probabilistic distributions","","6","","67","IEEE","27 Sep 2022","","","IEEE","IEEE Conferences"
"End-to-End Trajectory Distribution Prediction Based on Occupancy Grid Maps","K. Guo; W. Liu; J. Pan","The University of Hong Kong; College of Computer and Data Science, Fuzhou University; The University of Hong Kong","2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)","27 Sep 2022","2022","","","2232","2241","In this paper, we aim to forecast a future trajectory distribution of a moving agent in the real world, given the social scene images and historical trajectories. Yet, it is a challenging task because the ground-truth distribution is unknown and unobservable, while only one of its samples can be applied for supervising model learning, which is prone to bias. Most recent works focus on predicting diverse trajectories in order to cover all modes of the real distribution, but they may despise the precision and thus give too much credit to unrealistic predictions. To address the issue, we learn the distribution with symmetric cross-entropy using occupancy grid maps as an explicit and scene-compliant approximation to the ground-truth distribution, which can effectively penalize unlikely predictions. In specific, we present an inverse reinforcement learning based multi-modal trajectory distribution forecasting framework that learns to plan by an approximate value iteration network in an end-to-end manner. Besides, based on the predicted distribution, we generate a small set of representative trajectories through a differentiable Transformer-based network, whose attention mechanism helps to model the relations of trajectories. In experiments, our method achieves state-of-the-art performance on the Stanford Drone Dataset and Intersection Drone Dataset.","2575-7075","978-1-6654-6946-3","10.1109/CVPR52688.2022.00228","National Natural Science Foundation of China(grant numbers:62072110); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9879534","Behavior analysis; Navigation and autonomous driving; Robot vision","Computer vision;Reinforcement learning;Predictive models;Transformers;Trajectory;Pattern recognition;Task analysis","entropy;iterative methods;learning (artificial intelligence)","unrealistic predictions;explicit scene-compliant approximation;ground-truth distribution;unlikely predictions;based multimodal trajectory distribution forecasting framework;approximate value iteration network;predicted distribution;representative trajectories;differentiable Transformer-based network;end-to-end trajectory distribution prediction;occupancy grid;future trajectory distribution;moving agent;social scene images;historical trajectories;model learning;diverse trajectories","","3","","58","IEEE","27 Sep 2022","","","IEEE","IEEE Conferences"
"Rethinking Efficient Lane Detection via Curve Modeling","Z. Feng; S. Guo; X. Tan; K. Xu; M. Wang; L. Ma",Shanghai Jiao Tong University; Shanghai Jiao Tong University; East China Normal University; City University of Hong Kong; SenseTime Research; Shanghai Jiao Tong University,"2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)","27 Sep 2022","2022","","","17041","17049","This paper presents a novel parametric curve-based method for lane detection in RGB images. Unlike state-of-the-art segmentation-based and point detection-based methods that typically require heuristics to either decode predictions or formulate a large sum of anchors, the curve-based methods can learn holistic lane representations naturally. To handle the optimization difficulties of existing poly-nomial curve methods, we propose to exploit the parametric Bézier curve due to its ease of computation, stability, and high freedom degrees of transformations. In addition, we propose the deformable convolution-based feature flip fusion, for exploiting the symmetry properties of lanes in driving scenes. The proposed method achieves a new state-of-the-art performance on the popular LLAMAS benchmark. It also achieves favorable accuracy on the TuSimple and CULane datasets, while retaining both low latency (>150 FPS) and small model size (<10M). Our method can serve as a new baseline, to shed the light on the parametric curves modeling for lane detection. Codes of our model and PytorchAutoDrive: a unified framework for self-driving perception, are available at: https://github.com/voldemortX/pytorch-auto-drive.","2575-7075","978-1-6654-6946-3","10.1109/CVPR52688.2022.01655","National Key Research and Development Program of China(grant numbers:2019YFC1521104); National Natural Science Foundation of China(grant numbers:72192821,61972157); National Social Science Fund(grant numbers:I8ZD22); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9879543","Navigation and autonomous driving; Scene analysis and understanding","Deformable models;Convolutional codes;Image segmentation;Lane detection;Detectors;Benchmark testing;Stability analysis","computer vision;curve fitting;feature extraction;image colour analysis;image representation;image segmentation;learning (artificial intelligence);object detection;traffic engineering computing","rethinking efficient lane detection;curve modeling;parametric curve-based method;RGB images;state-of-the-art segmentation-based;point detection-based methods;decode predictions;curve-based methods;holistic lane representations;optimization difficulties;existing poly-nomial curve methods;parametric Bézier curve;deformable convolution-based feature flip fusion;small model size;parametric curves","","20","","32","IEEE","27 Sep 2022","","","IEEE","IEEE Conferences"
"Consistent Direct Time-of-Flight Video Depth Super-Resolution","Z. Sun; W. Ye; J. Xiong; G. Choe; J. Wang; S. Su; R. Ranjan",Stanford University; Meta Reality Labs; Meta Reality Labs; Meta Reality Labs; Meta Research; Meta Reality Labs; Meta Reality Labs,"2023 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)","22 Aug 2023","2023","","","5075","5085","Direct time-of-flight (dToF) sensors are promising for next-generation on-device 3D sensing. However, limited by manufacturing capabilities in a compact module, the dToF data has a low spatial resolution (e.g. $\sim 20\times 30$ for iPhone dToF), and it requires a super-resolution step before being passed to downstream tasks. In this paper, we solve this super-resolution problem by fusing the low-resolution dToF data with the corresponding high-resolution RGB guidance. Unlike the conventional RGB-guided depth enhancement approaches, which perform the fusion in a per-frame manner, we propose the first multi-frame fusion scheme to mitigate the spatial ambiguity resulting from the low-resolution dToF imaging. In addition, dToF sensors provide unique depth histogram information for each local patch, and we incorporate this dToF-specific feature in our network design to further alleviate spatial ambiguity. To evaluate our models on complex dynamic indoor environments and to provide a large-scale dToF sensor dataset, we introduce Dy-DToF, the first synthetic RGB-dToF video dataset that features dynamic objects and a realistic dToF simulator following the physical imaging process. We believe the methods and dataset are beneficial to a broad community as dToF depth sensing is becoming mainstream on mobile devices. Our code and data are publicly available. https://github.com/facebookresearch/DVSR/","2575-7075","979-8-3503-0129-8","10.1109/CVPR52729.2023.00491","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10205312","3D from multi-view and sensors","Three-dimensional displays;Superresolution;Sensor phenomena and characterization;Mobile handsets;Sensors;Pattern recognition;Manufacturing","image colour analysis;image enhancement;image fusion;image reconstruction;image resolution;image sensors;video signal processing","complex dynamic indoor environments;consistent direct time-of-flight video depth super-resolution;conventional RGB-guided depth enhancement approaches;dToF depth sensing;dToF-specific feature;Dy-DToF;high-resolution RGB guidance;iPhone dToF;large-scale dToF sensor dataset;local patch;low spatial resolution;low-resolution dToF data;low-resolution dToF imaging;multiframe fusion scheme;network design;on-device 3D sensing;physical imaging process;realistic dToF simulator;spatial ambiguity;super-resolution problem;super-resolution step;synthetic RGB-dToF video dataset;time-of-flight sensors;unique depth histogram information","","","","58","IEEE","22 Aug 2023","","","IEEE","IEEE Conferences"
"DriveGAN: Towards a Controllable High-Quality Neural Simulation","S. W. Kim; J. Philion; A. Torralba; S. Fidler",NVIDIA; NVIDIA; MIT; NVIDIA,"2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)","2 Nov 2021","2021","","","5816","5825","Realistic simulators are critical for training and verifying robotics systems. While most of the contemporary simulators are hand-crafted, a scaleable way to build simulators is to use machine learning to learn how the environment behaves in response to an action, directly from data. In this work, we aim to learn to simulate a dynamic environment directly in pixel-space, by watching unannotated sequences of frames and their associated actions. We introduce a novel high-quality neural simulator referred to as DriveGAN that achieves controllability by disentangling different components without supervision. In addition to steering controls, it also includes controls for sampling features of a scene, such as the weather as well as the location of non-player objects. Since DriveGAN is a fully differentiable simulator, it further allows for re-simulation of a given video sequence, offering an agent to drive through a recorded scene again, possibly taking different actions. We train DriveGAN on multiple datasets, including 160 hours of real-world driving data. We showcase that our approach greatly surpasses the performance of previous data-driven simulators, and al-lows for new key features not explored before.","2575-7075","978-1-6654-4509-2","10.1109/CVPR46437.2021.00576","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9577589","","Training;Computer vision;Computational modeling;Video sequences;Machine learning;Generative adversarial networks;Controllability","computer games;controllability;image sequences;learning (artificial intelligence);neural nets;robot programming;video signal processing","machine learning;dynamic environment;pixel-space;DriveGAN;controllability;steering controls;video sequence;real-world driving data;controllable high-quality neural simulation;hand-crafted simulation;data-driven simulators;robotics systems","","20","","58","IEEE","2 Nov 2021","","","IEEE","IEEE Conferences"
"FLAG3D: A 3D Fitness Activity Dataset with Language Instruction","Y. Tang; J. Liu; A. Liu; B. Yang; W. Dai; Y. Rao; J. Lu; J. Zhou; X. Li","Shenzhen International Graduate School, Tsinghua University; Shenzhen International Graduate School, Tsinghua University; Shenzhen International Graduate School, Tsinghua University; Shenzhen International Graduate School, Tsinghua University; Shenzhen International Graduate School, Tsinghua University; Department of Automation, Tsinghua University; Department of Automation, Tsinghua University; Department of Automation, Tsinghua University; Shenzhen International Graduate School, Tsinghua University","2023 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)","22 Aug 2023","2023","","","22106","22117","With the continuously thriving popularity around the world, fitness activity analytic has become an emerging research topic in computer vision. While a variety of new tasks and algorithms have been proposed recently, there are growing hunger for data resources involved in high-quality data, fine-grained labels, and diverse environments. In this paper, we present FLAG3D, a large-scale 3D fitness activity dataset with language instruction containing 180K sequences of 60 categories. FLAG3D features the following three aspects: 1) accurate and dense 3D human pose captured from advanced MoCap system to handle the complex activity and large movement, 2) detailed and professional language instruction to describe how to perform a specific activity, 3) versatile video resources from a high-tech MoCap system, rendering software, and cost-effective smartphones in natural environments. Extensive experiments and in-depth analysis show that FLAG3D contributes great research value for various challenges, such as cross-domain human action recognition, dynamic human mesh recovery, and language-guided human action generation. Our dataset and source code are publicly available at https://andytang15.github.io/FLAG3D.","2575-7075","979-8-3503-0129-8","10.1109/CVPR52729.2023.02117","National Natural Science Foundation of China(grant numbers:62206153,62125603); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10203838","Video: Action and event understanding","Computer vision;Three-dimensional displays;Source coding;Rendering (computer graphics);Software;Skeleton;Pattern recognition","computer vision;image capture;image motion analysis;image sequences;natural language processing;pose estimation;rendering (computer graphics);smart phones;solid modelling;video signal processing","3D human pose;computer vision;cost-effective smart phones;cross-domain human action recognition;data resources;dynamic human mesh recovery;fine-grained labels;fitness activity analytic;FLAG3D;high-quality data;high-tech MoCap system;language-guided human action generation;large-scale 3D fitness activity dataset;natural environments;professional language instruction;rendering software;source code;versatile video resources","","","","115","IEEE","22 Aug 2023","","","IEEE","IEEE Conferences"
"CRAVES: Controlling Robotic Arm With a Vision-Based Economic System","Y. Zuo; W. Qiu; L. Xie; F. Zhong; Y. Wang; A. L. Yuille",Tsinghua University; Johns Hopkins University; Johns Hopkins University; Peking University; Peking University; Johns Hopkins University,"2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)","9 Jan 2020","2019","","","4209","4218","Training a robotic arm to accomplish real-world tasks has been attracting increasing attention in both academia and industry. This work discusses the role of computer vision algorithms in this field. We focus on low-cost arms on which no sensors are equipped and thus all decisions are made upon visual recognition, e.g., real-time 3D pose estimation. This requires annotating a lot of training data, which is not only time-consuming but also laborious. In this paper, we present an alternative solution, which uses a 3D model to create a large number of synthetic data, trains a vision model in this virtual domain, and applies it to real-world images after domain adaptation. To this end, we design a semi-supervised approach, which fully leverages the geometric constraints among keypoints. We apply an iterative algorithm for optimization. Without any annotations on real images, our algorithm generalizes well and produces satisfying results on 3D pose estimation, which is evaluated on two real-world datasets. We also construct a vision-based control system for task accomplishment, for which we train a reinforcement learning agent in a virtual environment and apply it to the real-world. Moreover, our approach, with merely a 3D model being required, has the potential to generalize to other types of multi-rigid-body dynamic systems.","2575-7075","978-1-7281-3293-8","10.1109/CVPR.2019.00434","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8953695","Vision Applications and Systems;3D from Single Image; Robotics + Driving","","computer vision;learning (artificial intelligence);pose estimation;robot vision;solid modelling","computer vision algorithms;real-time 3D pose estimation;training data;synthetic data;vision model;virtual domain;real-world images;domain adaptation;semisupervised approach;geometric constraints;iterative algorithm;vision-based control system;virtual environment;multirigid-body dynamic systems;CRAVES;robotic arm control;vision-based economic system","","19","","47","IEEE","9 Jan 2020","","","IEEE","IEEE Conferences"
"4D Panoptic LiDAR Segmentation","M. Aygün; A. Ošep; M. Weber; M. Maximov; C. Stachniss; J. Behley; L. Leal-Taixé","Technical University of Munich, Germany; Technical University of Munich, Germany; Technical University of Munich, Germany; Technical University of Munich, Germany; University of Bonn, Germany; University of Bonn, Germany; Technical University of Munich, Germany","2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)","2 Nov 2021","2021","","","5523","5533","Temporal semantic scene understanding is critical for self-driving cars or robots operating in dynamic environments. In this paper, we propose 4D panoptic LiDAR segmentation to assign a semantic class and a temporally-consistent instance ID to a sequence of 3D points. To this end, we present an approach and a point-centric evaluation metric. Our approach determines a semantic class for every point while modeling object instances as probability distributions in the 4D spatio-temporal domain. We process multiple point clouds in parallel and resolve point-to-instance associations, effectively alleviating the need for explicit temporal data association. Inspired by recent advances in benchmarking of multi-object tracking, we propose to adopt a new evaluation metric that separates the semantic and point-to-instance association aspects of the task. With this work, we aim at paving the road for future developments of temporal LiDAR panoptic perception.","2575-7075","978-1-6654-4509-2","10.1109/CVPR46437.2021.00548","Ministry of Education; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9578492","","Measurement;Laser radar;Three-dimensional displays;Roads;Computational modeling;Semantics;Benchmark testing","graph theory;image motion analysis;image segmentation;image sequences;object detection;object tracking;optical radar;probability;video signal processing","4D panoptic LiDAR segmentation;temporal semantic scene understanding;self-driving cars;dynamic environments;semantic class;temporally-consistent instance ID;point-centric evaluation metric;object instances;4D spatio-temporal domain;multiple point clouds;point-to-instance associations;explicit temporal data association;semantic point-to-instance association aspects;temporal LiDAR panoptic perception","","18","","86","IEEE","2 Nov 2021","","","IEEE","IEEE Conferences"
"HCRF-Flow: Scene Flow from Point Clouds with Continuous High-order CRFs and Position-aware Flow Embedding","R. Li; G. Lin; T. He; F. Liu; C. Shen","S-Lab, Nanyang Technological University, Singapore; S-Lab, Nanyang Technological University, Singapore; University of Adelaide, Australia; Institute for Infocomm Research, A*STAR, Singapore; University of Adelaide, Australia","2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)","2 Nov 2021","2021","","","364","373","Scene flow in 3D point clouds plays an important role in understanding dynamic environments. Although significant advances have been made by deep neural networks, the performance is far from satisfactory as only per-point translational motion is considered, neglecting the constraints of the rigid motion in local regions. To address the issue, we propose to introduce the motion consistency to force the smoothness among neighboring points. In addition, constraints on the rigidity of the local transformation are also added by sharing unique rigid motion parameters for all points within each local region. To this end, a high-order CRFs based relation module (Con-HCRFs) is deployed to explore both point-wise smoothness and region-wise rigidity. To empower the CRFs to have a discriminative unary term, we also introduce a position-aware flow estimation module to be incorporated into the Con-HCRFs. Comprehensive experiments on FlyingThings3D and KITTI show that our proposed framework (HCRF-Flow) achieves state-of-the-art performance and significantly outperforms previous approaches substantially.","2575-7075","978-1-6654-4509-2","10.1109/CVPR46437.2021.00043","National Research Foundation; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9578049","","Deep learning;Computer vision;Three-dimensional displays;Costs;Force;Dynamics;Estimation","deep learning (artificial intelligence);image sequences;motion estimation;random processes;smoothing methods;stereo image processing","dynamic environments;deep neural networks;per-point translational motion;local region;motion consistency;neighboring points;local transformation;unique rigid motion parameters;continuous high-order CRFs;point-wise smoothness;region-wise rigidity;position-aware flow estimation module;HCRF-Flow;scene flow;3D point clouds;position-aware flow embedding;FlyingThings3D;KITTI;high-order CRFs based relation module","","15","","49","IEEE","2 Nov 2021","","","IEEE","IEEE Conferences"
"Image De-raining via Continual Learning","M. Zhou; J. Xiao; Y. Chang; X. Fu; A. Liu; J. Pan; Z. -J. Zha","University of Science and Technology of China, China; University of Science and Technology of China, China; University of Science and Technology of China, China; University of Science and Technology of China, China; University of Science and Technology of China, China; Nanjing University of Science and Technology, China; University of Science and Technology of China, China","2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)","2 Nov 2021","2021","","","4905","4914","While deep convolutional neural networks (CNNs) have achieved great success on image de-raining task, most existing methods can only learn fixed mapping rules between paired rainy/clean images on a single dataset. This limits their applications in practical situations with multiple and incremental datasets where the mapping rules may change for different types of rain streaks. However, the catastrophic forgetting of traditional deep CNN model challenges the design of generalized framework for multiple and incremental datasets. A strategy of sharing the network structure but in-dependently updating and storing the network parameters on each dataset has been developed as a potential solution. Nevertheless, this strategy is not applicable to compact systems as it dramatically increases the overall training time and parameter space. To alleviate such limitation, in this study, we propose a parameter importance guided weights modification approach, named PIGWM. Specifically, with new dataset (e.g. new rain dataset), the well-trained network weights are updated according to their importance evaluated on previous training dataset. With extensive experimental validation, we demonstrate that a single network with a single parameter set of our proposed method can process multiple rain datasets almost without performance degradation. The proposed model is capable of achieving superior performance on both inhomogeneous and incremental datasets, and is promising for highly compact systems to gradually learn myriad regularities of the different types of rain streaks. The results indicate that our proposed method has great potential for other computer vision tasks with dynamic learning environments.","2575-7075","978-1-6654-4509-2","10.1109/CVPR46437.2021.00487","National Natural Science Foundation of China; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9577362","","Training;Degradation;Computer vision;Rain;Computational modeling;Benchmark testing;Nonhomogeneous media","computer vision;deep learning (artificial intelligence)","computer vision;deep CNN model;dynamic learning environments;inhomogeneous datasets;single parameter set;network weights;rain dataset;parameter importance guided weights modification approach;training time;network parameters;network structure;rain streaks;incremental datasets;multiple datasets;single dataset;fixed mapping rules;image de-raining task;deep convolutional neural networks;continual learning","","12","","50","IEEE","2 Nov 2021","","","IEEE","IEEE Conferences"
"Lifelong Unsupervised Domain Adaptive Person Re-identification with Coordinated Anti-forgetting and Adaptation","Z. Huang; Z. Zhang; C. Lan; W. Zeng; P. Chu; Q. You; J. Wang; Z. Liu; Z. -J. Zha",University of Science and Technology of China; Microsoft; Microsoft; EIT Institute for Advanced Study; Microsoft; Microsoft; Microsoft; Microsoft; University of Science and Technology of China,"2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)","27 Sep 2022","2022","","","14268","14277","Unsupervised domain adaptive person re-identification (ReID) has been extensively investigated to mitigate the adverse effects of domain gaps. Those works assume the target domain data can be accessible all at once. However, for the real-world streaming data, this hinders the timely adaptation to changing data statistics and sufficient exploitation of increasing samples. In this paper, to address more practical scenarios, we propose a new task, Lifelong Un-supervised Domain Adaptive (LUDA) person ReID. This is challenging because it requires the model to continuously adapt to unlabeled data in the target environments while alleviating catastrophic forgetting for such a fine-grained person retrieval task. We design an effective scheme for this task, dubbed CLUDA-ReID, where the anti-forgetting is harmoniously coordinated with the adaptation. Specifically, a meta-based Coordinated Data Replay strategy is proposed to replay old data and update the network with a coordinated optimization direction for both adaptation and memorization. Moreover, we propose Relational Consistency Learning for old knowledge distillation/inheritance in line with the objective of retrieval-based tasks. We set up two evaluation settings to simulate the practical application scenarios. Extensive experiments demonstrate the effectiveness of our CLUDA-ReID for both scenarios with stationary target streams and scenarios with dynamic target streams.","2575-7075","978-1-6654-6946-3","10.1109/CVPR52688.2022.01389","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9878424","Recognition: detection;categorization;retrieval; Self-& semi-& meta- Vision applications and systems","Adaptation models;Data privacy;Computer vision;Target recognition;Machine vision;Data models;Pattern recognition","data analysis;information retrieval;unsupervised learning","dubbed CLUDA-ReID;coordinated optimization direction;retrieval-based tasks;stationary target streams;dynamic target streams;target domain data;real-world streaming data;data statistics;unlabeled data;target environments;catastrophic forgetting;fine-grained person retrieval task;unsupervised domain adaptive person ReID;lifelong unsupervised domain adaptive person re-identification;meta-based coordinated data replay strategy;coordinated anti-forgetting","","2","","51","IEEE","27 Sep 2022","","","IEEE","IEEE Conferences"
"WildLight: In-the-wild Inverse Rendering with a Flashlight","Z. Cheng; J. Li; H. Li",Australian National University; Australian National University; Australian National University,"2023 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)","22 Aug 2023","2023","","","4305","4314","This paper proposes a practical photometric solution for the challenging problem of in-the-wild inverse rendering under unknown ambient lighting. Our system recovers scene geometry and reflectance using only multi-view images captured by a smartphone. The key idea is to exploit smartphone's built-in flashlight as a minimally controlled light source, and decompose image intensities into two photometric components – a static appearance corresponds to ambient flux, plus a dynamic reflection induced by the moving flashlight. Our method does not require flash/non-flash images to be captured in pairs. Building on the success of neural light fields, we use an off-the-shelf method to capture the ambient reflections, while the flashlight component enables physically accurate photometric constraints to decouple reflectance and illumination. Compared to existing inverse rendering methods, our setup is applicable to non-darkroom environments yet sidesteps the inherent difficulties of explicit solving ambient reflections. We demonstrate by extensive experiments that our method is easy to implement, casual to set up, and consistently outperforms existing in-the-wild inverse rendering techniques. Finally, our neural reconstruction can be easily exported to PBR textured triangle mesh ready for industrial renderers. Our source code and data are released to https://github.com/za-cheng/WildLight.","2575-7075","979-8-3503-0129-8","10.1109/CVPR52729.2023.00419","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10203448","3D from multi-view and sensors","Reflectivity;Geometry;Three-dimensional displays;Source coding;Lighting;Rendering (computer graphics);Reflection","image capture;rendering (computer graphics);smart phones","ambient flux;decouple reflectance;dynamic reflection;existing inverse rendering methods;explicit solving ambient reflections;flashlight component;illumination;image intensities;in-the-wild inverse rendering techniques;industrial renderers;minimally controlled light source;moving flashlight;multiview images;neural light fields;nondarkroom environments;off-the-shelf method;photometric components;physically accurate photometric constraints;practical photometric solution;smartphone;static appearance corresponds;system recovers scene geometry;unknown ambient lighting","","","","44","IEEE","22 Aug 2023","","","IEEE","IEEE Conferences"
"SR-LSTM: State Refinement for LSTM Towards Pedestrian Trajectory Prediction","P. Zhang; W. Ouyang; P. Zhang; J. Xue; N. Zheng","Institute of Artificial Intelligence and Robotics, Xian Jiaotong University, China; SenseTime Computer Vision Research Group, The University of Sydney, Australia; Institute of Artificial Intelligence and Robotics, Xian Jiaotong University, China; Institute of Artificial Intelligence and Robotics, Xian Jiaotong University, China; Institute of Artificial Intelligence and Robotics, Xian Jiaotong University, China","2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)","9 Jan 2020","2019","","","12077","12086","In crowd scenarios, reliable trajectory prediction of pedestrians requires insightful understanding of their social behaviors. These behaviors have been well investigated by plenty of studies, while it is hard to be fully expressed by hand-craft rules. Recent studies based on LSTM networks have shown great ability to learn social behaviors. However, many of these methods rely on previous neighboring hidden states but ignore the important current intention of the neighbors. In order to address this issue, we propose a data-driven state refinement module for LSTM network (SR-LSTM), which activates the utilization of the current intention of neighbors, and jointly and iteratively refines the current states of all participants in the crowd through a message passing mechanism. To effectively extract the social effect of neighbors, we further introduce a social-aware information selection mechanism consisting of an element-wise motion gate and a pedestrian-wise attention to select useful message from neighboring pedestrians. Experimental results on two public datasets, i.e. ETH and UCY, demonstrate the effectiveness of our proposed SR-LSTM and we achieve state-of-the-art results.","2575-7075","978-1-7281-3293-8","10.1109/CVPR.2019.01236","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8954402","Motion and Tracking;Deep Learning","","image motion analysis;learning (artificial intelligence);message passing;pedestrians","message passing mechanism;social effect;social-aware information selection mechanism;pedestrian-wise attention;neighboring pedestrians;SR-LSTM;pedestrian trajectory prediction;crowd scenarios;reliable trajectory prediction;insightful understanding;social behaviors;hand-craft rules;LSTM network;previous neighboring hidden states;important current intention;data-driven state refinement module;current states","","256","","55","IEEE","9 Jan 2020","","","IEEE","IEEE Conferences"
"Contextual Instance Decoupling for Robust Multi-Person Pose Estimation","D. Wang; S. Zhang","School of Computer Science, Peking University; School of Computer Science, Peking University","2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)","27 Sep 2022","2022","","","11050","11058","Crowded scenes make it challenging to differentiate persons and locate their pose keypoints. This paper proposes the Contextual Instance Decoupling (CID), which presents a new pipeline for multi-person pose estimation. Instead of relying on person bounding boxes to spatially differentiate persons, CID decouples persons in an image into multiple instance-aware feature maps. Each of those feature maps is hence adopted to infer keypoints for a specific person. Compared with bounding box detection, CID is differentiable and robust to detection errors. Decoupling persons into different feature maps allows to isolate distractions from other persons, and explore context cues at scales larger than the bounding box size. Experiments show that CID outperforms previous multi-person pose estimation pipelines on crowded scenes pose estimation benchmarks in both accuracy and efficiency. For instance, it achieves 71.3% AP on CrowdPose, outperforming the recent single-stage DEKR by 5.6%, the bottom-up CenterAttention by 3.7%, and the top-down JC-SPPE by 5.3%. This advantage sustains on the commonly used COCO benchmark††Code is available at https://github.com/kennethwdk/CID.","2575-7075","978-1-6654-6946-3","10.1109/CVPR52688.2022.01078","National Key Research and Development Program of China(grant numbers:2018YFE0118400); Natural Science Foundation of China(grant numbers:U20B2052,61936011); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9879833","Pose estimation and tracking","Computer vision;Pose estimation;Pipelines;Benchmark testing;Encoding;Pattern recognition","feature extraction;image classification;object detection;pose estimation","Contextual Instance Decoupling;robust multiperson pose estimation;crowded scenes;pose keypoints;spatially differentiate persons;CID decouples persons;multiple instance-aware feature maps;box detection;different feature maps;bounding box size;estimation benchmarks","","7","","35","IEEE","27 Sep 2022","","","IEEE","IEEE Conferences"
"MotionTrack: Learning Robust Short-Term and Long-Term Motions for Multi-Object Tracking","Z. Qin; S. Zhou; L. Wang; J. Duan; G. Hua; W. Tang","National Key Laboratory of Human-Machine Hybrid Augmented Intelligence, National Engineering Research Center for Visual Information and Applications, Institute of Artificial Intelligence and Robotics, Xi'an Jiaotong University; National Key Laboratory of Human-Machine Hybrid Augmented Intelligence, National Engineering Research Center for Visual Information and Applications, Institute of Artificial Intelligence and Robotics, Xi'an Jiaotong University; National Key Laboratory of Human-Machine Hybrid Augmented Intelligence, National Engineering Research Center for Visual Information and Applications, Institute of Artificial Intelligence and Robotics, Xi'an Jiaotong University; School of Software Engineering, Xi'an Jiaotong University; Wormpex AI Research; University of Illinois at Chicago","2023 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)","22 Aug 2023","2023","","","17939","17948","The main challenge of Multi-Object Tracking (MOT) lies in maintaining a continuous trajectory for each target. Existing methods often learn reliable motion patterns to match the same target between adjacent frames and discriminative appearance features to re-identify the lost targets after a long period. However, the reliability of motion prediction and the discriminability of appearances can be easily hurt by dense crowds and extreme occlusions in the tracking process. In this paper, we propose a simple yet effective multi-object tracker, i.e., MotionTrack, which learns robust short-term and long-term motions in a unified framework to associate trajectories from a short to long range. For dense crowds, we design a novel Interaction Module to learn interaction-aware motions from short-term trajectories, which can estimate the complex movement of each target. For extreme occlusions, we build a novel Refind Module to learn reliable long-term motions from the target's history trajectory, which can link the interrupted trajectory with its corresponding detection. Our Interaction Module and Refind Module are embedded in the well-known tracking-by-detection paradigm, which can work in tandem to maintain superior performance. Extensive experimental results on MOT17 and MOT20 datasets demonstrate the superiority of our approach in challenging scenarios, and it achieves state-of-the-art performances at various MOT metrics. Code is available at https://github.com/qwomeng/MotionTrack.","2575-7075","979-8-3503-0129-8","10.1109/CVPR52729.2023.01720","National Key R&D Program of China(grant numbers:2021YFB1714700); NSFC(grant numbers:62088102,62106192); Natural Science Foundation of Shaanxi Province(grant numbers:2022JC-41); China Postdoctoral Science Foundation(grant numbers:2022T150518); Fundamental Research Funds for the Central Universities(grant numbers:XTR042021005,XTR072022001); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10204896","Video: Low-level analysis;motion;and tracking","Measurement;Computer vision;Target tracking;Image color analysis;Error compensation;Trajectory;Reliability","feature extraction;learning (artificial intelligence);object detection;object tracking;target tracking;traffic engineering computing;video signal processing","continuous trajectory;dense crowds;discriminative appearance features;extreme occlusions;interaction-aware motions;long-term motions;lost targets;motion prediction;MultiObject Tracking;reliable motion patterns;robust short-term;short-term trajectories;simple yet effective multiobject tracker;tracking process;tracking-by-detection paradigm","","1","","59","IEEE","22 Aug 2023","","","IEEE","IEEE Conferences"
"HSC4D: Human-centered 4D Scene Capture in Large-scale Indoor-outdoor Space Using Wearable IMUs and LiDAR","Y. Dai; Y. Lin; C. Wen; S. Shen; L. Xu; J. Yu; Y. Ma; C. Wang","Xiamen University, China; Xiamen University, China; Xiamen University, China; Xiamen University, China; Xiamen University, China; Xiamen University, China; Xiamen University, China; ShanghaiTech University, China","2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)","27 Sep 2022","2022","","","6782","6792","We propose Human-centered 4D Scene Capture (HSC4D) to accurately and efficiently create a dynamic digital world, containing large-scale indoor-outdoor scenes, diverse human motions, and rich interactions between humans and environments. Using only body-mounted IMUs and LiDAR, HSC4D is space-free without any external devices' constraints and map-free without pre-built maps. Considering that IMUs can capture human poses but always drift for long-period use, while LiDAR is stable for global localization but rough for local positions and orientations, HSC4D makes both sensors complement each other by a joint optimization and achieves promising results for long-term capture. Relationships between humans and environments are also explored to make their interaction more realistic. To facilitate many down-stream tasks, like AR, VR, robots, autonomous driving, etc., we propose a dataset containing three large scenes (1k-5k m2) with accurate dynamic human motions and locations. Diverse scenarios (climbing gym, multi-story building, slope, etc.) and challenging human activities (exercising, walking up/down stairs, climbing, etc.) demonstrate the effectiveness and the generalization ability of HSC4D. The dataset and code is available at lidarhumanmotion.net/hsc4d.","2575-7075","978-1-6654-6946-3","10.1109/CVPR52688.2022.00667","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9880196","Pose estimation and tracking; Datasets and evaluation; Face and gestures; Motion and tracking","Location awareness;Laser radar;Dynamics;Stairs;Robot sensing systems;Motion capture;Sensors","gait analysis;image motion analysis;optical radar;pose estimation;wearable sensors","accurate dynamic human motions;large-scale indoor-outdoor space;LiDAR;large-scale indoor-outdoor scenes;diverse human motions;human poses;long-term capture;HSC4D;human-centered 4D scene capture;wearable IMU;climbing gym;multistory building;body-mounted IMU","","5","","55","IEEE","27 Sep 2022","","","IEEE","IEEE Conferences"
"SoPhie: An Attentive GAN for Predicting Paths Compliant to Social and Physical Constraints","A. Sadeghian; V. Kosaraju; A. Sadeghian; N. Hirose; H. Rezatofighi; S. Savarese","Stanford University; Stanford University; University of Florida; Stanford University; Stanford University, Stanford, CA, USA; Stanford University","2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)","9 Jan 2020","2019","","","1349","1358","This paper addresses the problem of path prediction for multiple interacting agents in a scene, which is a crucial step for many autonomous platforms such as self-driving cars and social robots. We present SoPhie; an interpretable framework based on Generative Adversarial Network (GAN), which leverages two sources of information, the path history of all the agents in a scene, and the scene context information, using images of the scene. To predict a future path for an agent, both physical and social information must be leveraged. Previous work has not been successful to jointly model physical and social interactions. Our approach blends a social attention mechanism with physical attention that helps the model to learn where to look in a large scene and extract the most salient parts of the image relevant to the path. Whereas, the social attention component aggregates information across the different agent interactions and extracts the most important trajectory information from the surrounding neighbors. SoPhie also takes advantage of GAN to generates more realistic samples and to capture the uncertain nature of the future paths by modeling its distribution. All these mechanisms enable our approach to predict socially and physically plausible paths for the agents and to achieve state-of-the-art performance on several different trajectory forecasting benchmarks.","2575-7075","978-1-7281-3293-8","10.1109/CVPR.2019.00144","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8953374","Motion and Tracking;Vision Applications and Systems","","mobile robots;multi-agent systems;neural nets;path planning;robot vision","physical constraints;social constraints;paths compliant;generative adversarial network;agent interactions;social attention component;social attention mechanism;social interactions;social information;physical information;scene context information;path history;social robots;self-driving cars;autonomous platforms;multiple interacting agents;path prediction;attentive GAN;plausible paths;future path;SoPhie;trajectory information","","384","","32","IEEE","9 Jan 2020","","","IEEE","IEEE Conferences"
"Towards Social Artificial Intelligence: Nonverbal Social Signal Prediction in a Triadic Interaction","H. Joo; T. Simon; M. Cikara; Y. Sheikh",Carnegie Mellon University; Carnegie Mellon University; Harvard University; Carnegie Mellon University,"2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)","9 Jan 2020","2019","","","10865","10875","We present a new research task and a dataset to understand human social interactions via computational methods, to ultimately endow machines with the ability to encode and decode a broad channel of social signals humans use. This research direction is essential to make a machine that genuinely communicates with humans, which we call Social Artificial Intelligence. We first formulate the ""social signal prediction'' problem as a way to model the dynamics of social signals exchanged among interacting individuals in a data-driven way. We then present a new 3D motion capture dataset to explore this problem, where the broad spectrum of social signals (3D body, face, and hand motions) are captured in a triadic social interaction scenario. Baseline approaches to predict speaking status, social formation, and body gestures of interacting individuals are presented in the defined social prediction framework.","2575-7075","978-1-7281-3293-8","10.1109/CVPR.2019.01113","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8954115","Face;Gesture;and Body Pose;Datasets and Evaluation","","gesture recognition;human computer interaction;human-robot interaction;image motion analysis;multi-agent systems;service robots;social aspects of automation","social artificial intelligence;nonverbal social signal prediction;triadic interaction;research task;human social interactions;social signal prediction problem;interacting individuals;3D motion capture dataset;triadic social interaction scenario;social formation;social prediction framework","","39","","72","IEEE","9 Jan 2020","","","IEEE","IEEE Conferences"
"M2I: From Factored Marginal Trajectory Prediction to Interactive Prediction","Q. Sun; X. Huang; J. Gu; B. C. Williams; H. Zhao","IIIS, Tsinghua University; CSAIL, Massachusetts Institute of Technology; IIIS, Tsinghua University; CSAIL, Massachusetts Institute of Technology; IIIS, Tsinghua University","2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)","27 Sep 2022","2022","","","6533","6542","Predicting future motions of road participants is an important task for driving autonomously in urban scenes. Existing models excel at predicting marginal trajectories for single agents, yet it remains an open question to jointly predict scene compliant trajectories over multiple agents. The challenge is due to exponentially increasing prediction space as a function of the number of agents. In this work, we exploit the underlying relations between interacting agents and decouple the joint prediction problem into marginal prediction problems. Our proposed approach M2I first classifies interacting agents as pairs of influencers and reactors, and then leverages a marginal prediction model and a conditional prediction model to predict trajectories for the influencers and reactors, respectively. The predictions from interacting agents are combined and selected according to their joint likelihoods. Experiments show that our simple but effective approach achieves state-of-the-art performance on the Waymo Open Motion Dataset interactive prediction benchmark.","2575-7075","978-1-6654-6946-3","10.1109/CVPR52688.2022.00643","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9879119","Motion and tracking; Action and event recognition; Navigation and autonomous driving","Computer vision;Tracking;Navigation;Roads;Predictive models;Benchmark testing;Trajectory","learning (artificial intelligence);multi-agent systems;natural scenes;neural nets;regression analysis","factored marginal trajectory prediction;urban scenes;marginal trajectories;scene compliant trajectories;multiple agents;prediction space;interacting agents;joint prediction problem;conditional prediction model;Waymo Open Motion Dataset interactive prediction benchmark;M2I","","19","","47","IEEE","27 Sep 2022","","","IEEE","IEEE Conferences"
"Habitat-Web: Learning Embodied Object-Search Strategies from Human Demonstrations at Scale","R. Ramrakhya; E. Undersander; D. Batra; A. Das",Georgia Institute of Technology; Meta AI Research; Georgia Institute of Technology; Meta AI Research,"2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)","27 Sep 2022","2022","","","5163","5173","We present a large-scale study of imitating human demonstrations on tasks that require a virtual robot to search for objects in new environments - (1) ObjectGoal Navigation (e.g. 'find & go to a chair’) and (2) Pick&place (e.g. 'find mug, pick mug, find counter, place mug on counter’). First, we develop a virtual teleoperation data-collection infrastructure - connecting Habitat simulator running in a web browser to Amazon Mechanical Turk, allowing remote users to teleoperate virtual robots, safely and at scale. We collect 80k demonstrations for OBJECTNAV and 12k demonstrations for PICK&PLACE, which is an order of magnitude larger than existing human demonstration datasets in simulation or on real robots. Our virtual teleoperation data contains 29.3M actions, and is equivalent to 22.6k hours of real-world teleoperation time, and illustrates rich, diverse strategies for solving the tasks. Second, we use this data to answer the question - how does large-scale imitation learning (IL) (which has not been hitherto possible) compare to reinforcement learning (RL) (which is the status quo)? On OBJECTNAV, we find that IL (with no bells or whistles) using 70k human demonstrations outperforms RL using 240k agent-gathered trajectories. This effectively establishes an ‘exchange rate’ - a single human demonstration appears to be worth ~4 agent-gathered ones. More importantly, we find the IL-trained agent learns efficient object-search behavior from humans - it peeks into rooms, checks corners for small objects, turns in place to get a panoramic view - none of these are exhibited as prominently by the RL agent, and to induce these behaviors via contemporary RL techniques would require tedious reward engineering. Finally, accuracy vs. training data size plots show promising scaling behavior, suggesting that simply collecting more demonstrations is likely to advance the state of art further. On PICK&PLACE, the comparison is starker - IL agents achieve ~18% success on episodes with new object-receptacle locations when trained with 9.5k human demonstrations, while RL agents fail to get beyond 0%. Overall, our work provides compelling evidence for investing in large-scale imitation learning.","2575-7075","978-1-6654-6946-3","10.1109/CVPR52688.2022.00511","NSF; ONR; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9880429","Vision + language","Computer vision;Navigation;Training data;Reinforcement learning;Search problems;Behavioral sciences;Trajectory","control engineering computing;learning (artificial intelligence);mobile robots;multi-agent systems;telerobotics;virtual reality","Habitat-web;embodied object-search strategies;large-scale study;virtual robot;chair;place mug;virtual teleoperation data-collection infrastructure;connecting Habitat simulator;PICK&PLACE;human demonstration datasets;real-world teleoperation time;rich strategies;diverse strategies;large-scale imitation learning;reinforcement learning;240k agent-gathered trajectories;single human demonstration;worth ~4 agent-gathered ones;IL-trained agent;efficient object-search behavior;RL agent;training data size plots;promising scaling behavior;IL agents;object-receptacle locations;9.5k human demonstrations","","14","","51","IEEE","27 Sep 2022","","","IEEE","IEEE Conferences"
"Transitional Adaptation of Pretrained Models for Visual Storytelling","Y. Yu; J. Chung; H. Yun; J. Kim; G. Kim",Allen Institute for AI; Seoul National University; Seoul National University; Violet; Seoul National University,"2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)","2 Nov 2021","2021","","","12653","12663","Previous models for vision-to-language generation tasks usually pretrain a visual encoder and a language generator in the respective domains and jointly finetune them with the target task. However, this direct transfer practice may suffer from the discord between visual specificity and language fluency since they are often separately trained from large corpora of visual and text data with no common ground. In this work, we claim that a transitional adaptation task is required between pretraining and finetuning to harmonize the visual encoder and the language model for challenging downstream target tasks like visual storytelling. We propose a novel approach named Transitional Adaptation of Pre-trained Model (TAPM) that adapts the multi-modal modules to each other with a simpler alignment task between visual inputs only with no need for text labels. Through extensive experiments, we show that the adaptation step significantly improves the performance of multiple language models for sequential video and image captioning tasks. We achieve new state-of-the-art performance on both language metrics and human evaluation in the multi-sentence description task of LSMDC 2019 [50] and the image storytelling task of VIST [18]. Our experiments reveal that this improvement in caption quality does not depend on the specific choice of language models.","2575-7075","978-1-6654-4509-2","10.1109/CVPR46437.2021.01247","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9577852","","Measurement;Visualization;Adaptation models;Computer vision;Computational modeling;Generators;Pattern recognition","image enhancement;learning (artificial intelligence);multi-agent systems;natural language processing;text analysis;video signal processing","direct transfer practice;visual specificity;language fluency;visual text data;transitional adaptation task;pretraining;visual encoder;language model;downstream target tasks;visual storytelling;multimodal modules;simpler alignment task;visual inputs;text labels;adaptation step;multiple language models;sequential video;image captioning tasks;language metrics;multisentence description task;image storytelling task;pretrained models;vision-to-language generation tasks;language generator;respective domains;target task","","10","","74","IEEE","2 Nov 2021","","","IEEE","IEEE Conferences"
"Is Mapping Necessary for Realistic PointGoal Navigation?","R. Partsey; E. Wijmans; N. Yokoyama; O. Dobosevych; D. Batra; O. Maksymets",Ukrainian Catholic University; Georgia Institute of Technology; Ukrainian Catholic University; Ukrainian Catholic University; Georgia Institute of Technology; Ukrainian Catholic University,"2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)","27 Sep 2022","2022","","","17211","17220","Can an autonomous agent navigate in a new environment without building an explicit map? For the task of PointGoal navigation ('Go to Δx, Δy’) under idealized settings (no RGB-D and actuation noise, perfect GPS+Compass), the answer is a clear ‘yes' - mapless neural models composed of task-agnostic components (CNNs and RNNs) trained with large-scale reinforcement learning achieve 100% Success on a standard dataset (Gibson [24] ). However, for PointNav in a realistic setting (RGB-D and actuation noise, no GPS+Compass), this is an open question; one we tackle in this paper. The strongest published result for this task is 71.7% Success [39]. 11According to Habitat Challenge 2020 PointNav benchmark held annually. A concurrent as-yet-unpublished result has reported 91% Success on 2021's benchmark, but we are unable to comment on the details because an associated report is not available.First, we identify the main (perhaps, only) cause of the drop in performance: absence of GPS+Compass. An agent with perfect GPS+Compass faced with RGB-D sensing and actuation noise achieves 99.8% Success (Gibson- v2 val). This suggests that (to paraphrase a meme) robust visual odometry is all we need for realistic PointNav; if we can achieve that, we can ignore the sensing and actuation noise. With that as our operating hypothesis, we scale dataset size, model size, and develop human-annotation-free dataaugmentation techniques to train neural models for visual odometry. We advance state of the art on the Habitat Realistic PointNav Challenge - SPL by 40% (relative), 53 to 74, and Success by 31% (relative), 71 to 94. While our approach does not saturate or ‘solve’ this dataset, this strong improvement combined with promising zero-shot sim2real transfer (to a LoCoBot robot) provides evidence consistent with the hypothesis that explicit mapping may not be necessary for navigation, even in a realistic setting.","2575-7075","978-1-6654-6946-3","10.1109/CVPR52688.2022.01672","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9879299","Navigation and autonomous driving; Robot vision","Recurrent neural networks;Navigation;Robot vision systems;Reinforcement learning;Benchmark testing;Sensors;Pattern recognition","control engineering computing;convolutional neural nets;Global Positioning System;image colour analysis;mobile robots;multi-agent systems;path planning;recurrent neural nets;reinforcement learning;robot vision","LoCoBot robot;zero-shot sim2real transfer;human-annotation-free data-augmentation;visual odometry;RNNs;CNNs;GPS+Compass;PointGoal navigation;Habitat Realistic PointNav Challenge;dataset size;large-scale reinforcement learning;task-agnostic components;neural models;actuation noise;RGB-D;autonomous agent","","6","","39","IEEE","27 Sep 2022","","","IEEE","IEEE Conferences"
"Pushing it out of the Way: Interactive Visual Navigation","K. -H. Zeng; L. Weihs; A. Farhadi; R. Mottaghi","Paul G. Allen School of Computer Science & Engineering, University of Washington; PRIOR @ Allen Institute for AI; Paul G. Allen School of Computer Science & Engineering, University of Washington; Paul G. Allen School of Computer Science & Engineering, University of Washington","2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)","2 Nov 2021","2021","","","9863","9872","We have observed significant progress in visual navigation for embodied agents. A common assumption in studying visual navigation is that the environments are static; this is a limiting assumption. Intelligent navigation may involve interacting with the environment beyond just moving forward/backward and turning left/right. Sometimes, the best way to navigate is to push something out of the way. In this paper, we study the problem of interactive navigation where agents learn to change the environment to navigate more efficiently to their goals. To this end, we introduce the Neural Interaction Engine (NIE) to explicitly predict the change in the environment caused by the agent’s actions. By modeling the changes while planning, we find that agents exhibit significant improvements in their navigational capabilities. More specifically, we consider two downstream tasks in the physics-enabled, visually rich, AI2-THOR environment: (1) reaching a target while the path to the target is blocked (2) moving an object to a target location by pushing it. For both tasks, agents equipped with an NIE significantly outperform agents without the understanding of the effect of the actions indicating the benefits of our approach. The code and dataset are available at github.com/KuoHaoZeng/Interactive_Visual_Navigation.","2575-7075","978-1-6654-4509-2","10.1109/CVPR46437.2021.00974","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9578659","","Visualization;Computer vision;Limiting;Navigation;Predictive models;Turning;Planning","interactive systems;learning (artificial intelligence);multi-agent systems;navigation;software agents","embodied agents;intelligent navigation;navigational capabilities;AI2-THOR environment;interactive visual navigation;neural interaction engine","","6","","48","IEEE","2 Nov 2021","","","IEEE","IEEE Conferences"
"Think Global, Act Local: Dual-scale Graph Transformer for Vision-and-Language Navigation","S. Chen; P. -L. Guhur; M. Tapaswi; C. Schmid; I. Laptev","Inria, École normale supérieure, CNRS, PSL Research University; Inria, École normale supérieure, CNRS, PSL Research University; IIIT Hyderabad; Inria, École normale supérieure, CNRS, PSL Research University; Inria, École normale supérieure, CNRS, PSL Research University","2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)","27 Sep 2022","2022","","","16516","16526","Following language instructions to navigate in unseenaenvironments is a challenging problem for autonomous embodied agents. The agent not only needs to ground languages in visual scenes, but also should explore the environment to reach its target. In this work, we propose a dual-scale graph transformer (DUET) for joint long-term action planning and fine-grained cross-modal understanding. We build a topological map on-the-fly to enable efficient exploration in global action space. To balance the complexity of large action space reasoning and fine-grained language grounding, we dynamically combine a fine-scale encoding over local observations and a coarse-scale encoding on a global map via graph transformers. The proposed approach, DUET, significantly outperforms state-of-the-art methods on goal-oriented vision-and-language navigation (VLN) benchmarks REVERIE and SOON. It also improves the success rate on the fine-grained VLN benchmark R2R.","2575-7075","978-1-6654-6946-3","10.1109/CVPR52688.2022.01604","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9879544","Vision + language; Navigation and autonomous driving","Visualization;Computer vision;Navigation;Grounding;Benchmark testing;Transformers;Encoding","cognitive systems;graph theory;learning (artificial intelligence);mobile robots;multi-agent systems;robot vision","graph transformers;DUET;goal-oriented vision;-language navigation;fine-grained VLN benchmark R2R;think global;act local;dual-scale graph transformer;language instructions;autonomous embodied agents;long-term action planning;fine-grained cross-modal;topological map on-the-fly;global action space;action space reasoning;fine-grained language grounding;fine-scale encoding;local observations;coarse-scale encoding","","6","","57","IEEE","27 Sep 2022","","","IEEE","IEEE Conferences"
"MUSE-VAE: Multi-Scale VAE for Environment-Aware Long Term Trajectory Prediction","M. Lee; S. S. Sohn; S. Moon; S. Yoon; M. Kapadia; V. Pavlovic",Rutgers University; Rutgers University; Rutgers University; The College of New Jersey; Rutgers University; Rutgers University,"2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)","27 Sep 2022","2022","","","2211","2220","Accurate long-term trajectory prediction in complex scenes, where multiple agents (e.g., pedestrians or vehicles) interact with each other and the environment while attempting to accomplish diverse and often unknown goals, is a challenging stochastic forecasting problem. In this work, we propose MUSEVAE, a new probabilistic modeling framework based on a cascade of Conditional VAEs, which tackles the long-term, uncertain trajectory prediction task using a coarse-to-fine multi-factor forecasting architecture. In its Macro stage, the model learns a joint pixel-space representation of two key factors, the underlying environment and the agent movements, to predict the long and short term motion goals. Conditioned on them, the Micro stage learns a fine-grained spatio-temporal representation for the prediction of individual agent trajectories. The VAE backbones across the two stages make it possible to naturally account for the joint uncertainty at both levels of granularity. As a result, MUSEVAE offers diverse and simultaneously more accurate predictions compared to the current state-of-the-art. We demonstrate these assertions through a comprehensive set of experiments on nuScenes and SDD benchmarks as well as PFSD, a new synthetic dataset, which challenges the forecasting ability of models on complex agent-environment interaction scenarios.","2575-7075","978-1-6654-6946-3","10.1109/CVPR52688.2022.00226","NSF(grant numbers:IIS-1703883,IIS-1955404,IIS-1955365,RETTL-2119265,EAGER-2122119); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9880379","Behavior analysis; Computer vision for social good","Measurement;Uncertainty;Semantics;Stochastic processes;Predictive models;Probabilistic logic;Trajectory","forecasting theory;image representation;image resolution;multi-agent systems;probability;stochastic processes;traffic engineering computing","complex agent environment interaction;VAE backbones;individual agent trajectories;fine grained spatio temporal representation;microstage;short term motion goals;long term motion goals;agent movements;joint pixel space representation;Macro stage;coarse-to-fine multifactor;conditional VAE;uncertain trajectory prediction task;probabilistic modeling framework;MUSEVAE;stochastic forecasting problem;long-term trajectory prediction;environment aware long term trajectory prediction;multiscale VAE","","6","","46","IEEE","27 Sep 2022","","","IEEE","IEEE Conferences"
"Envedit: Environment Editing for Vision-and-Language Navigation","J. Li; H. Tan; M. Bansal",UNC Chapel Hill; UNC Chapel Hill; UNC Chapel Hill,"2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)","27 Sep 2022","2022","","","15386","15396","In Vision-and-Language Navigation (VLN), an agent needs to navigate through the environment based on nat-ural language instructions. Due to limited available data for agent training and finite diversity in navigation environments, it is challenging for the agent to generalize to new, unseen environments. To address this problem, we propose Envedit, a data augmentation method that cre-ates new environments by editing existing environments, which are used to train a more generalizable agent. Our augmented environments can differ from the seen environ-ments in three diverse aspects: style, object appearance, and object classes. Training on these edit-augmented environments prevents the agent from overfitting to existing en-vironments and helps generalize better to new, unseen en-vironments. Empirically, on both the Room-to-Room and the multi-lingual Room-Across-Room datasets, we show that our proposed Envedit method gets significant im-provements in all metrics on both pre-trained and non-pre-trained VLN agents, and achieves the new state-of-the-art on the test leaderboard. We further ensemble the VLN agents augmented on different edited environments and show that these edit methods are complementary.11Code and data are available at https://github.com/jialuli-luka/EnvEdit.","2575-7075","978-1-6654-6946-3","10.1109/CVPR52688.2022.01497","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9879298","Vision + language","Training;Measurement;Computer vision;Navigation;Pattern recognition","computer vision;learning (artificial intelligence);multi-agent systems","edit-augmented environments;Envedit method;nonpre-trained VLN agents;edit methods;environment editing;natural language instructions;agent training;navigation environments;data augmentation method;generalizable agent;room-across-room datasets;room-to-room datasets","","6","","69","IEEE","27 Sep 2022","","","IEEE","IEEE Conferences"
"Bridging the Gap Between Learning in Discrete and Continuous Environments for Vision-and-Language Navigation","Y. Hong; Z. Wang; Q. Wu; S. Gould",The Australian National University; The Australian National University; University of Adelaide; The Australian National University,"2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)","27 Sep 2022","2022","","","15418","15428","Most existing works in vision-and-language navigation (VLN) focus on either discrete or continuous environments, training agents that cannot generalize across the two. Although learning to navigate in continuous spaces is closer to the real-world, training such an agent is significantly more difficult than training an agent in discrete spaces. However, recent advances in discrete VLN are challenging to translate to continuous VLN due to the domain gap. The fundamental difference between the two setups is that discrete navigation assumes prior knowledge of the connectivity graph of the environment, so that the agent can effectively transfer the problem of navigation with low-level controls to jumping from node to node with high-level actions by grounding to an image of a navigable direction. To bridge the discrete-to-continuous gap, we propose a predictor to generate a set of candidate waypoints during navigation, so that agents designed with high-level actions can be transferred to and trained in continuous environments. We refine the connectivity graph of Matterport3D to fit the continuous Habitat-Matterport3D, and train the waypoints predictor with the refined graphs to produce accessible waypoints at each time step. Moreover, we demonstrate that the predicted waypoints can be augmented during training to diversify the views and paths, and therefore enhance agent's generalization ability. Through extensive experiments we show that agents navigating in continuous environments with predicted waypoints perform significantly better than agents using low-level actions, which reduces the absolute discrete-to-continuous gap by 11.76% Success Weighted by Path Length (SPL) for the Cross-Modal Matching Agent and 18.24% SPL for the VLN$$BERT. Our agents, trained with a simple imitation learning objective, outperform previous methods by a large margin, achieving new state-of-the-art results on the testing environments of the R2R-CE and the RxR-CE datasets.","2575-7075","978-1-6654-6946-3","10.1109/CVPR52688.2022.01500","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9879946","Vision + language","Training;Bridges;Computer vision;Navigation;Grounding;Pattern recognition;Task analysis","graph theory;learning (artificial intelligence);multi-agent systems;path planning","discrete environments;continuous environments;training agents;continuous spaces;discrete spaces;discrete VLN;continuous VLN;domain gap;discrete navigation;connectivity graph;low-level controls;high-level actions;navigable direction;discrete-to-continuous gap;waypoints predictor;predicted waypoints;low-level actions;Cross-Modal Matching Agent;VLN$$BERT;testing environments","","4","","59","IEEE","27 Sep 2022","","","IEEE","IEEE Conferences"
"Opening up Open World Tracking","Y. Liu; I. E. Zulfikar; J. Luiten; A. Dave; D. Ramanan; B. Leibe; A. Ošep; L. Leal-Taixé","Technical University of Munich, Germany; RWTH Aachen University, Germany; RWTH Aachen University, Germany; Carnegie Mellon University, USA; Carnegie Mellon University, USA; RWTH Aachen University, Germany; Technical University of Munich, Germany; Technical University of Munich, Germany","2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)","27 Sep 2022","2022","","","19023","19033","Tracking and detecting any object, including ones never-seen-before during model training, is a crucial but elusive capability of autonomous systems. An autonomous agent that is blind to never-seen-before objects poses a safety hazard when operating in the real world - and yet this is how almost all current systems work. One of the main obstacles towards advancing tracking any object is that this task is notoriously difficult to evaluate. A benchmark that would allow us to perform an apples-to-apples comparison of existing efforts is a crucial first step towards advancing this important research field. This paper addresses this evaluation deficit and lays out the landscape and evaluation methodology for detecting and tracking both known and unknown objects in the open-world setting. We propose a new benchmark, TAOOW. Tracking Any Object in an Open World, analyze existing efforts in multi-object tracking, and construct a baseline for this task while highlighting future challenges. We hope to open a new front in multi-object tracking research that will hopefully bring us a step closer to intelligent systems that can operate safely in the real world.","2575-7075","978-1-6654-6946-3","10.1109/CVPR52688.2022.01846","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9880207","Motion and tracking; Datasets and evaluation","Training;Heart;Computer vision;Autonomous systems;Benchmark testing;Hazards;Pattern recognition","multi-agent systems;object detection;object tracking","Open World tracking;model training;autonomous systems;autonomous agent;safety hazard;apples-to-apples comparison;evaluation deficit;known objects;unknown objects;open-world setting;multiobject tracking research;intelligent systems;object detection;TAOOW","","4","","92","IEEE","27 Sep 2022","","","IEEE","IEEE Conferences"
"Temporal Complementarity-Guided Reinforcement Learning for Image-to-Video Person Re-Identification","W. Wu; J. Liu; K. Zheng; Q. Sun; Z. Zha","University of Science and Technology of China, China; University of Science and Technology of China, China; University of Science and Technology of China, China; University of Science and Technology of China, China; University of Science and Technology of China, China","2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)","27 Sep 2022","2022","","","7309","7318","Image-to-video person re-identification aims to retrieve the same pedestrian as the image-based query from a video-based gallery set. Existing methods treat it as a cross-modality retrieval task and learn the common latent embeddings from image and video modalities, which are both less effective and efficient due to large modality gap and redundant feature learning by utilizing all video frames. In this work, we first regard this task as point-to-set matching problem identical to human decision process, and propose a novel Temporal Complementarity-Guided Reinforcement Learning (TCRL) approach for image-to-video person re-identification. TCRL employs deep reinforcement learning to make sequential judgments on dynamically selecting suitable amount of frames from gallery videos, and accumulate adequate temporal complementary information among these frames by the guidance of the query image, towards balancing efficiency and accuracy. Specifically, TCRL formulates point-to-set matching procedure as Markov decision process, where a sequential judgement agent measures the uncertainty between the query image and all historical frames at each time step, and verifies that sufficient complementary clues are accumulated for judgment (same or different) or one more frames are requested to assist judgment. Moreover, TCRL maintains a sequential feature extraction module with complementary residual detectors to dynamically suppress redundant salient regions and thoroughly mine diverse complementary clues among these selected frames for enhancing frame-level representation. Extensive experiments demonstrate the superiority of our method.","2575-7075","978-1-6654-6946-3","10.1109/CVPR52688.2022.00717","National Key R&D Program of China(grant numbers:2020AAA0105702); National Natural Science Foundation of China (NSFC)(grant numbers:U19B2038,62106245); Fundamental Research Funds for the Central Universities(grant numbers:WK2100000021); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9880409","Recognition: detection;categorization;retrieval; Representation learning; Video analysis and understanding","Representation learning;Uncertainty;Measurement uncertainty;Reinforcement learning;Detectors;Markov processes;Feature extraction","data mining;deep learning (artificial intelligence);feature extraction;image representation;image retrieval;image sequences;Markov processes;multi-agent systems;pedestrians;reinforcement learning;video signal processing","video modality;point-to-set matching problem;deep reinforcement learning;TCRL;image-to-video person re-identification;image-based query;video-based gallery;cross-modality retrieval;temporal complementarity-guided reinforcement learning;image modality;Markov decision process;sequential judgement agent;sequential feature extraction module;diverse complementary clue mining;frame-level representation;pedestrian;image retrieval","","3","","48","IEEE","27 Sep 2022","","","IEEE","IEEE Conferences"
"Consistency driven Sequential Transformers Attention Model for Partially Observable Scenes","S. B. Rangrej; C. L. Srinidhi; J. J. Clark","McGill University, Canada; Sunnybrook Research Institute, University of Toronto, Canada; McGill University, Canada","2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)","27 Sep 2022","2022","","","2508","2517","Most hard attention models initially observe a complete scene to locate and sense informative glimpses, and predict class-label of a scene based on glimpses. However, in many applications (e.g., aerial imaging), observing an entire scene is not always feasible due to the limited time and resources available for acquisition. In this paper, we develop a Sequential Transformers Attention Model (STAM) that only partially observes a complete image and predicts informative glimpse locations solely based on past glimpses. We design our agent using DeiT-distilled [44] and train it with a one-step actorcritic algorithm. Furthermore, to improve classification performance, we introduce a novel training objective, which enforces consistency between the class distribution predicted by a teacher model from a complete image and the class distribution predicted by our agent using glimpses. When the agent senses only 4% of the total image area, the inclusion of the proposed consistency loss in our training objective yields 3% and 8% higher accuracy on ImageNet and fMoW datasets, respectively. Moreover, our agent outperforms previous state-of-the-art by observing nearly 27% and 42% fewer pixels in glimpses on ImageNet and fMoW.","2575-7075","978-1-6654-6946-3","10.1109/CVPR52688.2022.00255","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9880412","Vision applications and systems; Efficient learning and inferences; Recognition: detection;categorization;retrieval; Scene analysis and understanding; Visual reasoning","Training;Computer vision;Computational modeling;Imaging;Predictive models;Transformers;Prediction algorithms","deep learning (artificial intelligence);image classification;image sequences;multi-agent systems;natural scenes","partially observable scenes;complete scene;informative glimpse locations;one-step actorcritic algorithm;class distribution;teacher model;agent;sequential transformers attention model;informative glimpse sensing;classification performance;ImageNet dataset;fMoW dataset","","2","","54","IEEE","27 Sep 2022","","","IEEE","IEEE Conferences"
"ProphNet: Efficient Agent-Centric Motion Forecasting with Anchor-Informed Proposals","X. Wang; T. Su; F. Da; X. Yang",QCraft; QCraft; QCraft; QCraft,"2023 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)","22 Aug 2023","2023","","","21995","22003","Motion forecasting is a key module in an autonomous driving system. Due to the heterogeneous nature of multi-sourced input, multimodality in agent behavior, and low latency required by onboard deployment, this task is notoriously challenging. To cope with these difficulties, this paper proposes a novel agent-centric model with anchor-informed proposals for efficient multimodal motion prediction. We design a modality-agnostic strategy to concisely encode the complex input in a unified manner. We generate diverse proposals, fused with anchors bearing goal-oriented scene context, to induce multimodal prediction that covers a wide range of future trajectories. Our network architecture is highly uniform and succinct, leading to an efficient model amenable for real-world driving deployment. Experiments reveal that our agent-centric network compares favorably with the state-of-the-art methods in prediction accuracy, while achieving scene-centric level inference latency.","2575-7075","979-8-3503-0129-8","10.1109/CVPR52729.2023.02106","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10204544","Autonomous driving","Predictive models;Network architecture;Encoding;Trajectory;Pattern recognition;Proposals;Forecasting","image motion analysis;inference mechanisms;mobile robots;multi-agent systems;prediction theory;road vehicles;robot vision","agent behavior;agent-centric motion forecasting;anchor-informed proposals;autonomous driving system;goal-oriented scene context;modality-agnostic strategy;multimodal motion prediction;multisourced input;onboard deployment;real-world driving deployment;scene-centric level inference","","2","","30","IEEE","22 Aug 2023","","","IEEE","IEEE Conferences"
"Decentralized Learning with Multi-Headed Distillation","A. Zhmoginov; M. Sandler; N. Miller; G. Kristiansen; M. Vladymyrov","Google AI, Mountain View, CA, USA; Google AI, Mountain View, CA, USA; Google AI, Mountain View, CA, USA; Google AI, Mountain View, CA, USA; Google AI, Mountain View, CA, USA","2023 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)","22 Aug 2023","2023","","","8053","8063","Decentralized learning with private data is a central problem in machine learning. We propose a novel distillation-based decentralized learning technique that allows multiple agents with private non-lid data to learn from each other, without having to share their data, weights or weight updates. Our approach is communication efficient, utilizes an unlabeled public dataset and uses multiple auxiliary heads for each client, greatly improving training efficiency in the case of heterogeneous data. This approach allows individual models to preserve and enhance performance on their private tasks while also dramatically improving their performance on the global aggregated data distribution. We study the effects of data and model architecture heterogeneity and the impact of the underlying communication graph topology on learning efficiency and show that our agents can significantly improve their performance compared to learning in isolation.","2575-7075","979-8-3503-0129-8","10.1109/CVPR52729.2023.00778","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10203315","Efficient and scalable vision","Training;Computer vision;Computational modeling;Machine learning;Computer architecture;Data models;Magnetic heads","data privacy;graph theory;learning (artificial intelligence);multi-agent systems","distillation-based decentralized learning;global aggregated data distribution;heterogeneous data;machine learning;model architecture heterogeneity;multiheaded distillation;multiple agents;multiple auxiliary heads;nonlid data;private data;private tasks;unlabeled public dataset;weight updates","","","","40","IEEE","22 Aug 2023","","","IEEE","IEEE Conferences"
"EXCALIBUR: Encouraging and Evaluating Embodied Exploration","H. Zhu; R. Kapoor; S. Y. Min; W. Han; J. Li; K. Geng; G. Neubig; Y. Bisk; A. Kembhavi; L. Weihs",Carnegie Mellon University; Carnegie Mellon University; Carnegie Mellon University; Allen Institute for Artificial Intelligence; Carnegie Mellon University; Carnegie Mellon University; Carnegie Mellon University; Carnegie Mellon University; Allen Institute for Artificial Intelligence; Allen Institute for Artificial Intelligence,"2023 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)","22 Aug 2023","2023","","","14931","14942","Experience precedes understanding. Humans constantly explore and learn about their environment out of curiosity, gather information, and update their models of the world. On the other hand, machines are either trained to learn passively from static and fixed datasets, or taught to complete specific goal-conditioned tasks. To encourage the development of exploratory interactive agents, we present the EXCALIBUR benchmark. EXCALIBUR allows agents to explore their environment for long durations and then query their understanding of the physical world via inquiries like: “is the small heavy red bowl made from glass?” or “is there a silver spoon heavier than the egg?”. This design encourages agents to perform free-form home exploration without myopia induced by goal conditioning. Once the agents have answered a series of questions, they can renter the scene to refine their knowledge, update their beliefs, and improve their performance on the questions. Our experiments demonstrate the challenges posed by this dataset for the present-day state-of-the-art embodied systems and the headroom afforded to develop new innovative methods. Finally, we present a virtual reality interface that enables humans to seamlessly interact within the simulated world and use it to gather human performance measures. EXCALIBUR affords unique challenges in comparison to presentday benchmarks and represents the next frontier for embodied AI research.","2575-7075","979-8-3503-0129-8","10.1109/CVPR52729.2023.01434","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10204393","Embodied vision: Active agents;simulation","Solid modeling;Silver;Computer vision;Virtual reality;Glass;Benchmark testing;Pattern recognition","artificial intelligence;learning (artificial intelligence);multi-agent systems;virtual reality","embodied AI research;embodied exploration evaluation;EXCALIBUR benchmark;exploratory interactive agents;fixed datasets;free-form home exploration;goal conditioning;heavy red bowl;human performance measures;physical world;present-day state-of-the-art embodied systems;silver spoon;simulated world;specific goal-conditioned tasks;static datasets;virtual reality interface","","","","68","IEEE","22 Aug 2023","","","IEEE","IEEE Conferences"
"Imitation Learning as State Matching via Differentiable Physics","S. Chen; X. Ma; Z. Xu",National University of Singapore; Sea AI Lab; Sea AI Lab,"2023 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)","22 Aug 2023","2023","","","7846","7855","Existing imitation learning (IL) methods such as inverse reinforcement learning (IRL) usually have a double-loop training process, alternating between learning a reward function and a policy and tend to suffer long training time and high variance. In this work, we identify the benefits of differentiable physics simulators and propose a new IL method, i.e., Imitation Learning as State Matching via Differentiable Physics (ILD), which gets rid of the double-loop design and achieves significant improvements in final performance, convergence speed, and stability. The proposed ILD incorporates the differentiable physics simulator as a physics prior into its computational graph for policy learning. ILD unrolls the dynamics by sampling actions from a parameterized policy and minimizing the distance between the expert trajectory and the agent trajectory. It back-propagates the gradient into the policy via temporal physics operators, which improves the transferability to unseen environments and yields higher final performance. ILD has a single-loop structure that stabilizes and speeds up training. It dynamically selects learning objectives for each state during optimization to simplify the complex optimization land-scape. Experiments show that ILD outperforms state-of-the-art methods in continuous control tasks with Brax, and can be applied to deformable object manipulation tasks, generalized to unseen configurations.11The link to the code: https://github.com/sail-sg/ILD","2575-7075","979-8-3503-0129-8","10.1109/CVPR52729.2023.00758","National Research Foundation, Singapore(grant numbers:AISG2-PhD-2021-08-015T); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10204050","Robotics","Training;Heuristic algorithms;Reinforcement learning;Stability analysis;Trajectory;Pattern recognition;Task analysis","gradient methods;graph theory;learning (artificial intelligence);multi-agent systems","agent trajectory;backpropagation;computational graph;differentiable physics simulators;double-loop design;double-loop training process;DPS;expert trajectory;gradient method;ILD;imitation learning as state matching via differentiable physics;policy learning;temporal physics operators","","","","41","IEEE","22 Aug 2023","","","IEEE","IEEE Conferences"
"D2Former: Jointly Learning Hierarchical Detectors and Contextual Descriptors via Agent-Based Transformers","J. He; Y. Gao; T. Zhang; Z. Zhang; F. Wu",University of Science and Technology of China; University of Science and Technology of China; University of Science and Technology of China; Deep Space Exploration Laboratory; University of Science and Technology of China,"2023 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)","22 Aug 2023","2023","","","2904","2914","Establishing pixel-level matches between image pairs is vital for a variety of computer vision applications. How-ever, achieving robust image matching remains challenging because CNN extracted descriptors usually lack discrim-inative ability in texture-less regions and keypoint detec-tors are only good at identifying keypoints with a specific level of structure. To deal with these issues, a novel im-age matching method is proposed by Jointly Learning Hier-archical Detectors and Contextual Descriptors via Agent-based Transformers (D2Former), including a contextual feature descriptor learning (CFDL) module and a hierar-chical keypoint detector learning (HKDL) module. The proposed D2 Former enjoys several merits. First, the pro-posed CFDL module can model long-range contexts effi-ciently and effectively with the aid of designed descriptor agents. Second, the HKDL module can generate keypoint detectors in a hierarchical way, which is helpful for detecting keypoints with diverse levels of structures. Extensive experimental results on four challenging benchmarks show that our proposed method significantly outperforms state-of-the-art image matching methods.","2575-7075","979-8-3503-0129-8","10.1109/CVPR52729.2023.00284","National Defense Basic Scientific Research Program of China(grant numbers:JCKY2021601B013); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10203099","Recognition: Categorization;detection;retrieval","Computer vision;Image matching;Detectors;Benchmark testing;Transformers;Feature extraction;Pattern recognition","blockchains;computer vision;digital signatures;feature extraction;image matching;Internet of Things;learning (artificial intelligence);multi-agent systems;transforms","Agent-based Transformers;CFDL module;computer vision applications;Contextual Descriptors;contextual feature descriptor;designed descriptor agents;detec-tors;discrim-inative ability;establishing pixel-level matches;Hier-archical Detectors;hierar-chical keypoint detector learning module;hierarchical Detectors;HKDL module;image pairs;keypoint detectors;novel im-age matching method;robust image matching;state-of-the-art image matching methods;texture-less regions","","","","55","IEEE","22 Aug 2023","","","IEEE","IEEE Conferences"
"MIXSIM: A Hierarchical Framework for Mixed Reality Traffic Simulation","S. Suo; K. Wong; J. Xu; J. Tu; A. Cui; S. Casas; R. Urtasun",Waabi; Waabi; Waabi; Waabi; Waabi; Waabi; Waabi,"2023 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)","22 Aug 2023","2023","","","9622","9631","The prevailing way to test a self-driving vehicle (SDV) in simulation involves non-reactive open-loop replay of real world scenarios. However, in order to safely deploy SDVs to the real world, we need to evaluate them in closed-loop. Towards this goal, we propose to leverage the wealth of interesting scenarios captured in the real world and make them reactive and controllable to enable closed-loop SDV evaluation in what-if situations. In particular, we present MIXSIM, a hierarchical framework for mixed reality traffic simulation. MIXSIM explicitly models agent goals as routes along the road network and learns a reactive route-conditional policy. By inferring each agent's route from the original scenario, MIXSIM can reactively re-simulate the scenario and enable testing different autonomy systems under the same conditions. Furthermore, by varying each agent's route, we can expand the scope of testing to what-if situations with realistic variations in agent behaviors or even safety critical interactions. Our experiments show that MIXSIM can serve as a realistic, reactive, and controllable digital twin of real world scenarios. For more information, please visit the project website: https://waabi.ai/research/mixsim/","2575-7075","979-8-3503-0129-8","10.1109/CVPR52729.2023.00928","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10205254","Autonomous driving","Training;Roads;Mixed reality;Traffic control;Behavioral sciences;Safety;Pattern recognition","control engineering computing;digital twins;intelligent transportation systems;multi-agent systems;road traffic;road vehicles;traffic engineering computing;virtual reality","agent behaviors;closed-loop SDV evaluation;controllable digital twin;hierarchical framework;interesting scenarios;mixed reality traffic simulation;MIXSIM explicitly models agent goals;open-loop replay;original scenario;reactive route-conditional policy;reactive, twin;realistic twin;self-driving vehicle;world scenarios","","","","46","IEEE","22 Aug 2023","","","IEEE","IEEE Conferences"
"Behavioral Analysis of Vision-and-Language Navigation Agents","Z. Yang; A. Majumdar; S. Lee",Oregon State University; Georgia Institute of Technology; Oregon State University,"2023 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)","22 Aug 2023","2023","","","2574","2582","To be successful, Vision-and-Language Navigation (VLN) agents must be able to ground instructions to actions based on their surroundings. In this work, we develop a methodology to study agent behavior on a skill-specific basis - examining how well existing agents ground instructions about stopping, turning, and moving towards specified objects or rooms. Our approach is based on generating skill-specific interventions and measuring changes in agent predictions. We present a detailed case study analyzing the behavior of a recent agent and then compare multiple agents in terms of skill-specific competency scores. This analysis suggests that biases from training have lasting effects on agent behavior and that existing models are able to ground simple referring expressions. Our comparisons between models show that skill-specific scores correlate with improvements in overall VLN task performance.","2575-7075","979-8-3503-0129-8","10.1109/CVPR52729.2023.00253","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10203467","Vision;language;reasoning","Training;Computer vision;Analytical models;Navigation;Computational modeling;Turning;Behavioral sciences","multi-agent systems;natural language interfaces;navigation","agent behavior;agent predictions;behavioral analysis;skill-specific basis;skill-specific competency scores;skill-specific intervention;vision-and-language navigation;VLN agents;VLN task performance","","","","23","IEEE","22 Aug 2023","","","IEEE","IEEE Conferences"
"Coaching a Teachable Student","J. Zhang; Z. Huang; E. Ohn-Bar",Boston University; Boston University; Boston University,"2023 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)","22 Aug 2023","2023","","","7805","7815","We propose a novel knowledge distillation framework for effectively teaching a sensorimotor student agent to drive from the supervision of a privileged teacher agent. Current distillation for sensorimotor agents methods tend to result in suboptimal learned driving behavior by the student, which we hypothesize is due to inherent differences between the input, modeling capacity, and optimization processes of the two agents. We develop a novel distillation scheme that can address these limitations and close the gap between the sensorimotor agent and its privileged teacher. Our key insight is to design a student which learns to align their input features with the teacher's privileged Bird's Eye View (BEV) space. The student then can benefit from direct supervision by the teacher over the internal representation learning. To scaffold the difficult sensorimotor learning task, the student model is optimized via a student-paced coaching mechanism with various auxiliary supervision. We further propose a high-capacity imitation learned privileged agent that surpasses prior privileged agents in CARLA and ensures the student learns safe driving behavior. Our proposed sensorimotor agent results in a robust image-based behavior cloning agent in CARLA, improving over current models by over 20.6% in driving score without requiring LiDAR, historical observations, ensemble of models, on-policy data aggregation or reinforcement learning.","2575-7075","979-8-3503-0129-8","10.1109/CVPR52729.2023.00754","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10203984","Autonomous driving","Representation learning;Training;Analytical models;Computational modeling;Reinforcement learning;Data models;Behavioral sciences","learning (artificial intelligence);multi-agent systems;reinforcement learning;teaching","auxiliary supervision;current distillation;difficult sensorimotor learning task;direct supervision;driving score;internal representation learning;knowledge distillation framework;novel distillation scheme;on-policy data aggregation;privileged agent;privileged teacher agent;safe driving behavior;sensorimotor agent results;sensorimotor agents methods;sensorimotor student agent;student model;student-paced coaching mechanism;suboptimal learned driving behavior;teachable student;teacher's privileged Bird's Eye View space","","","","85","IEEE","22 Aug 2023","","","IEEE","IEEE Conferences"
"Towards real-world navigation with deep differentiable planners","S. Ishida; J. F. Henriques","Visual Geometry Group, University of Oxford; Visual Geometry Group, University of Oxford","2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)","27 Sep 2022","2022","","","17306","17315","We train embodied neural networks to plan and navigate unseen complex 3D environments, emphasising real-world deployment. Rather than requiring prior knowledge of the agent or environment, the planner learns to model the state transitions and rewards. To avoid the potentially hazardous trial-and-error of reinforcement learning, we focus on differentiable planners such as Value Iteration Networks (VIN), which are trained offline from safe expert demonstrations. Although they work well in small simulations, we address two major limitations that hinder their deployment. First, we observed that current differentiable planners struggle to plan long-term in environments with a high branching complexity. While they should ideally learn to assign low rewards to obstacles to avoid collisions, these penalties are not strong enough to guarantee collision-free operation. We thus impose a structural constraint on the value iteration, which explicitly learns to model impossible actions and noisy motion. Secondly, we extend the model to plan exploration with a limited perspective camera under translation and fine rotations, which is crucial for real robot deployment. Our proposals significantly improve semantic navigation and exploration on several 2D and 3D environments, succeeding in settings that are otherwise challenging for differentiable planners. As far as we know, we are the first to successfully apply them to the difficult Active Vision Dataset, consisting of real images captured from a robot. 11Code available: https://github.com/shuishida/calvin","2575-7075","978-1-6654-6946-3","10.1109/CVPR52688.2022.01681","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9878917","Navigation and autonomous driving; Deep learning architectures and techniques; Recognition: detection;categorization;retrieval; Representation learning; RGBD sensors and analytics; Robot vision; Vision applications and systems","Representation learning;Three-dimensional displays;Navigation;Robot vision systems;Semantics;Neural networks;Reinforcement learning","collision avoidance;learning (artificial intelligence);mobile robots;multi-agent systems;neural nets;neurocontrollers;path planning;remotely operated vehicles","safe expert demonstrations;current differentiable planners struggle;high branching complexity;low rewards;collision-free operation;model impossible actions;plan exploration;robot deployment;semantic navigation;real-world navigation;deep differentiable planners;neural networks;real-world deployment;state transitions;-error;reinforcement learning;Value Iteration Networks","","","","49","IEEE","27 Sep 2022","","","IEEE","IEEE Conferences"
"GATSBI: Generative Agent-centric Spatio-temporal Object Interaction","C. -H. Min; J. Bae; J. Lee; Y. M. Kim","Dept. of Electrical and Computer Engineering, Seoul National University, Korea; Dept. of Electrical and Computer Engineering, Seoul National University, Korea; Dept. of Electrical and Computer Engineering, Seoul National University, Korea; Dept. of Electrical and Computer Engineering, Seoul National University, Korea","2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)","2 Nov 2021","2021","","","3073","3082","We present GATSBI, a generative model that can transform a sequence of raw observations into a structured latent representation that fully captures the spatiotemporal context of the agent’s actions. In vision-based decision making scenarios, an agent faces complex high-dimensional observations where multiple entities interact with each other. The agent requires a good scene representation of the visual observation that discerns essential components and consistently propagates along the time horizon. Our method, GATSBI, utilizes unsupervised object-centric scene representation learning to separate an active agent, static back-ground, and passive objects. GATSBI then models the interactions reflecting the causal relationships among decomposed entities and predicts physically plausible future states. Our model generalizes to a variety of environments where different types of robots and objects dynamically interact with each other. We show GATSBI achieves superior performance on scene decomposition and video prediction compared to its state-of-the-art counterparts.","2575-7075","978-1-6654-4509-2","10.1109/CVPR46437.2021.00309","National Research Foundation; National Research Foundation; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9578878","","Visualization;Computational modeling;Transforms;Reinforcement learning;Predictive models;Trajectory;Spatiotemporal phenomena","data visualisation;decision making;image representation;image sequences;multi-agent systems;robot vision;spatiotemporal phenomena;unsupervised learning","good scene representation;visual observation;discerns essential components;GATSBI;unsupervised object-centric scene representation learning;active agent;passive objects;scene decomposition;video prediction;generative agent-centric spatio-temporal object interaction;generative model;raw observations;structured latent representation;spatiotemporal context;vision-based decision making scenarios;high-dimensional observations;multiple entities interact","","","","46","IEEE","2 Nov 2021","","","IEEE","IEEE Conferences"
"GeoSim: Realistic Video Simulation via Geometry-Aware Composition for Self-Driving","Y. Chen; F. Rong; S. Duggal; S. Wang; X. Yan; S. Manivasagam; S. Xue; E. Yumer; R. Urtasun",Uber Advanced Technologies Group; Uber Advanced Technologies Group; Uber Advanced Technologies Group; Uber Advanced Technologies Group; Uber Advanced Technologies Group; Uber Advanced Technologies Group; Uber Advanced Technologies Group; Uber Advanced Technologies Group; Uber Advanced Technologies Group,"2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)","2 Nov 2021","2021","","","7226","7236","Scalable sensor simulation is an important yet challenging open problem for safety-critical domains such as self-driving. Current works in image simulation either fail to be photorealistic or do not model the 3D environment and the dynamic objects within, losing high-level control and physical realism. In this paper, we present GeoSim, a geometry-aware image composition process which synthesizes novel urban driving scenarios by augmenting existing images with dynamic objects extracted from other scenes and rendered at novel poses. Towards this goal, we first build a diverse bank of 3D objects with both realistic geometry and appearance from sensor data. During simulation, we perform a novel geometry-aware simulation-by-composition procedure which 1) proposes plausible and realistic object placements into a given scene, 2) renders novel views of dynamic objects from the asset bank, and 3) composes and blends the rendered image segments. The resulting synthetic images are realistic, traffic-aware, and geometrically consistent, allowing our approach to scale to complex use cases. We demonstrate two such important applications: long-range realistic video simulation across multiple camera sensors, and synthetic data generation for data augmentation on downstream segmentation tasks. Please check https://tmux.top/publication/geosim/ for high-resolution video results.","2575-7075","978-1-6654-4509-2","10.1109/CVPR46437.2021.00715","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9578059","","Geometry;Image segmentation;Solid modeling;Visualization;Three-dimensional displays;Semantics;Manuals","cameras;feature extraction;geometry;image segmentation;image sensors;object detection;rendering (computer graphics)","rendered image segments;resulting synthetic images;traffic-aware;long-range realistic video simulation;multiple camera sensors;data augmentation;high-resolution video results;geometry-aware composition;scalable sensor simulation;open problem;safety-critical domains;current works;image simulation;dynamic objects;high-level control;physical realism;geometry-aware image composition process;urban driving scenarios;existing images;diverse bank;realistic geometry;sensor data;novel geometry-aware simulation-by-composition procedure;realistic object placements","","30","","83","IEEE","2 Nov 2021","","","IEEE","IEEE Conferences"
"PifPaf: Composite Fields for Human Pose Estimation","S. Kreiss; L. Bertoni; A. Alahi","EPFL VITA lab, Lausanne; EPFL VITA lab, Lausanne; EPFL VITA lab, Lausanne","2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)","9 Jan 2020","2019","","","11969","11978","We propose a new bottom-up method for multi-person 2D human pose estimation that is particularly well suited for urban mobility such as self-driving cars and delivery robots. The new method, PifPaf, uses a Part Intensity Field (PIF) to localize body parts and a Part Association Field (PAF) to associate body parts with each other to form full human poses. Our method outperforms previous methods at low resolution and in crowded, cluttered and occluded scenes thanks to (i) our new composite field PAF encoding fine-grained information and (ii) the choice of Laplace loss for regressions which incorporates a notion of uncertainty. Our architecture is based on a fully convolutional, single-shot, box-free design. We perform on par with the existing state-of-the-art bottom-up method on the standard COCO keypoint task and produce state-of-the-art results on a modified COCO keypoint task for the transportation domain.","2575-7075","978-1-7281-3293-8","10.1109/CVPR.2019.01225","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8953198","Face;Gesture;and Body Pose;Deep Learning","","neural nets;pose estimation","self-driving cars;delivery robots;PifPaf;part intensity field;part association field;box-free design;urban mobility;multiperson 2D human pose estimation;COCO keypoint task;transportation domain;composite field PAF encoding;single-shot design;fully convolutional design;occluded scenes;cluttered scenes;crowded scenes","","234","","46","IEEE","9 Jan 2020","","","IEEE","IEEE Conferences"
"Parsing R-CNN for Instance-Level Human Analysis","L. Yang; Q. Song; Z. Wang; M. Jiang",Beijing University of Posts and Telecommunications; Beijing University of Posts and Telecommunications; Beijing University of Posts and Telecommunications; WiWide Inc.,"2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)","9 Jan 2020","2019","","","364","373","Instance-level human analysis is common in real-life scenarios and has multiple manifestations, such as human part segmentation, dense pose estimation, human-object interactions, etc. Models need to distinguish different human instances in the image panel and learn rich features to represent the details of each instance. In this paper, we present an end-to-end pipeline for solving the instance-level human analysis, named Parsing R-CNN. It processes a set of human instances simultaneously through comprehensive considering the characteristics of region-based approach and the appearance of a human, thus allowing representing the details of instances. Parsing R-CNN is very flexible and efficient, which is applicable to many issues in human instance analysis. Our approach outperforms all state-of-the-art methods on CIHP (Crowd Instance-level Human Parsing), MHP v2.0 (Multi-Human Parsing) and DensePose-COCO datasets. Based on the proposed Parsing R-CNN, we reach the 1st place in the COCO 2018 Challenge DensePose Estimation task. Code and models are publicly available.","2575-7075","978-1-7281-3293-8","10.1109/CVPR.2019.00045","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8953214","Deep Learning;Face;Gesture;and Body Pose;Recognition: Detection;Categorization;Retrieval;Segmentation;Grou","Image segmentation;Computer vision;Analytical models;Codes;Pose estimation;Pipelines;Benchmark testing","image representation;image segmentation;learning (artificial intelligence);pose estimation","instance-level human analysis;human part segmentation;human-object interactions;human instances;Parsing R-CNN;human instance analysis;multihuman parsing;crowd instance-level human parsing","","62","","50","IEEE","9 Jan 2020","","","IEEE","IEEE Conferences"
"Deep Single Image Camera Calibration With Radial Distortion","M. López; R. Marí; P. Gargallo; Y. Kuang; J. Gonzalez-Jimenez; G. Haro","Mapillary; CMLA, ENS Cachan; Mapillary; Mapillary; Universidad de Málaga; Universitat Pompeu Fabra","2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)","9 Jan 2020","2019","","","11809","11817","Single image calibration is the problem of predicting the camera parameters from one image. This problem is of importance when dealing with images collected in uncontrolled conditions by non-calibrated cameras, such as crowd-sourced applications. In this work we propose a method to predict extrinsic (tilt and roll) and intrinsic (focal length and radial distortion) parameters from a single image. We propose a parameterization for radial distortion that is better suited for learning than directly predicting the distortion parameters. Moreover, predicting additional heterogeneous variables exacerbates the problem of loss balancing. We propose a new loss function based on point projections to avoid having to balance heterogeneous loss terms. Our method is, to our knowledge, the first to jointly estimate the tilt, roll, focal length, and radial distortion parameters from a single image. We thoroughly analyze the performance of the proposed method and the impact of the improvements and compare with previous approaches for single image radial distortion correction.","2575-7075","978-1-7281-3293-8","10.1109/CVPR.2019.01209","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8954230","3D from Single Image;Deep Learning;Vision + Graphics;Vision Applications and Systems","Learning systems;Computer vision;Structure from motion;Computational modeling;Distortion;Cameras;Robustness","calibration;cameras","deep single image camera calibration;noncalibrated cameras;crowd-sourced applications;loss balancing;radial distortion parameters;single image radial distortion correction;heterogeneous loss variable terms;camera parameter prediction","","28","1","22","IEEE","9 Jan 2020","","","IEEE","IEEE Conferences"
"How many Observations are Enough? Knowledge Distillation for Trajectory Forecasting","A. Monti; A. Porrello; S. Calderara; P. Coscia; L. Ballan; R. Cucchiara","University of Modena and Reggio Emilia, Italy; University of Modena and Reggio Emilia, Italy; University of Modena and Reggio Emilia, Italy; University of Padova, Italy; University of Padova, Italy; University of Modena and Reggio Emilia, Italy","2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)","27 Sep 2022","2022","","","6543","6552","Accurate prediction of future human positions is an essential task for modern video-surveillance systems. Current state-of-the-art models usually rely on a “history” of past tracked locations (e.g., 3 to 5 seconds) to predict a plausible sequence of future locations (e.g., up to the next 5 seconds). We feel that this common schema neglects critical traits of realistic applications: as the collection of input trajectories involves machine perception (i.e., detection and tracking), incorrect detection and fragmentation errors may accumulate in crowded scenes, leading to tracking drifts. On this account, the model would be fed with corrupted and noisy input data, thus fatally affecting its prediction performance. In this regard, we focus on delivering accurate predictions when only few input observations are used, thus potentially lowering the risks associated with automatic perception. To this end, we conceive a novel distillation strategy that allows a knowledge transfer from a teacher network to a student one, the latter fed with fewer observations (just two ones). We show that a properly defined teacher super-vision allows a student network to perform comparably to state-of-the-art approaches that demand more observations. Besides, extensive experiments on common trajectory forecasting datasets highlight that our student network better generalizes to unseen scenarios.","2575-7075","978-1-6654-6946-3","10.1109/CVPR52688.2022.00644","CUP(grant numbers:E94I19000650001); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9879765","Motion and tracking; Action and event recognition; Navigation and autonomous driving; Robot vision","Training;Tracking;Robot vision systems;Predictive models;Transformers;Trajectory;Pattern recognition","computer vision;learning (artificial intelligence);object detection;object tracking;video surveillance","knowledge distillation;video-surveillance systems;tracked locations;plausible sequence;realistic applications;input trajectories;machine perception;fragmentation errors;crowded scenes;tracking drifts;corrupted input data;noisy input data;prediction performance;input observations;automatic perception;distillation strategy;knowledge transfer;teacher network;student network;trajectory forecasting dataset;time 3.0 s to 5.0 s","","6","","52","IEEE","27 Sep 2022","","","IEEE","IEEE Conferences"
"Instance-wise Occlusion and Depth Orders in Natural Scenes","H. Lee; J. Park",LG AI Research; POSTECH GSAI & CSE,"2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)","27 Sep 2022","2022","","","21178","21189","In this paper, we introduce a new dataset, named InstaOrder, that can be used to understand the geometrical relationships of instances in an image. The dataset consists of 2.9M annotations of geometric orderings for class-labeled instances in 101K natural scenes. The scenes were annotated by 3,659 crowd-workers regarding (1) occlusion order that identifies occluder/occludee and (2) depth order that describes ordinal relations that consider relative distance from the camera. The dataset provides joint annotation of two kinds of orderings for the same instances, and we discover that the occlusion order and depth order are complementary. We also introduce a geometric order prediction network called InstaOrderNet, which is superior to state-of-the-art approaches. Moreover, we propose a dense depth prediction network called InstaDepthNet that uses auxiliary geometric order loss to boost the accuracy of the state-of-the-art depth prediction approach, MiDaS [54].","2575-7075","978-1-6654-6946-3","10.1109/CVPR52688.2022.02053","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9879265","Datasets and evaluation; 3D from single images; Scene analysis and understanding","Computer vision;Annotations;Cameras;Pattern recognition","cameras;hidden feature removal;image motion analysis;image segmentation;natural scenes;stereo image processing","natural scenes;geometrical relationships;geometric orderings;class-labeled instances;crowd-workers;ordinal relations;joint annotation;geometric order prediction network;dense depth prediction network;auxiliary geometric order loss;state-of-the-art depth prediction approach;InstaOrder;InstaDepthNet","","6","","85","IEEE","27 Sep 2022","","","IEEE","IEEE Conferences"
"Privacy Preserving Localization and Mapping from Uncalibrated Cameras","M. Geppert; V. Larsson; P. Speciale; J. L. Schönberger; M. Pollefeys","Department of Computer Science, ETH Zurich; Department of Computer Science, ETH Zurich; Microsoft; Microsoft; Department of Computer Science, ETH Zurich","2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)","2 Nov 2021","2021","","","1809","1819","Recent works on localization and mapping from privacy preserving line features have made significant progress towards addressing the privacy concerns arising from cloud-based solutions in mixed reality and robotics. The requirement for calibrated cameras is a fundamental limitation for these approaches, which prevents their application in many crowd-sourced mapping scenarios. In this paper, we propose a solution to the uncalibrated privacy preserving localization and mapping problem. Our approach simultaneously recovers the intrinsic and extrinsic calibration of a camera from line-features only. This enables uncalibrated devices to both localize themselves within an existing map as well as contribute to the map, while preserving the privacy of the image contents. Furthermore, we also derive a solution to bootstrapping maps from scratch using only uncalibrated devices. Our approach provides comparable performance to the calibrated scenario and the privacy compromising alternatives based on traditional point features.","2575-7075","978-1-6654-4509-2","10.1109/CVPR46437.2021.00185","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9578648","","Location awareness;Performance evaluation;Privacy;Robot vision systems;Pose estimation;Poles and towers;Cameras","calibration;cameras;cloud computing;data privacy","intrinsic calibration;extrinsic calibration;line-features;uncalibrated devices;existing map;bootstrapping maps;calibrated scenario;privacy compromising alternatives;traditional point features;uncalibrated cameras;privacy preserving line features;privacy concerns;cloud-based solutions;mixed reality;robotics;calibrated cameras;fundamental limitation;crowd-sourced mapping scenarios;uncalibrated privacy preserving localization;mapping problem","","5","","68","IEEE","2 Nov 2021","","","IEEE","IEEE Conferences"
"Beyond Supervised vs. Unsupervised: Representative Benchmarking and Analysis of Image Representation Learning","M. Gwilliam; A. Shrivastava","University of Maryland, College Park; University of Maryland, College Park","2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)","27 Sep 2022","2022","","","9632","9642","By leveraging contrastive learning, clustering, and other pretext tasks, unsupervised methods for learning image representations have reached impressive results on standard benchmarks. The result has been a crowded field - many methods with substantially different implementations yield results that seem nearly identical on popular benchmarks, such as linear evaluation on ImageNet. However, a single result does not tell the whole story. In this paper, we compare methods using performance-based benchmarks such as linear evaluation, nearest neighbor classification, and clustering for several different datasets, demonstrating the lack of a clear front-runner within the current state-of-the-art. In contrast to prior work that performs only supervised vs. unsupervised comparison, we compare several different unsupervised methods against each other. To enrich this comparison, we analyze embeddings with measurements such as uniformity, tolerance, and centered kernel alignment (CKA), and propose two new metrics of our own: nearest neighbor graph similarity and linear prediction overlap. We reveal through our analysis that in isolation, single popular methods should not be treated as though they represent the field as a whole, and that future work ought to consider how to leverage the complimentary nature of these methods. We also leverage CKA to provide a framework to robustly quantify augmentation invariance, and provide a reminder that certain types of invariance will be undesirable for downstream tasks.","2575-7075","978-1-6654-6946-3","10.1109/CVPR52688.2022.00942","DARPA SAIL-ON(grant numbers:W911NF2020009); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9879692","Self-& semi-& meta- Representation learning","Representation learning;Measurement;Computer vision;Current measurement;Benchmark testing;Image representation;Pattern recognition","graph theory;image classification;image representation;pattern clustering;supervised learning;unsupervised learning","CKA;downstream tasks;supervised method;representative benchmarking;image representation learning;contrastive learning;pretext tasks;image representations;standard benchmarks;crowded field;linear evaluation;performance-based benchmarks;nearest neighbor classification;unsupervised methods;nearest neighbor graph similarity;linear prediction overlap;centered kernel alignment","","3","","60","IEEE","27 Sep 2022","","","IEEE","IEEE Conferences"
"KeyTr: Keypoint Transporter for 3D Reconstruction of Deformable Objects in Videos","D. Novotny; I. Rocco; S. Sinha; A. Carlier; G. Kerchenbaum; R. Shapovalov; N. Smetanin; N. Neverova; B. Graham; A. Vedaldi",Meta AI; Meta AI; Meta AI; Meta AI; Meta AI; Meta AI; Meta AI; Meta AI; Meta AI; Meta AI,"2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)","27 Sep 2022","2022","","","5585","5594","We consider the problem of reconstructing the depth of dynamic objects from videos. Recent progress in dynamic video depth prediction has focused on improving the output of monocular depth estimators by means of multi-view constraints while imposing little to no restrictions on the deformation of the dynamic parts of the scene. However, the theory of Non-Rigid Structure from Motion prescribes to constrain the deformations for 3D reconstruction. We thus propose a new model that departs significantly from this prior work. The idea is to fit a dynamic point cloud to the video data using Sinkhorn's algorithm to associate the 3D points to 2D pixels and use a differentiable point renderer to ensure the compatibility of the 3D deformations with the measured optical flow. In this manner, our algorithm, called Keypoint Transporter, models the overall deformation of the object within the entire video, so it can constrain the reconstruction correspondingly. Compared to weaker deformation models, this significantly reduces the reconstruction ambiguity and, for dynamic objects, allows Keypoint Transporter to obtain reconstructions of the quality superior or at least comparable to prior approaches while being much faster and reliant on a pre-trained monocular depth estimator network. To assess the method, we evaluate on new datasets of synthetic videos depicting dynamic humans and animals with ground-truth depth. We also show qualitative results on crowd-sourced real-world videos of pets.","2575-7075","978-1-6654-6946-3","10.1109/CVPR52688.2022.00551","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9879736","3D from multi-view and sensors","Deformable models;Solid modeling;Three-dimensional displays;Structure from motion;Heuristic algorithms;Dynamics;Sensor systems","deformation;image motion analysis;image reconstruction;image resolution;image sequences;object detection;rendering (computer graphics);robot vision;video signal processing","deformable objects;dynamic objects;dynamic video depth prediction;monocular depth estimators;multiview constraints;dynamic parts;NonRigid Structure;Motion prescribes;deformations;dynamic point cloud;video data;Sinkhorn's algorithm;differentiable point renderer;measured optical flow;called Keypoint Transporter;weaker deformation models;reconstruction ambiguity;pre-trained monocular depth estimator network;synthetic videos;dynamic humans;ground-truth depth;crowd-sourced real-world videos","","2","","51","IEEE","27 Sep 2022","","","IEEE","IEEE Conferences"
"Trajectory-Aware Body Interaction Transformer for Multi-Person Pose Forecasting","X. Peng; S. Mao; Z. Wu","Department of Digital Media Technology, Hangzhou Dianzi University, Hangzhou, China; Department of Digital Media Technology, Hangzhou Dianzi University, Hangzhou, China; Department of Digital Media Technology, Hangzhou Dianzi University, Hangzhou, China","2023 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)","22 Aug 2023","2023","","","17121","17130","Multi-person pose forecasting remains a challenging problem, especially in modeling fine-grained human body interaction in complex crowd scenarios. Existing methods typically represent the whole pose sequence as a temporal series, yet overlook interactive influences among people based on skeletal body parts. In this paper, we propose a novel Trajectory-Aware Body Interaction Transformer (TBIFormer) for multi-person pose forecasting via effectively modeling body part interactions. Specifically, we construct a Temporal Body Partition Module that transforms all the pose sequences into a Multi-Person Body-Part sequence to retain spatial and temporal information based on body semantics. Then, we devise a Social Body Interaction Self-Attention (SBI-MSA) module, utilizing the transformed sequence to learn body part dynamics for inter- and intra-individual interactions. Furthermore, different from prior Euclidean distance-based spatial encodings, we present a novel and efficient Trajectory-Aware Relative Position Encoding for SBI-MSA to offer discriminative spatial information and additional interactive clues. On both short-and long-term horizons, we empirically evaluate our frame-work on CMU-Mocap, MuPoTS-3D as well as synthesized datasets (6 ~ 10 persons), and demonstrate that our method greatly outperforms the state-of-the-art methods.","2575-7075","979-8-3503-0129-8","10.1109/CVPR52729.2023.01642","Zhejiang Provincial Natural Science Foundation(grant numbers:LGF21F20012); Zhejiang Provincial Science and Technology Program in China(grant numbers:2021C03137); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10203728","Humans: Face;body;pose;gesture;movement","Training;Visualization;Semantics;Transforms;Predictive models;Transformers;Feature extraction","deep learning (artificial intelligence);image sequences;pose estimation","body part dynamics;body part interactions;body semantics;CMU-Mocap;complex crowd scenarios;fine-grained human body interaction;interactive clues;interactive influences;intra-individual interactions;multiperson body-part sequence;multiperson pose forecasting;MuPoTS-3D;pose sequences;skeletal body parts;social body interaction self-attention module;spatial information;temporal body partition module;temporal information;temporal series;trajectory-aware body interaction transformer;trajectory-aware relative position encoding;transformed sequence","","1","","44","IEEE","22 Aug 2023","","","IEEE","IEEE Conferences"
"MoDi: Unconditional Motion Synthesis from Diverse Data","S. Raab; I. Leibovitch; P. Li; K. Aberman; O. Sorkine-Hornung; D. Cohen-Or",Tel-Aviv University; Tel-Aviv University; ETH Zurich; Google Research; ETH Zurich; Tel-Aviv University,"2023 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)","22 Aug 2023","2023","","","13873","13883","The emergence of neural networks has revolutionized the field of motion synthesis. Yet, learning to unconditionally synthesize motions from a given distribution remains challenging, especially when the motions are highly diverse. In this work, we present MoDi - a generative model trained in an unsupervised setting from an extremely diverse, unstructured and unlabeled dataset. During inference, MoDi can synthesize high-quality, diverse motions. Despite the lack of any structure in the dataset, our model yields a well-behaved and highly structured latent space, which can be semantically clustered, constituting a strong motion prior that facilitates various applications including semantic editing and crowd animation. In addition, we present an encoder that inverts real motions into MoDi's natural motion manifold, issuing solutions to various ill-posed challenges such as completion from prefix and spatial editing. Our qualitative and quantitative experiments achieve state-of-the-art results that outperform recent SOTA techniques. Code and trained models are available at https://sigal-raab.github.io/MoDi.","2575-7075","979-8-3503-0129-8","10.1109/CVPR52729.2023.01333","Israel Science Foundation(grant numbers:2492/20,3441/21); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10205193","Humans: Face;body;pose;gesture;movement","Manifolds;Deep learning;Computer vision;Codes;Semantics;Neural networks;Training data","computer animation;deep learning (artificial intelligence);learning (artificial intelligence);unsupervised learning","crowd animation;diverse motions;extremely diverse dataset;generative model;given distribution;model yields;MoDi's natural motion manifold;neural networks;semantic editing;strong motion;trained models;unconditional motion synthesis;unlabeled dataset;unstructured dataset","","1","","62","IEEE","22 Aug 2023","","","IEEE","IEEE Conferences"
"Box-Level Active Detection","M. Lyu; J. Zhou; H. Chen; Y. Huang; D. Yu; Y. Li; Y. Guo; Y. Guo; L. Xiang; G. Ding",Tsinghua University; Tsinghua University; Tsinghua University; OPPO Research Institute; OPPO Research Institute; OPPO Research Institute; OPPO Research Institute; Tsinghua University; Beijing University of Posts and Telecommunications; Tsinghua University,"2023 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)","22 Aug 2023","2023","","","23766","23775","Active learning selects informative samples for annotation within budget, which has proven efficient recently on object detection. However, the widely used active detection benchmarks conduct image-level evaluation, which is unrealistic in human workload estimation and biased towards crowded images. Furthermore, existing methods still perform image-level annotation, but equally scoring all targets within the same image incurs waste of budget and redundant labels. Having revealed above problems and limitations, we introduce a box-level active detection framework that controls a box-based budget per cycle, prioritizes informative targets and avoids redundancy for fair comparison and efficient application. Under the proposed box-level setting, we devise a novel pipeline, namely Complementary Pseudo Active Strategy (ComPAS). It exploits both human annotations and the model intelligence in a complementary fashion: an efficient input-end committee queries labels for informative objects only; meantime well-learned targets are identified by the model and compensated with pseudo-labels. ComPAS consistently outperforms 10 competitors under 4 settings in a unified codebase. With supervision from labeled data only, it achieves 100% supervised performance of VOC0712 with merely 19% box annotations. On the COCO dataset, it yields up to 4.3% mAP improvement over the second-best method. ComPAS also supports training with the unlabeled pool, where it surpasses 90% COCO supervised performance with 85% label reduction. Our source code is publicly available at https://github.com/lyumengyao/blad.","2575-7075","979-8-3503-0129-8","10.1109/CVPR52729.2023.02276","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10203997","Recognition: Categorization;detection;retrieval","Training;Location awareness;Annotations;Source coding;Pipelines;Redundancy;Object detection","object detection;supervised learning","active learning;box-based budget;box-level active detection;ComPAS;complementary pseudoactive strategy;crowded images;efficient input-end committee;human annotations;human workload estimation;image-level annotation;image-level evaluation;label reduction;object detection;pseudolabels;redundant labels;supervised performance;VOC0712","","","","35","IEEE","22 Aug 2023","","","IEEE","IEEE Conferences"
"Common Pets in 3D: Dynamic New-View Synthesis of Real-Life Deformable Categories","S. Sinha; R. Shapovalov; J. Reizenstein; I. Rocco; N. Neverova; A. Vedaldi; D. Novotny",University of Toronto; Meta AI; Meta AI; Meta AI; Meta AI; Meta AI; Meta AI,"2023 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)","22 Aug 2023","2023","","","4881","4891","Obtaining photorealistic reconstructions of objects from sparse views is inherently ambiguous and can only be achieved by learning suitable reconstruction priors. Earlier works on sparse rigid object reconstruction successfully learned such priors from large datasets such as CO3D. In this paper, we extend this approach to dynamic objects. We use cats and dogs as a representative example and introduce Common Pets in 3D (CoP3D), a collection of crowd-sourced videos of approximately 4,200 distinct pets. CoP3D is one of the first large-scale datasets for benchmarking non-rigid 3D reconstruction “in the wild”. We also propose Tracker-NeRF, a method for learning 4D reconstruction from our dataset. At test time, given a small number of video frames of an unseen object, Tracker-NeRF predicts the trajectories of its 3D points and generates new views, interpolating viewpoint and time. Results on CoP3D reveal significantly better non-rigid new-view synthesis performance than existing baselines. The data will be available on the project webpage: https://cop3d.github.io/.","2575-7075","979-8-3503-0129-8","10.1109/CVPR52729.2023.00473","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10204777","3D from multi-view and sensors","Training;Visualization;Interpolation;Three-dimensional displays;Deformation;Dogs;Trajectory","image motion analysis;image reconstruction;interpolation;learning (artificial intelligence);video signal processing","200 distinct pets;approximately 4 pets;benchmarking nonrigid 3D reconstruction;CO3D;Common Pets;crowd-sourced videos;dynamic new-view synthesis;dynamic objects;large-scale datasets;nonrigid new-view synthesis performance;real-life deformable categories;sparse rigid object reconstruction;sparse views;suitable reconstruction priors;unseen object","","","","60","IEEE","22 Aug 2023","","","IEEE","IEEE Conferences"
"Mutual Information-Based Temporal Difference Learning for Human Pose Estimation in Video","R. Feng; Y. Gao; X. Ma; T. H. E. Tse; H. J. Chang","School of Artificial Intelligence, Jilin University; School of Artificial Intelligence, Jilin University; School of Artificial Intelligence, Jilin University; School of Computer Science, University of Birmingham; School of Computer Science, University of Birmingham","2023 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)","22 Aug 2023","2023","","","17131","17141","Temporal modeling is crucial for multi-frame human pose estimation. Most existing methods directly employ optical flow or deformable convolution to predict full-spectrum motion fields, which might incur numerous irrelevant cues, such as a nearby person or background. Without further efforts to excavate meaningful motion priors, their results are suboptimal, especially in complicated spatio-temporal interactions. On the other hand, the temporal difference has the ability to encode representative motion information which can potentially be valuable for pose estimation but has not been fully exploited. In this paper, we present a novel multi-frame human pose estimation framework, which employs temporal differences across frames to model dynamic contexts and engages mutual information objectively to facilitate useful motion information disentanglement. To be specific, we design a multi-stage Temporal Difference Encoder that performs incremental cascaded learning conditioned on multi-stage feature difference sequences to derive informative motion representation. We further propose a Representation Disentanglement module from the mutual information perspective, which can grasp discriminative task-relevant motion signals by explicitly defining useful and noisy constituents of the raw motion features and minimizing their mutual information. These place us to rank No.1 in the Crowd Pose Estimation in Complex Events Challenge on benchmark dataset HiEve, and achieve state-of-the-art performance on three benchmarks PoseTrack2017, PoseTrack2018, and PoseTrack21.","2575-7075","979-8-3503-0129-8","10.1109/CVPR52729.2023.01643","National Natural Science Foundation of China(grant numbers:62203184); ITRC(grant numbers:IITP-2022-2020-0-01789); IITP(grant numbers:RS-2022-00155054); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10204825","Humans: Face;body;pose;gesture;movement","Convolution;Pose estimation;Dynamics;Benchmark testing;Excavation;Pattern recognition;Noise measurement","feature extraction;image motion analysis;image sequences;learning (artificial intelligence);object detection;pose estimation;spatiotemporal phenomena;video signal processing","complicated spatio-temporal interactions;Crowd Pose Estimation;deformable convolution;discriminative task-relevant motion signals;dynamic contexts;engages mutual information;full-spectrum motion fields;informative motion representation;meaningful motion priors;multistage feature difference sequences;multistage Temporal Difference Encoder;mutual information perspective;mutual information-based Temporal Difference learning;nearby person;novel multiframe human pose estimation;numerous irrelevant cues;optical flow;raw motion features;Representation Disentanglement module;representative motion information;Temporal modeling;useful motion information disentanglement","","","","72","IEEE","22 Aug 2023","","","IEEE","IEEE Conferences"
"Learning to Adapt for Stereo","A. Tonioni; O. Rahnama; T. Joy; L. Di Stefano; T. Ajanthan; P. H. S. Torr","University of Bologna; University of Oxford; University of Oxford; Australian National University, Canberra, ACT, Australia; Australian National University; Univ. of Oxford","2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)","9 Jan 2020","2019","","","9653","9662","Real world applications of stereo depth estimation require models that are robust to dynamic variations in the environment. Even though deep learning based stereo methods are successful, they often fail to generalize to unseen variations in the environment, making them less suitable for practical applications such as autonomous driving. In this work, we introduce a ``learning-to-adapt'' framework that enables deep stereo methods to continuously adapt to new target domains in an unsupervised manner. Specifically, our approach incorporates the adaptation procedure into the learning objective to obtain a base set of parameters that are better suited for unsupervised online adaptation. To further improve the quality of the adaptation, we learn a confidence measure that effectively masks the errors introduced during the unsupervised adaptation. We evaluate our method on synthetic and real-world stereo datasets and our experiments evidence that learning-to-adapt is, indeed beneficial for online adaptation on vastly different domains.","2575-7075","978-1-7281-3293-8","10.1109/CVPR.2019.00989","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8954176","3D from Multiview and Sensors;Deep Learning","","learning (artificial intelligence);stereo image processing;unsupervised learning","stereo depth estimation;dynamic variations;deep learning;unseen variations;autonomous driving;learning-to-adapt framework;unsupervised manner;adaptation procedure;learning objective;unsupervised online adaptation;real-world stereo datasets;synthetic stereo datasets","","45","","35","IEEE","9 Jan 2020","","","IEEE","IEEE Conferences"
"Meta Agent Teaming Active Learning for Pose Estimation","J. Gong; Z. Fan; Q. Ke; H. Rahmani; J. Liu","Singapore University of Technology and Design, Singapore; New York University, United States; The University of Melbourne, Australia; Lancaster University, United Kingdom; Singapore University of Technology and Design, Singapore","2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)","27 Sep 2022","2022","","","11069","11079","The existing pose estimation approaches often require a large number of annotated images to attain good estimation performance, which are laborious to acquire. To reduce the human efforts on pose annotations, we propose a novel Meta Agent Teaming Active Learning (MATAL) framework to actively select and label informative images for effective learning. Our MATAL formulates the image selection procedure as a Markov Decision Process and learns an optimal sampling policy that directly maximizes the performance of the pose estimator based on the reward. Our framework consists of a novel state-action representation as well as a multi-agent team to enable batch sampling in the active learning procedure. The framework could be effectively optimized via Meta-Optimization to accelerate the adaptation to the gradually expanded labeled data during deployment. Finally, we show experimental results on both human hand and body pose estimation benchmark datasets and demonstrate that our method significantly outperforms all baselines continuously under the same amount of annotation budget. Moreover, to obtain similar pose estimation accuracy, our MATAL framework can save around 40% labeling efforts on average compared to state-of-the-art active learning frameworks.","2575-7075","978-1-6654-6946-3","10.1109/CVPR52688.2022.01080","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9880091","Pose estimation and tracking","Computer vision;Annotations;Computational modeling;Accelerated aging;Pose estimation;Markov processes;Benchmark testing","learning (artificial intelligence);Markov processes;pose estimation","existing pose estimation;annotated images;good estimation performance;human efforts;pose annotations;novel Meta Agent Teaming Active Learning framework;informative images;effective learning;MATAL formulates;image selection procedure;Markov Decision Process;optimal sampling policy;pose estimator;multiagent team;active learning procedure;Meta-Optimization;gradually expanded labeled data;human hand;body pose estimation;annotation budget;similar pose estimation accuracy;MATAL framework;40% labeling efforts;state-of-the-art active learning frameworks","","5","","64","IEEE","27 Sep 2022","","","IEEE","IEEE Conferences"
"Seeing in Extra Darkness Using a Deep-Red Flash","J. Xiong; J. Wang; W. Heidrich; S. Nayar",KAUST; Snap Research; KAUST; Snap Research,"2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)","2 Nov 2021","2021","","","9995","10004","We propose a new flash technique for low-light imaging, using deep-red light as an illuminating source. Our main observation is that in a dim environment, the human eye mainly uses rods for the perception of light, which are not sensitive to wavelengths longer than 620nm, yet the camera sensor still has a spectral response. We propose a novel modulation strategy when training a modern CNN model for guided image filtering, fusing a noisy RGB frame and a flash frame. This fusion network is further extended for video reconstruction. We have built a prototype with minor hardware adjustments and tested the new flash technique on a variety of static and dynamic scenes. The experimental results demonstrate that our method produces compelling reconstructions, even in extra dim conditions.","2575-7075","978-1-6654-4509-2","10.1109/CVPR46437.2021.00987","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9578549","","Training;Photography;Sensitivity;Modulation;Prototypes;Cameras;Pattern recognition","convolutional neural nets;image filtering;image fusion;image reconstruction;video signal processing;visual perception","extra darkness;deep-red flash;flash technique;low-light imaging;deep-red light;illuminating source;dim environment;human eye;camera sensor;spectral response;modulation strategy;modern CNN model;guided image filtering;noisy RGB frame;flash frame;fusion network;extra dim conditions;size 620.0 nm","","3","","44","IEEE","2 Nov 2021","","","IEEE","IEEE Conferences"
"SIGNet: Semantic Instance Aided Unsupervised 3D Geometry Perception","Y. Meng; Y. Lu; A. Raj; S. Sunarjo; R. Guo; T. Javidi; G. Bansal; D. Bharadia",UC San Diego; UC San Diego; UC San Diego; UC San Diego; Toyota InfoTechnology Center; UC San Diego; Toyota InfoTechnology Center; UC San Diego,"2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)","9 Jan 2020","2019","","","9802","9812","Unsupervised learning for geometric perception (depth, optical flow, etc.) is of great interest to autonomous systems. Recent works on unsupervised learning have made considerable progress on perceiving geometry; however, they usually ignore the coherence of objects and perform poorly under scenarios with dark and noisy environments. In contrast, supervised learning algorithms, which are robust, require large labeled geometric dataset. This paper introduces SIGNet, a novel framework that provides robust geometry perception without requiring geometrically informative labels. Specifically, SIGNet integrates semantic information to make depth and flow predictions consistent with objects and robust to low lighting conditions. SIGNet is shown to improve upon the state-of-the-art unsupervised learning for depth prediction by 30% (in squared relative error). In particular, SIGNet improves the dynamic object class performance by 39% in depth prediction and 29% in flow prediction. Our code will be made available at https://github.com/mengyuest/SIGNet","2575-7075","978-1-7281-3293-8","10.1109/CVPR.2019.01004","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8953332","3D from Single Image;Deep Learning;Motion and Tracking;Robotics + Driving;Scene Analysis and Understanding;Segmentat","Geometry;Three-dimensional displays;Semantics;Supervised learning;Thermal sensors;Prediction algorithms;Pattern recognition","image sequences;learning (artificial intelligence);unsupervised learning","robust to low lighting conditions;unsupervised learning;dynamic object class performance;flow prediction;semantic instance;geometric perception;optical flow;supervised learning algorithms;geometric dataset;geometrically informative labels;semantic information;geometry perception;SIGNet framework","","35","","54","IEEE","9 Jan 2020","","","IEEE","IEEE Conferences"
"Neuro-Inspired Eye Tracking With Eye Movement Dynamics","K. Wang; H. Su; Q. Ji",RPI; RPI and IBM; RPI,"2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)","9 Jan 2020","2019","","","9823","9832","Generalizing eye tracking to new subjects/environments remains challenging for existing appearance-based methods. To address this issue, we propose to leverage on eye movement dynamics inspired by neurological studies. Studies show that there exist several common eye movement types, independent of viewing contents and subjects, such as fixation, saccade, and smooth pursuits. Incorporating generic eye movement dynamics can therefore improve the generalization capabilities. In particular, we propose a novel Dynamic Gaze Transition Network (DGTN) to capture the underlying eye movement dynamics and serve as the topdown gaze prior. Combined with the bottom-up gaze measurements from the deep convolutional neural network, our method achieves better performance for both within-dataset and cross-dataset evaluations compared to state-of-the-art. In addition, a new DynamicGaze dataset is also constructed to study eye movement dynamics and eye gaze estimation.","2575-7075","978-1-7281-3293-8","10.1109/CVPR.2019.01006","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8953735","Face;Gesture;and Body Pose","","convolutional neural nets;gaze tracking;user interfaces","neuro-inspired eye tracking;appearance-based methods;eye gaze estimation;generic eye movement dynamics;dynamic gaze transition network;eye tracking generalization;DGTN;bottom-up gaze measurements;deep convolutional neural network","","12","","55","IEEE","9 Jan 2020","","","IEEE","IEEE Conferences"
"Neural Cellular Automata Manifold","A. H. Ruiz; A. Vilalta; F. Moreno-Noguer","Institut de Robòtica i Informàtica Industrial, CSIC-UPC, Spain; Barcelona Supercomputing Center, Spain; Institut de Robòtica i Informàtica Industrial, CSIC-UPC, Spain","2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)","2 Nov 2021","2021","","","10015","10023","Very recently, the Neural Cellular Automata (NCA) has been proposed to simulate the morphogenesis process with deep networks. NCA learns to grow an image starting from a fixed single pixel. In this work, we show that the neural network (NN) architecture of the NCA can be encapsulated in a larger NN. This allows us to propose a new model that encodes a manifold of NCA, each of them capable of generating a distinct image. Therefore, we are effectively learning an embedding space of CA, which shows generalization capabilities. We accomplish this by introducing dynamic convolutions inside an Auto-Encoder architecture, for the first time used to join two different sources of information, the encoding and cell’s environment information. In biological terms, our approach would play the role of the transcription factors, modulating the mapping of genes into specific proteins that drive cellular differentiation, which occurs right before the morphogenesis. We thoroughly evaluate our approach in a dataset of synthetic emojis and also in real images of CIFAR-10. Our model introduces a general-purpose network, which can be used in a broad range of problems beyond image generation.","2575-7075","978-1-6654-4509-2","10.1109/CVPR46437.2021.00989","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9578885","","Manifolds;Training;Adaptation models;Evolution (biology);Learning automata;Biological system modeling;Artificial neural networks","cellular automata;convolutional neural nets;deep learning (artificial intelligence);image colour analysis;neural net architecture","image generation;neural cellular automata;NCA;morphogenesis process;deep networks;image pixel;neural network architecture;generalization capabilities;dynamic convolutions;autoencoder architecture;cellular differentiation;RGB images","","1","","30","IEEE","2 Nov 2021","","","IEEE","IEEE Conferences"
"Dense Distinct Query for End-to-End Object Detection","S. Zhang; X. Wang; J. Wang; J. Pang; C. Lyu; W. Zhang; P. Luo; K. Chen",Shanghai AI Laboratory; SenseTime Research; Shanghai AI Laboratory; Shanghai AI Laboratory; Shanghai AI Laboratory; Shanghai AI Laboratory; The University of Hong Kong; Shanghai AI Laboratory,"2023 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)","22 Aug 2023","2023","","","7329","7338","One-to-one label assignment in object detection has successfully obviated the need for non-maximum suppression (NMS) as postprocessing and makes the pipeline end-to-end. However, it triggers a new dilemma as the widely used sparse queries cannot guarantee a high recall, while dense queries inevitably bring more similar queries and encounter optimization difficulties. As both sparse and dense queries are problematic, then what are the expected queries in end-to-end object detection? This paper shows that the solution should be Dense Distinct Queries (DDQ). Concretely, we first lay dense queries like traditional detectors and then select distinct ones for one-to-one assignments. DDQ blends the advantages of traditional and recent end-to-end detectors and significantly improves the performance of various detectors including FCN, R-CNN, and DETRs. Most impressively, DDQ-DETR achieves 52.1 AP on MS-COCO dataset within 12 epochs using a ResNet-50 backbone, outperforming all existing detectors in the same setting. DDQ also shares the benefit of end-to-end detectors in crowded scenes and achieves 93.8 AP on Crowd-Human. We hope DDQ can inspire researchers to consider the complementarity between traditional methods and end-to-end detectors. The source code can be found at https://github.com/jshilong/DDQ.","2575-7075","979-8-3503-0129-8","10.1109/CVPR52729.2023.00708","National Key R&D Program of China(grant numbers:2022ZD0161600,2022ZD0161000); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10203166","Recognition: Categorization;detection;retrieval","Computer vision;Source coding;Pipelines;Detectors;Object detection;Pattern recognition;Optimization","convolutional neural nets;deep learning (artificial intelligence);feature extraction;object detection;pipelines","DDQ;Dense Distinct Queries;Dense Distinct query;dense queries;end-to-end object detection;expected queries;pipeline end-to-end;recent end-to-end detectors;similar queries;traditional end-to-end detectors;widely used sparse queries","","2","","38","IEEE","22 Aug 2023","","","IEEE","IEEE Conferences"
"Real-time Object Detection for Streaming Perception","J. Yang; S. Liu; Z. Li; X. Li; J. Sun",Huazhong University of Science and Technology; Megvii Technology; Megvii Technology; Huazhong University of Science and Technology; Megvii Technology,"2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)","27 Sep 2022","2022","","","5375","5385","Autonomous driving requires the model to perceive the environment and (re)act within a low latency for safety. While past works ignore the inevitable changes in the environment after processing, streaming perception is proposed to jointly evaluate the latency and accuracy into a single metric for video online perception. In this paper, instead of searching trade-offs between accuracy and speed like previous works, we point out that endowing real-time models with the ability to predict the future is the key to dealing with this problem. We build a simple and effective frame-work for streaming perception. It equips a novel Dual-Flow Perception module (DFP), which includes dynamic and static flows to capture the moving trend and basic detection feature for streaming prediction. Further, we introduce a Trend-Aware Loss (TAL) combined with a trend factor to generate adaptive weights for objects with different moving speeds. Our simple method achieves competitive performance on Argoverse-HD dataset and improves the AP by 4.9% compared to the strong baseline, validating its effectiveness. Our code will be made available at https://github.com/yancie-yjr/StreamYOLO.","2575-7075","978-1-6654-6946-3","10.1109/CVPR52688.2022.00531","Ministry of Science and Technology(grant numbers:2020AAA0104400); China Postdoctoral Science Foundation(grant numbers:2021M690375); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9879790","Recognition: detection;categorization;retrieval; Navigation and autonomous driving","Measurement;Detectors;Streaming media;Market research;Search problems;Real-time systems;Pattern recognition","computer vision;feature extraction;mobile robots;object detection","streaming perception;video online perception;endowing real-time models;simple frame-work;effective frame-work;novel Dual-Flow Perception module;basic detection feature;streaming prediction;time object detection;autonomous driving","","13","","62","IEEE","27 Sep 2022","","","IEEE","IEEE Conferences"
"FlowNet3D: Learning Scene Flow in 3D Point Clouds","X. Liu; C. R. Qi; L. J. Guibas",Stanford University; Facebook AI Research; Stanford University,"2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)","9 Jan 2020","2019","","","529","537","Many applications in robotics and human-computer interaction can benefit from understanding 3D motion of points in a dynamic environment, widely noted as scene flow. While most previous methods focus on stereo and RGB-D images as input, few try to estimate scene flow directly from point clouds. In this work, we propose a novel deep neural network named FlowNet3D that learns scene flow from point clouds in an end-to-end fashion. Our network simultaneously learns deep hierarchical features of point clouds and flow embeddings that represent point motions, supported by two newly proposed learning layers for point sets. We evaluate the network on both challenging synthetic data from FlyingThings3D and real Lidar scans from KITTI. Trained on synthetic data only, our network successfully generalizes to real scans, outperforming various baselines and showing competitive results to the prior art. We also demonstrate two applications of our scene flow output (scan registration and motion segmentation) to show its potential wide use cases.","2575-7075","978-1-7281-3293-8","10.1109/CVPR.2019.00062","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8953876","Deep Learning;Low-level Vision;Motion and Tracking;Robotics + Driving","Point cloud compression;Deep learning;Human computer interaction;Computer vision;Three-dimensional displays;Laser radar;Art","feature extraction;image motion analysis;image registration;image representation;image segmentation;image sequences;learning (artificial intelligence);neural nets;stereo image processing","FlowNet3D;scene flow learning;3D point clouds;flow embeddings;scene flow output;deep neural network;point motions representation;deep hierarchical features;FlyingThings3D;Lidar scans;scan registration;motion segmentation","","224","","34","IEEE","9 Jan 2020","","","IEEE","IEEE Conferences"
"Self-Supervised Pillar Motion Learning for Autonomous Driving","C. Luo; X. Yang; A. Yuille",QCraft; QCraft; Johns Hopkins University,"2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)","2 Nov 2021","2021","","","3182","3191","Autonomous driving can benefit from motion behavior comprehension when interacting with diverse traffic participants in highly dynamic environments. Recently, there has been a growing interest in estimating class-agnostic motion directly from point clouds. Current motion estimation methods usually require vast amount of annotated training data from self-driving scenes. However, manually labeling point clouds is notoriously difficult, error-prone and time-consuming. In this paper, we seek to answer the research question of whether the abundant unlabeled data collections can be utilized for accurate and efficient motion learning. To this end, we propose a learning framework that leverages free supervisory signals from point clouds and paired camera images to estimate motion purely via self-supervision. Our model involves a point cloud based structural consistency augmented with probabilistic motion masking as well as a cross-sensor motion regularization to realize the desired self-supervision. Experiments reveal that our approach performs competitively to supervised methods, and achieves the state-of-the-art result when combining our self-supervised model with supervised fine-tuning.","2575-7075","978-1-6654-4509-2","10.1109/CVPR46437.2021.00320","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9578855","","Computer vision;Motion estimation;Dynamics;Training data;Data collection;Probabilistic logic;Cameras","learning (artificial intelligence);motion estimation;object detection;traffic engineering computing","cross-sensor motion regularization;supervised fine-tuning;self-supervised pillar motion;autonomous driving;motion behavior comprehension;diverse traffic participants;point cloud;motion estimation;annotated training data;self-driving scenes;abundant unlabeled data collections;motion learning;learning framework;probabilistic motion masking","","22","","45","IEEE","2 Nov 2021","","","IEEE","IEEE Conferences"
"Class-Incremental Exemplar Compression for Class-Incremental Learning","Z. Luo; Y. Liu; B. Schiele; Q. Sun","Singapore Management University; Max Planck Institute for Informatics, Saarland Informatics Campus; Max Planck Institute for Informatics, Saarland Informatics Campus; Singapore Management University","2023 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)","22 Aug 2023","2023","","","11371","11380","Exemplar-based class-incremental learning (CIL) [36] finetunes the model with all samples of new classes but few-shot exemplars of old classes in each incremental phase, where the “few-shot” abides by the limited memory budget. In this paper, we break this “few-shot” limit based on a simple yet surprisingly effective idea: compressing exemplars by downsampling non-discriminative pixels and saving “many-shot” compressed exemplars in the memory. Without needing any manual annotation, we achieve this compression by generating 0–1 masks on discriminative pixels from class activation maps (CAM) [49]. We propose an adaptive mask generation model called class-incremental masking (CIM) to explicitly resolve two difficulties of using CAM: 1) transforming the heatmaps of CAM to 0–1 masks with an arbitrary threshold leads to a trade-off between the coverage on discriminative pixels and the quantity of exemplars, as the total memory is fixed; and 2) optimal thresholds vary for different object classes, which is particularly obvious in the dynamic environment of CIL. We optimize the CIM model alternatively with the conventional CIL model through a bilevel optimization problem [40]. We conduct extensive experiments on high-resolution CIL benchmarks including Food-101, ImageNet-100, and ImageNet-1000, and show that using the compressed exemplars by CIM can achieve a new state-of-the-art CIL accuracy, e.g., 4.8 percentage points higher than FOSTER [42] on 10-Phase ImageNet-1000. Our code is available at https://github.com/xfflzlICIM-CIL.","2575-7075","979-8-3503-0129-8","10.1109/CVPR52729.2023.01094","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10203819","Transfer;meta;low-shot;continual;or long-tail learning","Heating systems;Adaptation models;Computer vision;Image coding;Codes;Computational modeling;Manuals","data compression;image classification;image sampling;learning (artificial intelligence);optimisation;storage management","adaptive mask generation model;bilevel optimization problem;CAM;CIM model;class activation maps;class-incremental exemplar compression;class-incremental masking;conventional CIL model;discriminative pixels;exemplar-based class-incremental learning;few-shot exemplars;Food-101;ImageNet-1000;many-shot compressed exemplars;memory budget;nondiscriminative pixel downsampling;object class","","1","","50","IEEE","22 Aug 2023","","","IEEE","IEEE Conferences"
"Weakly Supervised Class-agnostic Motion Prediction for Autonomous Driving","R. Li; H. Shi; Z. Fu; Z. Wang; G. Lin","S-Lab, Nanyang Technological University; School of Computer Science and Engineering, Nanyang Technological University; SenseTime Research; S-Lab, Nanyang Technological University; S-Lab, Nanyang Technological University","2023 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)","22 Aug 2023","2023","","","17599","17608","Understanding the motion behavior of dynamic environments is vital for autonomous driving, leading to increasing attention in class-agnostic motion prediction in LiDAR point clouds. Outdoor scenes can often be decomposed into mobile foregrounds and static backgrounds, which enables us to associate motion understanding with scene parsing. Based on this observation, we study a novel weakly supervised motion prediction paradigm, where fully or partially (1 %, 0.1%) annotated foreground/background binary masks are used for supervision, rather than using expensive motion annotations. To this end, we propose a two-stage weakly supervised approach, where the segmentation model trained with the incomplete binary masks in Stage1 will facilitate the self-supervised learning of the motion prediction network in Stage2 by estimating possible moving foregrounds in advance. Furthermore, for robust self-supervised motion learning, we design a Consistency-aware Chamfer Distance loss by exploiting multi-frame information and explicitly suppressing potential outliers. Comprehensive experiments show that, with fully or partially binary masks as supervision, our weakly supervised models surpass the self-supervised models by a large margin and perform on par with some supervised ones. This further demonstrates that our approach achieves a good compromise between annotation effort and performance.","2575-7075","979-8-3503-0129-8","10.1109/CVPR52729.2023.01688","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10205104","Video: Low-level analysis;motion;and tracking","Point cloud compression;Computer vision;Laser radar;Annotations;Motion segmentation;Dynamics;Self-supervised learning","image motion analysis;image segmentation;learning (artificial intelligence);object detection;optical radar;supervised learning;unsupervised learning","autonomous driving;expensive motion annotations;fully masks;incomplete binary masks;LiDAR point clouds;mobile foregrounds;motion behavior;motion prediction network;motion prediction paradigm;outdoor scenes;partially binary masks;possible moving foregrounds;scene parsing;self-supervised learning;self-supervised models;self-supervised motion learning;static backgrounds;supervised ones;two-stage weakly supervised approach;weakly supervised class-agnostic motion prediction;weakly supervised models","","","","47","IEEE","22 Aug 2023","","","IEEE","IEEE Conferences"
"Event-Guided Person Re-Identification via Sparse-Dense Complementary Learning","C. Cao; X. Fu; H. Liu; Y. Huang; K. Wang; J. Luo; Z. -J. Zha","University of Science and Technology of China, China; University of Science and Technology of China, China; University of Science and Technology of China, China; University of Science and Technology of China, China; University of Science and Technology of China, China; University of Rochester, USA; University of Science and Technology of China, China","2023 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)","22 Aug 2023","2023","","","17990","17999","Video-based person reidentification (Re-ID) is a prominent computer vision topic due to its wide range of video surveillance applications. Most existing methods utilize spatial and temporal correlations in frame sequences to obtain discriminative person features. However, inevitable degradation, e.g., motion blur contained in frames, leading to the loss of identity-discriminating cues. Recently, a new bio-inspired sensor called event camera, which can asynchronously record intensity changes, brings new vitality to the Re-ID task. With the microsecond resolution and low latency, it can accurately capture the movements of pedestrians even in the degraded environments. In this work, we propose a Sparse-Dense Complementary Learning (SDCL) Framework, which effectively extracts identity features by fully exploiting the complementary information of dense frames and sparse events. Specifically, for frames, we build a CNN-based module to aggregate the dense features of pedestrian appearance step by step, while for event streams, we design a bio-inspired spiking neural network (SNN) backbone, which encodes event signals into sparse feature maps in a spiking form, to extract the dynamic motion cues of pedestrians. Finally, a cross feature alignment module is constructed to fuse motion information from events and appearance cues from frames to enhance identity representation learning. Experiments on several benchmarks show that by employing events and SNN into Re-ID, our method significantly outperforms competitive methods. The code is available at https://github.com/Chengzhi-Cao/SDCL.","2575-7075","979-8-3503-0129-8","10.1109/CVPR52729.2023.01725","National Key R&D Program of China(grant numbers:2020AAA0105702); National Natural Science Foundation of China (NSFC)(grant numbers:62225207,62276243,U19B2038); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10203615","Recognition: Categorization;detection;retrieval","Representation learning;Computer vision;Pedestrians;Neural networks;Benchmark testing;Feature extraction;Video surveillance","computer vision;convolutional neural nets;feature extraction;image motion analysis;image representation;learning (artificial intelligence);object detection;pedestrians;video signal processing;video surveillance","appearance cues;bio-inspired spiking neural network backbone;bioinspired sensor;CNN-based module;complementary information;computer vision;cross feature alignment module;dense features;discriminative person features;event camera;event-guided person re-identification;frame sequences;identity feature extraction;identity representation learning;motion information;pedestrian movements;Re-ID;sparse feature maps;sparse-dense complementary learning;spatial correlations;temporal correlations;video surveillance;video-based person reidentification","","","","61","IEEE","22 Aug 2023","","","IEEE","IEEE Conferences"
"OTA: Optimal Transport Assignment for Object Detection","Z. Ge; S. Liu; Z. Li; O. Yoshie; J. Sun",Waseda University; Megvii Technology; Megvii Technology; Waseda University; Megvii Technology,"2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)","2 Nov 2021","2021","","","303","312","Recent advances in label assignment in object detection mainly seek to independently define positive/negative training samples for each ground-truth (gt) object. In this paper, we innovatively revisit the label assignment from a global perspective and propose to formulate the assigning procedure as an Optimal Transport (OT) problem – a well-studied topic in Optimization Theory. Concretely, we define the unit transportation cost between each demander (anchor) and supplier (gt) pair as the weighted summation of their classification and regression losses. After formulation, finding the best assignment solution is converted to solve the optimal transport plan at minimal transportation costs, which can be solved via Sinkhorn-Knopp Iteration. On COCO, a single FCOS-ResNet-50 detector equipped with Optimal Transport Assignment (OTA) can reach 40.7% mAP under 1× scheduler, outperforming all other existing assigning methods. Extensive experiments conducted on COCO and CrowdHuman further validate the effectiveness of our proposed OTA, especially its superiority in crowd scenarios. The code is available at https://github.com/Megvii-BaseDetection/OTA.","2575-7075","978-1-6654-4509-2","10.1109/CVPR46437.2021.00037","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9577660","","Training;Computer vision;Costs;Codes;Transportation;Estimation;Object detection","object detection;optimisation;transportation","regression losses;transportation costs;existing assigning methods;object detection;label assignment;ground-truth object;global perspective;optimal transport problem;optimization theory;unit transportation cost;classification losses;OTA;optimal transport assignment;FCOS-ResNet-50 detector;Sinkhorn-Knopp iteration;COCO;CrowdHuman","","135","","54","IEEE","2 Nov 2021","","","IEEE","IEEE Conferences"
"Diverse Part Discovery: Occluded Person Re-identification with Part-Aware Transformer","Y. Li; J. He; T. Zhang; X. Liu; Y. Zhang; F. Wu",University of Science and Technology of China; University of Science and Technology of China; University of Science and Technology of China; Dongguan University of Technology; University of Science and Technology of China; University of Science and Technology of China,"2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)","2 Nov 2021","2021","","","2897","2906","Occluded person re-identification (Re-ID) is a challenging task as persons are frequently occluded by various obstacles or other persons, especially in the crowd scenario. To address these issues, we propose a novel end-to-end Part-Aware Transformer (PAT) for occluded person Re-ID through diverse part discovery via a transformer encoder-decoder architecture, including a pixel context based transformer encoder and a part prototype based transformer decoder. The proposed PAT model enjoys several merits. First, to the best of our knowledge, this is the first work to exploit the transformer encoder-decoder architecture for occluded person Re-ID in a unified deep model. Second, to learn part prototypes well with only identity labels, we design two effective mechanisms including part diversity and part discriminability. Consequently, we can achieve diverse part discovery for occluded person Re-ID in a weakly supervised manner. Extensive experimental results on six challenging benchmarks for three tasks (occluded, partial and holistic Re-ID) demonstrate that our proposed PAT performs favor-ably against stat-of-the-art methods.","2575-7075","978-1-6654-4509-2","10.1109/CVPR46437.2021.00292","Research and Development; Chinese Academy of Sciences; Nature; National Laboratory of Pattern Recognition; Youth Innovation Promotion Association; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9578649","","Computer vision;Prototypes;Computer architecture;Benchmark testing;Transformers;Pattern recognition;Decoding","codecs;image coding;image recognition;supervised learning","diverse part discovery;occluded person re-identification;end-to-end Part-Aware Transformer;occluded person Re-ID;transformer encoder-decoder architecture;part prototype based transformer decoder;part diversity;part discriminability","","116","","62","IEEE","2 Nov 2021","","","IEEE","IEEE Conferences"
"Improving Multiple Pedestrian Tracking by Track Management and Occlusion Handling","D. Stadler; J. Beyerer",Karlsruhe Institute of Technology; Fraunhofer IOSB,"2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)","2 Nov 2021","2021","","","10953","10962","Multi-pedestrian trackers perform well when targets are clearly visible making the association task quite easy. However, when heavy occlusions are present, a mechanism to re-identify persons is needed. The common approach is to extract visual features from new detections and compare them with the features of previously found tracks. Since those detections can have substantial overlaps with nearby targets – especially in crowded scenarios – the extracted features are insufficient for a reliable re-identification. In contrast, we propose a novel occlusion handling strategy that explicitly models the relation between occluding and occluded tracks outperforming the feature-based approach, while not depending on a separate re-identification network. Furthermore, we improve the track management of a regression-based method in order to bypass missing detections and to deal with tracks leaving the scene at the border of the image. Finally, we apply our tracker in both temporal directions and merge tracklets belonging to the same target, which further enhances the performance. We demonstrate the effectiveness of our tracking components with ablative experiments and surpass the state-of-the-art methods on the three popular pedestrian tracking benchmarks MOT16, MOT17, and MOT20.","2575-7075","978-1-6654-4509-2","10.1109/CVPR46437.2021.01081","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9578417","","Visualization;Computer vision;Target tracking;Benchmark testing;Feature extraction;Pattern recognition;Reliability","feature extraction;object detection;object tracking;pedestrians;regression analysis;traffic engineering computing","feature-based approach;track management;regression-based method;multiple pedestrian tracking;multipedestrian trackers;occlusion handling strategy;MOT16 benchmark;MOT17 benchmark;MOT20 benchmark","","57","","40","IEEE","2 Nov 2021","","","IEEE","IEEE Conferences"
"An Aggregated Multicolumn Dilated Convolution Network for Perspective-Free Counting","D. Deb; J. Ventura","Georgia Institute of Technology; University of Colorado, Colorado Springs","2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)","16 Dec 2018","2018","","","308","30809","We propose the use of dilated filters to construct an aggregation module in a multicolumn convolutional neural network for perspective-free counting. Counting is a common problem in computer vision (e.g. traffic on the street or pedestrians in a crowd). Modern approaches to the counting problem involve the production of a density map via regression whose integral is equal to the number of objects in the image. However, objects in the image can occur at different scales (e.g. due to perspective effects) which can make it difficult for a learning agent to learn the proper density map. While the use of multiple columns to extract multiscale information from images has been shown before, our approach aggregates the multiscale information gathered by the multicolumn convolutional neural network to improve performance. Our experiments show that our proposed network outperforms the state-of-the-art on many benchmark datasets, and also that using our aggregation module in combination with a higher number of columns is beneficial for multiscale counting.","2160-7516","978-1-5386-6100-0","10.1109/CVPRW.2018.00057","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8575518","","Convolution;Feature extraction;Training;Aggregates;Kernel;Data mining;Convolutional neural networks","computer vision;feature extraction;learning (artificial intelligence);neural nets;object detection","aggregated multicolumn dilated convolution network;perspective-free counting;dilated filters;aggregation module;multicolumn convolutional neural network;common problem;computer vision;modern approaches;multiscale information;approach aggregates;multiscale counting","","44","","28","IEEE","16 Dec 2018","","","IEEE","IEEE Conferences"
"Learning a Facial Expression Embedding Disentangled from Identity","W. Zhang; X. Ji; K. Chen; Y. Ding; C. Fan","Virtual Human Group, Netease Fuxi AI Lab; Virtual Human Group, Netease Fuxi AI Lab; University of Science and Technology of China; Virtual Human Group, Netease Fuxi AI Lab; Virtual Human Group, Netease Fuxi AI Lab","2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)","2 Nov 2021","2021","","","6755","6764","The facial expression analysis requires a compact and identity-ignored expression representation. In this paper, we model the expression as the deviation from the identity by a subtraction operation, extracting a continuous and identity-invariant expression embedding. We propose a Deviation Learning Network (DLN) with a pseudo-siamese structure to extract the deviation feature vector. To reduce the optimization difficulty caused by additional fully connection layers, DLN directly provides high-order polynomial to nonlinearly project the high-dimensional feature to a low-dimensional manifold. Taking label noise into account, we add a crowd layer to DLN for robust embedding extraction. Also, to achieve a more compact representation, we use hierarchical annotation for data augmentation. We evaluate our facial expression embedding on the FEC validation set. The quantitative results prove that we achieve the state-of-the-art, both in terms of fine-grained and identity-invariant property. We further conduct extensive experiments to show that our expression embedding is of high quality for expression recognition, image retrieval, and face manipulation.","2575-7075","978-1-6654-4509-2","10.1109/CVPR46437.2021.00669","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9577420","","Manifolds;Computer vision;Image recognition;Head;Annotations;Face recognition;Image retrieval","emotion recognition;face recognition;feature extraction;image retrieval;learning (artificial intelligence)","facial expression analysis;continuous identity-invariant expression embedding;DLN;pseudosiamese structure;deviation feature vector;additional fully connection layers;high-order polynomial;high-dimensional feature;low-dimensional manifold;robust embedding extraction;compact representation;facial expression embedding;identity-invariant property;expression recognition","","26","","42","IEEE","2 Nov 2021","","","IEEE","IEEE Conferences"
"Learning Cross-Modal Retrieval with Noisy Labels","P. Hu; X. Peng; H. Zhu; L. Zhen; J. Lin","College of Computer Science, Sichuan University, Chengdu, China; College of Computer Science, Sichuan University, Chengdu, China; Institute for Infocomm Research, Agency for Science, Technology and Research, Singapore; Institute of High Performance Computing, Agency for Science, Technology and Research, Singapore; Institute for Infocomm Research, Agency for Science, Technology and Research, Singapore","2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)","2 Nov 2021","2021","","","5399","5409","Recently, cross-modal retrieval is emerging with the help of deep multimodal learning. However, even for unimodal data, collecting large-scale well-annotated data is expensive and time-consuming, and not to mention the additional challenges from multiple modalities. Although crowd-sourcing annotation, e.g., Amazon’s Mechanical Turk, can be utilized to mitigate the labeling cost, but leading to the unavoidable noise in labels for the non-expert annotating. To tackle the challenge, this paper presents a general Multi-modal Robust Learning framework (MRL) for learning with multimodal noisy labels to mitigate noisy samples and correlate distinct modalities simultaneously. To be specific, we propose a Robust Clustering loss (RC) to make the deep networks focus on clean samples instead of noisy ones. Besides, a simple yet effective multimodal loss function, called Multimodal Contrastive loss (MC), is proposed to maxi-mize the mutual information between different modalities, thus alleviating the interference of noisy samples and cross-modal discrepancy. Extensive experiments are conducted on four widely-used multimodal datasets to demonstrate the effectiveness of the proposed approach by comparing to 14 state-of-the-art methods.","2575-7075","978-1-6654-4509-2","10.1109/CVPR46437.2021.00536","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9577762","","Computer vision;Costs;Annotations;Interference;Pattern recognition;Noise measurement;Labeling","deep learning (artificial intelligence);information retrieval;pattern clustering","noisy samples;cross-modal discrepancy;multimodal datasets;cross-modal retrieval;deep multimodal learning;unimodal data;multiple modalities;Amazon Mechanical Turk;labeling cost;nonexpert annotating;multimodal noisy labels;correlate distinct modalities;deep networks focus;noisy ones;multimodal loss function;multimodal contrastive loss;robust clustering loss;general multimodal robust learning framework","","26","","63","IEEE","2 Nov 2021","","","IEEE","IEEE Conferences"
"Rethinking Spatial Invariance of Convolutional Networks for Object Counting","Z. -Q. Cheng; Q. Dai; H. Li; J. Song; X. Wu; A. G. Hauptmann",Carnegie Mellon University; Microsoft Research Asia; Southwest Jiaotong University; University of Electronic Science and Technology of China; Southwest Jiaotong University; Carnegie Mellon University,"2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)","27 Sep 2022","2022","","","19606","19616","Previous work generally believes that improving the spatial invariance of convolutional networks is the key to object counting. However, after verifying several mainstream counting networks, we surprisingly found too strict pixel-level spatial invariance would cause overfit noise in the density map generation. In this paper, we try to use locally connected Gaussian kernels to replace the original convolution filter to estimate the spatial position in the density map. The purpose of this is to allow the feature extraction process to potentially stimulate the density map generation process to overcome the annotation noise. Inspired by previous work, we propose a low-rank approximation accompanied with translation invariance to favorably implement the approximation of massive Gaussian convolution. Our work points a new direction for follow-up research, which should investigate how to properly relax the overly strict pixel-level spatial invariance for object counting. We evaluate our methods on 4 mainstream object counting networks (i.e., MCNN, CSRNet, SANet, and ResNet-50). Extensive experiments were conducted on 7 popular benchmarks for 3 applications (i.e., crowd, vehicle, and plant counting). Experimental results show that our methods significantly outperform other state-of-the-art methods and achieve promising learning of the spatial position of objects11Code is at https://github.com/zhiqic/Rethinking-Counting.","2575-7075","978-1-6654-6946-3","10.1109/CVPR52688.2022.01902","Air Force Research Laboratory(grant numbers:FA8750-19-2-0200,60NANB17D156); U.S. Department of Commerce; National Institute of Standards and Technology (NIST); Intelligence Advanced Research Projects Activity (IARPA); Defense Advanced Research Projects Agency (DARPA); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9879478","Scene analysis and understanding; Biometrics; Datasets and evaluation; Transfer/low-shot/long-tail learning; Vision applications and systems","Computer vision;Convolution;Annotations;Benchmark testing;Feature extraction;Pattern recognition;Kernel","approximation theory;convolutional neural nets;feature extraction;Gaussian processes;learning (artificial intelligence)","convolutional networks;mainstream counting networks;overfit noise;locally connected Gaussian kernels;convolution filter;spatial position;feature extraction process;density map generation process;annotation noise;low-rank approximation;translation invariance;massive Gaussian convolution;plant counting;object counting networks;strict pixel-level spatial invariance","","13","","82","IEEE","27 Sep 2022","","","IEEE","IEEE Conferences"
"Occluded Human Mesh Recovery","R. Khirodkar; S. Tripathi; K. Kitani","Carnegie Mellon University; Max Planck Institute for Intelligent Systems, Tübingen; Carnegie Mellon University","2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)","27 Sep 2022","2022","","","1705","1715","Top-down methods for monocular human mesh recovery have two stages: (1) detect human bounding boxes; (2) treat each bounding box as an independent single-human mesh recovery task. Unfortunately, the single-human assumption does not hold in images with multi-human occlusion and crowding. Consequently, top-down methods have difficulties in recovering accurate 3D human meshes under severe person-person occlusion. To address this, we present Occluded Human Mesh Recovery (OCHMR) - a novel top-down mesh recovery approach that incorporates image spatial context to overcome the limitations of the single-human assumption. The approach is conceptually simple and can be applied to any existing top-down architecture. Along with the input image, we condition the top-down model on spatial context from the image in the form of body-center heatmaps. To reason from the predicted body centermaps, we introduce Contextual Normalization (CoNorm) blocks to adaptively modulate intermediate features of the top-down model. The contextual conditioning helps our model disambiguate between two severely overlapping human boundingboxes, making it robust to multi-person occlusion. Compared with state-of-the-art methods, OCHMR achieves superior performance on challenging multi-person benchmarks like 3DPW, CrowdPose and OCHuman. Specifically, our proposed contextual reasoning architecture applied to the SPIN model with ResNet-50 backbone results in 75.2 PMPJPE on 3DPW-PC, 23.6 AP on CrowdPose and 37.7 AP on OCHu- man datasets, a significant improvement of 6.9 mm, 6.4 AP and 20.8 AP respectively over the baseline.","2575-7075","978-1-6654-6946-3","10.1109/CVPR52688.2022.00176","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9879237","3D from single images; Deep learning architectures and techniques; Machine learning; Pose estimation and tracking; Scene analysis and understanding; Vision applications and systems","Heating systems;Adaptation models;Computer vision;Three-dimensional displays;Computer architecture;Predictive models;Pattern recognition","feature extraction;image restoration;inference mechanisms;pose estimation;stereo image processing","multiperson occlusion;Occluded human mesh recovery;monocular human mesh recovery;human bounding boxes;image spatial context;OCHMR;CoNorm blocks;contextual reasoning architecture;SPIN model;ResNet-50 backbone;CrowdPose;body-center heatmaps","","12","","73","IEEE","27 Sep 2022","","","IEEE","IEEE Conferences"
"Variational Pedestrian Detection","Y. Zhang; H. He; J. Li; Y. Li; J. See; W. Lin","Shanghai Jiao Tong University, China; Shanghai Jiao Tong University, China; Ant Group; Shanghai Jiao Tong University, China; Heriot-Watt University, Malaysia; Shanghai Jiao Tong University, China","2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)","2 Nov 2021","2021","","","11617","11626","Pedestrian detection in a crowd is a challenging task due to a high number of mutually-occluding human instances, which brings ambiguity and optimization difficulties to the current IoU-based ground truth assignment procedure in classical object detection methods. In this paper, we develop a unique perspective of pedestrian detection as a variational inference problem. We formulate a novel and efficient algorithm for pedestrian detection by modeling the dense proposals as a latent variable while proposing a customized Auto-Encoding Variational Bayes (AEVB) algorithm. Through the optimization of our proposed algorithm, a classical detector can be fashioned into a variational pedestrian detector. Experiments conducted on CrowdHuman and CityPersons datasets show that the proposed algorithm serves as an efficient solution to handle the dense pedestrian detection problem for the case of single-stage detectors. Our method can also be flexibly applied to two-stage detectors, achieving notable performance enhancement.","2575-7075","978-1-6654-4509-2","10.1109/CVPR46437.2021.01145","National Natural Science Foundation of China; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9578255","","Computer vision;Computational modeling;Detectors;Object detection;Propulsion;Inference algorithms;Pattern recognition","Bayes methods;inference mechanisms;object detection;pedestrians","object detection;variational inference problem;variational pedestrian detector;dense pedestrian detection;variational pedestrian detection;IoU-based ground truth assignment;auto-encoding variational Bayes algorithm;AEVB algorithm;CrowdHuman datasets;CityPersons datasets","","11","","45","IEEE","2 Nov 2021","","","IEEE","IEEE Conferences"
"Behavior and Personality Analysis in a Nonsocial Context Dataset","D. Dotti; M. Popa; S. Asteriadis","Department of Data Science and Knowledge Engineering, Maastricht University, Maastricht, The Netherlands; Department of Data Science and Knowledge Engineering, Maastricht University, Maastricht, The Netherlands; Department of Data Science and Knowledge Engineering, Maastricht University, Maastricht, The Netherlands","2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)","16 Dec 2018","2018","","","2417","24178","Personality recognition using nonverbal behavioral cues is a challenging task in the Affective Computing field. The majority of existing methods investigate personality assessment in social contexts, such as crowded places or social events, but ignore the role of behaviors as well as personality in nonsocial situations (i.e. during individual activities). In this paper we introduce a novel dataset for behavior understanding and personality recognition in a nonsocial context. Forty-six participants were recorded in an unconstrained indoor space, related to a smart home environment, performing six tasks resembling Activities of Daily Living (ADL). During the experiment, personality scores were collected using self-assessment questionnaires. Furthermore, a temporal framework using a Long-Short Term Memory (LSTM) network is proposed to map nonverbal behavioral features to participants' personality labels. Our experiments showed that nonverbal behaviors are important predictors of personality, confirming theories from the personality psychology field.","2160-7516","978-1-5386-6100-0","10.1109/CVPRW.2018.00312","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8575484","","Skeleton;Task analysis;Psychology;Feature extraction;Problem-solving;Computational modeling;Computer vision","behavioural sciences computing;emotion recognition;psychology;recurrent neural nets","personality recognition;personality scores;self-assessment questionnaires;map nonverbal behavioral features;personality psychology field;nonsocial context dataset;Affective Computing field;personality assessment;social contexts;behavior understanding;personality analysis;behavior analysis;long-short term memory network;LSTM network","","9","","30","IEEE","16 Dec 2018","","","IEEE","IEEE Conferences"
"ElePose: Unsupervised 3D Human Pose Estimation by Predicting Camera Elevation and Learning Normalizing Flows on 2D Poses","B. Wandt; J. J. Little; H. Rhodin",University of British Columbia; University of British Columbia; University of British Columbia,"2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)","27 Sep 2022","2022","","","6625","6635","Human pose estimation from single images is a challenging problem that is typically solved by supervised learning. Unfortunately, labeled training data does not yet exist for many human activities since 3D annotation requires dedicated motion capture systems. Therefore, we propose an unsupervised approach that learns to predict a 3D human pose from a single image while only being trained with 2D pose data, which can be crowd-sourced and is already widely available. To this end, we estimate the 3D pose that is most likely over random projections, with the likelihood estimated using normalizing flows on 2D poses. While previous work requires strong priors on camera rotations in the training data set, we learn the distribution of camera angles which significantly improves the performance. Another part of our contribution is to stabilize training with normalizing flows on high-dimensional 3D pose data by first projecting the 2D poses to a linear subspace. We outperform the state-of-the-art unsupervised human pose estimation methods on the benchmark datasets Human3.6M and MPI-INF-3DHP in many metrics.","2575-7075","978-1-6654-6946-3","10.1109/CVPR52688.2022.00652","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9879908","Pose estimation and tracking; 3D from single images; Self-& semi-& meta- & unsupervised learning","Training;Three-dimensional displays;Annotations;Pose estimation;Supervised learning;Training data;Cameras","image motion analysis;image sensors;learning (artificial intelligence);pose estimation","motion capture systems;unsupervised approach;single image;2D pose data;camera rotations;camera angles;state-of-the-art unsupervised human pose estimation methods;benchmark datasets Human3.6M;MPI-INF-3DHP;unsupervised 3D;predicting camera elevation;learning normalizing flows;supervised learning;labeled training data;human activities","","6","","65","IEEE","27 Sep 2022","","","IEEE","IEEE Conferences"
"LMGP: Lifted Multicut Meets Geometry Projections for Multi-Camera Multi-Object Tracking","D. M. H. Nguyen; R. Henschel; B. Rosenhahn; D. Sonntag; P. Swoboda","Max Planck Institute for Informatics; Institute for Information Processing, Leibniz University Hannover; Institute for Information Processing, Leibniz University Hannover; Oldenburg University; Max Planck Institute for Informatics","2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)","27 Sep 2022","2022","","","8856","8865","Multi-Camera Multi-Object Tracking is currently drawing attention in the computer vision field due to its superior performance in real-world applications such as video surveillance with crowded scenes or in wide spaces. In this work, we propose a mathematically elegant multi-camera multiple object tracking approach based on a spatial-temporal lifted multicut formulation. Our model utilizes state-of-the-art tracklets produced by single-camera trackers as proposals. As these tracklets may contain ID-Switch errors, we refine them through a novel pre-clustering obtained from 3D geometry projections. As a result, we derive a better tracking graph without ID switches and more precise affinity costs for the data association phase. Tracklets are then matched to multi-camera trajectories by solving a global lifted multicut formulation that incorporates short and long-range temporal interactions on tracklets located in the same camera as well as inter-camera ones. Experimental results on the WildTrack dataset yield near-perfect performance, outperforming state-of-the-art trackers on Campus while being on par on the PETS-09 dataset. We will release our implementations at this link https://github.com/nhmduy/LMGP.","2575-7075","978-1-6654-6946-3","10.1109/CVPR52688.2022.00866","BMBF(grant numbers:01IW20005); BMG(grant numbers:2520DAT0P2); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9879609","Motion and tracking; Optimization methods; Scene analysis and understanding; Video analysis and understanding; Vision applications and systems","Geometry;Training;Computer vision;Three-dimensional displays;Video surveillance;Mathematical models;Trajectory","cameras;computer vision;object detection;object tracking;target tracking;tracking;video cameras;video surveillance","lifted multicut meets geometry projections;MultiCamera MultiObject Tracking;mathematically elegant multicamera multiple object tracking approach;spatial-temporal lifted multicut formulation;model utilizes state-of-the-art tracklets;single-camera trackers;multicamera trajectories;global lifted multicut formulation;inter-camera ones","","5","","53","IEEE","27 Sep 2022","","","IEEE","IEEE Conferences"
"Knowledge Mining with Scene Text for Fine-Grained Recognition","H. Wang; J. Liao; T. Cheng; Z. Gao; H. Liu; B. Ren; X. Bai; W. Liu",Huazhong University of Science and Technology; Huazhong University of Science and Technology; Huazhong University of Science and Technology; Huazhong University of Science and Technology; Tencent YouTu Lab; Tencent YouTu Lab; Huazhong University of Science and Technology; Huazhong University of Science and Technology,"2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)","27 Sep 2022","2022","","","4614","4623","Recently, the semantics of scene text has been proven to be essential in fine-grained image classification. However, the existing methods mainly exploit the literal meaning of scene text for fine-grained recognition, which might be irrelevant when it is not significantly related to objects/scenes. We propose an end-to-end trainable network that mines implicit contextual knowledge behind scene text image and enhance the semantics and correlation to fine-tune the image representation. Unlike the existing methods, our model integrates three modalities: visual feature extraction, text semantics extraction, and correlating background knowledge to fine-grained image classification. Specifically, we employ KnowBert to retrieve relevant knowledge for semantic representation and combine it with image features for fine-grained classification. Experiments on two benchmark datasets, Con-Text, and Drink Bottle, show that our method outperforms the state-of-the-art by 3.72% mAP and 5.39% mAp, respectively. To further validate the effectiveness of the proposed method, we create a new dataset on crowd activity recognition for the evaluation. The source code and new dataset of this work are available at this repository11https://github.com/lanfeng4659/KnowledgeMiningWithSceneText.","2575-7075","978-1-6654-6946-3","10.1109/CVPR52688.2022.00458","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9880093","Document analysis and understanding; Scene analysis and understanding; Vision + X; Visual reasoning","Knowledge engineering;Visualization;Image recognition;Text recognition;Semantics;Benchmark testing;Image representation","data mining;feature extraction;image classification;image recognition;image representation;text analysis","fine-grained image classification;semantic representation;image features;fine-grained classification;knowledge mining;fine-grained recognition;end-to-end trainable network;mines implicit contextual knowledge;scene text image;image representation;text semantics;background knowledge","","3","","43","IEEE","27 Sep 2022","","","IEEE","IEEE Conferences"
"OrienterNet: Visual Localization in 2D Public Maps with Neural Matching","P. -E. Sarlin; D. DeTone; T. -Y. Yang; A. Avetisyan; J. Straub; T. Malisiewicz; S. R. Bulo; R. Newcombe; P. Kontschieder; V. Balntas",ETH Zurich; Meta Reality Labs; Meta Reality Labs; Meta Reality Labs; Meta Reality Labs; Meta Reality Labs; Meta Reality Labs; Meta Reality Labs; Meta Reality Labs; Meta Reality Labs,"2023 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)","22 Aug 2023","2023","","","21632","21642","Humans can orient themselves in their 3D environments using simple 2D maps. Differently, algorithms for visual localization mostly rely on complex 3D point clouds that are expensive to build, store, and maintain over time. We bridge this gap by introducing OrienterNet, the first deep neural network that can localize an image with sub-meter accuracy using the same 2D semantic maps that humans use. OrienterNet estimates the location and orientation of a query image by matching a neural Bird's-Eye View with open and globally available maps from OpenStreetMap, enabling anyone to localize anywhere such maps are available. OrienterNet is supervised only by camera poses but learns to perform semantic matching with a wide range of map elements in an end-to-end manner. To enable this, we introduce a large crowd-sourced dataset of images captured across 12 cities from the diverse viewpoints of cars, bikes, and pedestrians. OrienterNet generalizes to new datasets and pushes the state of the art in both robotics and AR scenarios. The code is available at github.com/facebookresearch/OrienterNet.","2575-7075","979-8-3503-0129-8","10.1109/CVPR52729.2023.02072","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10204555","3D from single images","Location awareness;Training;Point cloud compression;Visualization;Three-dimensional displays;Pedestrians;Semantics","cameras;deep learning (artificial intelligence);image capture;image segmentation;learning (artificial intelligence)","2D public maps;2D semantic maps;complex 3D point clouds;deep neural network;globally available maps;map elements;neural Bird's-Eye View;open maps;query image;semantic matching;visual localization","","2","","82","IEEE","22 Aug 2023","","","IEEE","IEEE Conferences"
"MDQE: Mining Discriminative Query Embeddings to Segment Occluded Instances on Challenging Videos","M. Li; S. Li; W. Xiang; L. Zhang","Department of Computing, Hong Kong Polytechnic University; Department of Computing, Hong Kong Polytechnic University; Department of Computing, Hong Kong Polytechnic University; Department of Computing, Hong Kong Polytechnic University","2023 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)","22 Aug 2023","2023","","","10524","10533","While impressive progress has been achieved, video instance segmentation (VIS) methods with per-clip input often fail on challenging videos with occluded objects and crowded scenes. This is mainly because instance queries in these methods cannot encode well the discriminative embeddings of instances, making the query-based segmenter difficult to distinguish those ‘hard’ instances. To address these issues, we propose to mine discriminative query embeddings (MDQE) to segment occluded instances on challenging videos. First, we initialize the positional embeddings and content features of object queries by considering their spatial contextual information and the inter-frame object motion. Second, we propose an inter-instance mask repulsion loss to distance each instance from its nearby non-target instances. The proposed MDQE is the first VIS method with per-clip input that achieves state-of-the-art results on challenging videos and competitive performance on simple videos. In specific, MDQE with ResNet50 achieves 33.0% and 44.5% mask AP on OVIS and YouTube- Vis 2021, respectively. Code of MDQE can be found at https://github.com/MinghanLi/MDQE_CVPR2023.","2575-7075","979-8-3503-0129-8","10.1109/CVPR52729.2023.01014","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10203297","Segmentation;grouping and shape analysis","Computer vision;Codes;Motion segmentation;Spatiotemporal phenomena;Pattern recognition;Residual neural networks","convolutional neural nets;feature extraction;image classification;image segmentation;object detection;video signal processing","challenging videos;discriminative embeddings;hard instances;instance queries;inter-frame object motion;inter-instance mask repulsion loss;MDQE;mining discriminative query embeddings;nearby nontarget instances;object queries;occluded objects;per-clip input;positional embeddings;query-based segmenter;segment occluded instances;simple videos;video instance segmentation","","","","55","IEEE","22 Aug 2023","","","IEEE","IEEE Conferences"
"Trace and Pace: Controllable Pedestrian Animation via Guided Trajectory Diffusion","D. Rempe; Z. Luo; X. B. Peng; Y. Yuan; K. Kitani; K. Kreis; S. Fidler; O. Litany",NVIDIA; NVIDIA; NVIDIA; NVIDIA; Carnegie Mellon University; NVIDIA; NVIDIA; NVIDIA,"2023 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)","22 Aug 2023","2023","","","13756","13766","We introduce a method for generating realistic pedestrian trajectories and full-body animations that can be controlled to meet user-defined goals. We draw on recent advances in guided diffusion modeling to achieve test-time controllability of trajectories, which is normally only associated with rule-based systems. Our guided diffusion model allows users to constrain trajectories through target waypoints, speed, and specified social groups while accounting for the surrounding environment context. This trajectory diffusion model is integrated with a novel physics-based humanoid controller to form a closed-loop, full-body pedestrian animation system capable of placing large crowds in a simulated environment with varying terrains. We further propose utilizing the value function learned during RL training of the animation controller to guide diffusion to produce trajectories better suited for particular scenarios such as collision avoidance and traversing uneven terrain. Video results are available on the project page.","2575-7075","979-8-3503-0129-8","10.1109/CVPR52729.2023.01322","NVIDIA; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10203180","Humans: Face;body;pose;gesture;movement","Training;Computer vision;Pedestrians;Social groups;Humanoid robots;Animation;Controllability","collision avoidance;computer animation;humanoid robots;learning (artificial intelligence);mobile robots;pedestrians","animation controller;controllable pedestrian animation;full-body animations;full-body pedestrian animation system;guided diffusion model;guided diffusion modeling;guided trajectory diffusion;novel physics-based humanoid controller;realistic pedestrian trajectories;rule-based systems;surrounding environment context;test-time controllability;trajectory diffusion model;user-defined goals","","","","64","IEEE","22 Aug 2023","","","IEEE","IEEE Conferences"
"HumanBench: Towards General Human-Centric Perception with Projector Assisted Pretraining","S. Tang; C. Chen; Q. Xie; M. Chen; Y. Wang; Y. Ci; L. Bai; F. Zhu; H. Yang; L. Yi; R. Zhao; W. Ouyang",The University of Sydney; SenseTime Research; SenseTime Research; Zhejiang University; Zhejiang University; The University of Sydney; Shanghai AI Laboratory; SenseTime Research; SenseTime Research; SenseTime Research; SenseTime Research; Shanghai AI Laboratory,"2023 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)","22 Aug 2023","2023","","","21970","21982","Human-centric perceptions include a variety of vision tasks, which have widespread industrial applications, including surveillance, autonomous driving, and the metaverse. It is desirable to have a general pretrain model for versatile human-centric downstream tasks. This paper forges ahead along this path from the aspects of both benchmark and pretraining methods. Specifically, we propose a HumanBench based on existing datasets to comprehensively evaluate on the common ground the generalization abilities of different pretraining methods on 19 datasets from 6 diverse downstream tasks, including person ReID, pose estimation, human parsing, pedestrian attribute recognition, pedestrian detection, and crowd counting. To learn both coarse-grained and fine-grained knowledge in human bodies, we further propose a Projector AssisTed Hierarchical pretraining method (PATH) to learn diverse knowledge at different granularity levels. Comprehensive evaluations on HumanBench show that our PATH achieves new state-of-the-art results on 17 downstream datasets and on-par results on the other 2 datasets. The code will be publicly at https://github.com/OpenGVLab/HumanBench.","2575-7075","979-8-3503-0129-8","10.1109/CVPR52729.2023.02104","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10204833","Humans: Face;body;pose;gesture;movement","Learning systems;Computer vision;Pedestrians;Codes;Metaverse;Surveillance;Pose estimation","computer vision;feature extraction;learning (artificial intelligence);object detection;pedestrians;pose estimation;supervised learning;traffic engineering computing","17 downstream datasets;6 diverse downstream tasks;autonomous driving;common ground;comprehensive evaluations;different granularity levels;different pretraining methods;diverse knowledge;fine-grained knowledge;general pretrain model;generalization abilities;human bodies;human parsing;human-centric downstream tasks;human-centric perceptions;pedestrian attribute recognition;pedestrian detection;Projector AssisTed Hierarchical pretraining method;towards general human-centric perception;vision tasks;widespread industrial applications","","","","116","IEEE","22 Aug 2023","","","IEEE","IEEE Conferences"
