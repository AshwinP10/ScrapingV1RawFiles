Document Title,Authors,Publication Year,PDF Link,Implementation?,Abstract Summary
Measuring Crowd Collectiveness via Global Motion Correlation,L. Mei; J. Lai; Z. Chen; X. Xie,2019,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9022173,NO,"For intra-crowd collectiveness, we introduce the energy spread process to identify the impacting factors of collectiveness, then measure the collectiveness of individuals within a crowd cluster by computing their similarities of magnitude and direction from the optical flow. However, few studies consider the magnitude discrepancy of velocity in a crowd and the collectiveness among different crowds, which can also affect the overall crowd collectiveness."
Social and Scene-Aware Trajectory Prediction in Crowded Spaces,M. Lisotto; P. Coscia; L. Ballan,2019,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9021955,NO,"Mimicking human ability to forecast future positions or interpret complex interactions in urban scenarios, such as streets, shopping malls or squares, is essential to develop socially compliant robots or self-driving cars. To foresee plausible trajectories, we construct an LSTM (long short-term memory)-based model considering three fundamental factors: people interactions, past observations in terms of previously crossed areas and semantics of surrounding space."
SLAMANTIC - Leveraging Semantics to Improve VSLAM in Dynamic Environments,M. Schörghuber; D. Steininger; Y. Cabon; M. Humenberger; M. Gelautz,2019,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9022073,YES,"Evaluating our method on public datasets, we show that it can successfully solve challenging situations in dynamic environments which cause state-of-the-art baseline VSLAM algorithms to fail and that it maintains performance on static scenes. Points with high confidence are used to verify points with low confidence in order to select the final set of points for pose computation and mapping."
Towards Learning Multi-Agent Negotiations via Self-Play,Y. Tang,2019,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9022183,NO,"Leveraging the powerful Deep RL paradigm, we demonstrate that an iterative procedure of self-play can create progressively more diverse environments, leading to the learning of sophisticated and robust multi-agent policies. Qualitatively, we find that through self-play, our policies automatically learn interesting behaviors such as defensive driving, overtaking, yielding, and the use of signal lights to communicate intentions to other agents."
Crowd Counting on Images with Scale Variation and Isolated Clusters,H. Bai; S. Wen; S. . -H. G. Chan,2019,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9022196,NO,"SACANet consists of three major modules: the pyramid contextual module which extracts long-range contextual information and enlarges the receptive field, a scale-adaptive self-attention multi-branch module to attain high scale sensitivity and detection accuracy of isolated clusters, and a hierarchical fusion module to fuse multi-level self-attention features. Designing a general crowd counting algorithm applicable to a wide range of crowd images is challenging, mainly due to the possibly large variation in object scales and the presence of many isolated small clusters."
DECCNet: Depth Enhanced Crowd Counting,S. -D. Yang; H. -T. Su; W. H. Hsu; W. -C. Chen,2019,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9022022,NO,"In this paper, we present an effective crowd counting method, Depth Enhanced Crowd Counting Network (DECCNet), which leverages the estimated depth information with our novel Bidirectional Cross-modal Attention (BCA) mechanism. In our experiments, we demonstrate that DECCNet outperforms the state-of-the-art on the two largest crowd counting datasets available, including UCF-QNRF, which has the highest crowd density."
Characterizing Scattered Occlusions for Effective Dense-Mode Crowd Counting,K. J. Almalki; B. -Y. Choi; Y. Chen; S. Song,2021,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9607560,NO,"Through extensive evaluations, we demonstrate that the accuracy of CSONet with scattered occlusion images outperforms over the state-of-art existing crowd counting approaches by 30% to 100% in both mean absolute error and mean square error. We have designed and implemented a new crowd overfit reduction network by adding both spatial pyramid pooling and dilated convolution layers over modified VGG16 for capturing high-level features of extended receptive fields."
Pushing the Frontiers of Unconstrained Crowd Counting: New Dataset and Benchmark Method,V. Sindagi; R. Yasarla; V. Patel,2019,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9009496,NO,"The proposed method uses VGG16 as the backbone network and employs density map generated by the final layer as a coarse prediction to refine and generate finer density maps in a progressive fashion using residual learning. Furthermore, we introduce a new large scale unconstrained crowd counting dataset (JHU-CROWD) that is ~2.8 larger than the most recent crowd counting datasets in terms of the number of images."
VisDrone-CC2021: The Vision Meets Drone Crowd Counting Challenge Results,Z. Liu; Z. He; L. Wang; W. Wang; Y. Yuan; D. Zhang; J. Zhang; P. Zhu; L. V. Gool; J. Han; S. Hoi; Q. Hu; M. Liu; J. Pan; B. Yin; B. Zhang; C. Liu; D. Ding; D. Liang; G. Ding; H. Lu; H. Lin; J. Chen; J. Li; L. Liu; L. Zhou; M. Shi; Q. Yang; Q. He; S. Peng; W. Xu; W. Han; X. Bai; X. Chen; Y. Wang; Y. Xia; Y. Tao; Z. Chen; Z. Cao,2021,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9607764,NO,"Motivated by high quality and quantity study in crowding counting, we collect a drone-captured dataset formed by 5,468 images(images in RGB and thermal appear in pairs and 2,734 respectively). Based on this dataset, we organized the Vision Meets Drone Crowd Counting Challenge(Visdrone-CC2021) in conjunction with the International Conference on Computer Vision (ICCV 2021)."
Audio-Visual Transformer Based Crowd Counting,U. Sajid; X. Chen; H. Sajid; T. Kim; G. Wang,2021,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9607793,NO,"The most recent study tries to exploit auditory information to aid the visual models, however, the performance is limited due to the lack of an effective approach for feature extraction and integration. The paper proposes a new audiovisual multi-task network to address the critical challenges in crowd counting by effectively utilizing both visual and audio inputs for better modalities association and productive feature extraction."
Counterfactual Critic Multi-Agent Training for Scene Graph Generation,L. Chen; H. Zhang; J. Xiao; X. He; S. Pu; S. -F. Chang,2019,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9010810,NO,"However, we argue that the visual context is not properly learned by using the prevailing cross-entropy based supervised learning paradigm, which is not sensitive to graph inconsistency: errors at the hub or non-hub nodes should not be penalized equally. In particular, to assign the reward properly to each agent, CMAT uses a counterfactual baseline that disentangles the agent-specific reward by fixing the predictions of other agents."
Prior Guided Dropout for Robust Visual Localization in Dynamic Environments,Z. Huang; Y. Xu; J. Shi; X. Zhou; H. Bao; G. Zhang,2019,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9008288,YES,"Additionally, the dropout module enables the pose regressor to output multiple hypotheses from which the uncertainty of pose estimates can be quantified and leveraged in the following uncertainty-aware pose graph optimization to improve the robustness further. The key idea is a prior guided dropout module coupled with a self-attention module which can guide CNNs to ignore foreground objects during both training and inference."
Perspective-Guided Convolution Networks for Crowd Counting,Z. Yan; Y. Yuan; W. Zuo; X. Tan; Y. Wang; S. Wen; E. Ding,2019,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9010874,YES,"While most state-of-the-arts adopt multi-scale or multi-column architectures to address such issue, they generally fail in modeling continuous scale variations since only discrete representative scales are considered. An effective perspective estimation branch is also introduced to PGCNet, which can be trained in either supervised setting or weakly-supervised setting when the branch has been pre-trained."
Bayesian Loss for Crowd Count Estimation With Point Supervision,Z. Ma; X. Wei; X. Hong; Y. Gong,2019,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9009503,NO,"Most of the state-of-the-art methods are based on density map estimation, which convert the sparse point annotations into a “ground truth” density map through a Gaussian kernel, and then use it as the learning target to train a density map estimator. Instead of constraining the value at every pixel in the density map, the proposed training loss adopts a more reliable supervision on the count expectation at each annotated point."
PRECOG: PREdiction Conditioned on Goals in Visual Multi-Agent Settings,N. Rhinehart; R. Mcallister; K. Kitani; S. Levine,2019,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9009551,NO,"Beyond its general ability to perform conditional forecasting queries, we show that our model's predictions of all agents improve when conditioned on knowledge of the AV's goal, further illustrating its capability to model agent interactions. For autonomous vehicles (AVs) to behave appropriately on roads populated by human-driven vehicles, they must be able to reason about the uncertain intentions and decisions of other drivers from rich perceptual information."
An Indoor Crowd Detection Network Framework Based on Feature Aggregation Module and Hybrid Attention Selection Module,W. Shen; P. Qin; J. Zeng,2019,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9022292,NO,"Since the indoor population feature and background feature overlap and the classification boundaries are not obvious, the proposed improved hybrid attention selection module (HASM) combines the selection mechanism with the previously proposed mixed attention module. In order to better provide the details needed for small scale pupulation detection, we propose a novel feature aggregation module (FAM), which uses the idea of fusion and decomposition to aggregate contextual feature information."
DnD: Dense Depth Estimation in Crowded Dynamic Indoor Scenes,D. Jung; J. Choi; Y. Lee; D. Kim; C. Kim; D. Manocha; D. Lee,2021,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9709913,NO,"We present a novel approach for estimating depth from a monocular camera as it moves through complex and crowded indoor environments, e.g., a department store or a metro station. Since it is difficult to collect dense depth maps from crowded indoor environments, we design our training framework without requiring depths produced from depth sensing devices."
Adaptive Density Map Generation for Crowd Counting,J. Wan; A. Chan,2019,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9009065,NO,"To address this issue, we first show the impact of different density maps and that better ground-truth density maps can be obtained by refining the existing ones using a learned refinement network, which is jointly trained with the counter. In the sense of end-to-end training, the hand-crafted methods used for generating the density maps may not be optimal for the particular network or dataset used."
Crowd Counting With Deep Structured Scale Integration Network,L. Liu; Z. Qiu; G. Li; S. Liu; W. Ouyang; L. Lin,2019,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9010246,NO,"Unlike conventional methods which directly fuse multiple features with weighted average or concatenation, we first introduce a Structured Feature Enhancement Module based on conditional random fields (CRFs) to refine multiscale features mutually with a message passing mechanism. In this paper, we propose a novel Deep Structured Scale Integration Network (DSSINet) for crowd counting, which addresses the scale variation of people by using structured feature representation learning and hierarchically structured loss function optimization."
EM-Fusion: Dynamic Object-Level SLAM With Probabilistic Data Association,M. Strecke; J. Stueckler,2019,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9010932,NO,"We represent rigid objects in local volumetric signed distance function (SDF) maps, and formulate multi-object tracking as direct alignment of RGB-D images with the SDF representations. We analyze our approach in experiments and demonstrate that our approach compares favorably with the state-of-the-art methods in terms of robustness and accuracy."
Social NCE: Contrastive Learning of Socially-aware Motion Representations,Y. Liu; Q. Yan; A. Alahi,2021,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9711444,YES,"In this work, we aim to address this issue by explicitly modeling negative examples through self-supervision: (i) we intro-duce a social contrastive loss that regularizes the extracted motion representation by discerning the ground-truth positive events from synthetic negative ones; (ii) we construct informative negative samples based on our prior knowledge of rare but dangerous circumstances. Intuitively, if the training data only comes from human behaviors in safe spaces, i.e., from ""positive"" examples, it is difficult for learning algorithms to capture the notion of ""negative"" examples like collisions."
Learn to Scale: Generating Multipolar Normalized Density Maps for Crowd Counting,C. Xu; K. Qiu; J. Fu; S. Bai; Y. Xu; X. Bai,2019,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9009068,NO,"Our work outperforms the state-of-the-art by 4.2%, 14.3%, 27.1% and 20.1% in MAE, on the ShanghaiTech Part A, ShanghaiTech Part B, UCF_CC_50 and UCF-QNRF datasets, respectively. First, a patch-level density map is extracted by a density estimation model and further grouped into several density levels which are determined over full datasets."
Multi-Agent Reinforcement Learning Based Frame Sampling for Effective Untrimmed Video Recognition,W. Wu; D. He; X. Tan; S. Chen; S. Wen,2019,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9009461,NO,"Our MARL framework is composed of a novel RNN-based context-aware observation network which jointly models context information among nearby agents and historical states of a specific agent, a policy network which generates the probability distribution over a predefined action space at each step and a classification network for reward calculation as well as final recognition. We intuitively formulate the frame sampling procedure as multiple parallel Markov decision processes, each of which aims at picking out a frame/clip by gradually adjusting an initial sampling."
FullFusion: A Framework for Semantic Reconstruction of Dynamic Scenes,M. Bujanca; M. Lujan; B. Lennox,2019,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9022128,NO,"FullFusion is a step towards enabling robots to have a deeper and richer understanding of their surroundings, and can facilitate the study of interaction and scene dynamics. It is, however, important to highlight that the modular design of the framework allows us to easily replace any of the components with new or existing counterparts."
The Trajectron: Probabilistic Multi-Agent Trajectory Modeling With Dynamic Spatiotemporal Graphs,B. Ivanovic; M. Pavone,2019,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9009454,NO,"We demonstrate the performance of our model on several datasets, obtaining state-of-the-art results on standard trajectory prediction metrics as well as introducing a new metric for comparing models that output distributions. Towards this end, we present the Trajectron, a graph-structured model that predicts many potential future trajectories of multiple agents simultaneously in both highly dynamic and multimodal scenarios (i.e."
Learning Spatial Awareness to Improve Crowd Counting,Z. -Q. Cheng; J. -X. Li; Q. Dai; X. Wu; A. Hauptmann,2019,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9010678,NO,"Existing methods widely employ the Euclidean distance (i.e., $L_2$ loss) to optimize the model, which, however, has two main drawbacks: (1) the loss has difficulty in learning the spatial awareness (i.e., the position of head) since it struggles to retain the high-frequency variation in the density map, and (2) the loss is highly sensitive to various noises in crowd counting, such as the zero-mean noise, head size changes, and occlusions. Although the Maximum Excess over SubArrays (MESA) loss has been previously proposed by~\cite{nips-10} to address the above issues by finding the rectangular subregion whose predicted density map has the maximum difference from the ground truth, it cannot be solved by gradient descent, thus can hardly be integrated into the deep learning framework."
Env-QA: A Video Question Answering Benchmark for Comprehensive Understanding of Dynamic Environments,D. Gao; R. Wang; Z. Bai; X. Chen,2021,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9711383,NO,"To achieve complex tasks in volatile situations, the human can deeply understand the environment, quickly perceive events happening around, and continuously track objects’ state changes, which are still challenging for current AI systems. Moreover, we propose a video QA model, Temporal Segmentation and Event Attention network (TSEA), which introduces event-level video representation and corresponding attention mechanisms to better extract environment information and answer questions."
Multi-Level Bottom-Top and Top-Bottom Feature Fusion for Crowd Counting,V. Sindagi; V. Patel,2019,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9009788,NO,"Specifically, we present a network that involves: (i) a multi-level bottom-top and top-bottom fusion (MBTTBF) method to combine information from shallower to deeper layers and vice versa at multiple levels, (ii) scale complementary feature extraction blocks (SCFB) involving cross-scale residual functions to explicitly enable flow of complementary features from adjacent conv layers along the fusion paths. Experiments conducted on three datasets that contain highly congested scenes (ShanghaiTech, UCF_CC_50, and UCF-QNRF) demonstrate that the proposed method is able to outperform several recent methods in all the datasets."
Relational Attention Network for Crowd Counting,A. Zhang; J. Shen; Z. Xiao; F. Zhu; X. Zhen; X. Cao; L. Shao,2019,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9010829,NO,"The RANet enhances the self-attention mechanism by accounting both short-range and long-range interdependence of pixels, where we respectively denote these implementations as local self-attention (LSA) and global self-attention (GSA). Density estimation is a popular strategy for crowd counting, where conventional density estimation methods perform pixel-wise regression without explicitly accounting the interdependence of pixels."
LPD-Net: 3D Point Cloud Learning for Large-Scale Place Recognition and Environment Analysis,Z. Liu; S. Zhou; C. Suo; P. Yin; W. Chen; H. Wang; H. Li; Y. Liu,2019,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9009029,NO,"Two modules, the adaptive local feature extraction module and the graph-based neighborhood aggregation module, are proposed, which contribute to extract the local structures and reveal the spatial distribution of local features in the large-scale point cloud, with an end-to-end manner. Point cloud based place recognition is still an open issue due to the difficulty in extracting local features from the raw 3D point cloud and generating the global descriptor, and it's even harder in the large-scale dynamic environments."
Attentional Neural Fields for Crowd Counting,A. Zhang; L. Yue; J. Shen; F. Zhu; X. Zhen; X. Cao; L. Shao,2019,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9009565,NO,"To better model pair-wise potentials in CRFs, we incorperate non-local attention mechanism implemented as inter- and intra-layer attentions to expand the receptive field to the entire image respectively within the same layer and across different layers, which captures long-range dependencies to conquer huge scale variations. The CRFs coupled with the attention mechanism are seamlessly integrated into the encoder-decoder network, establishing an ANF that can be optimized end-to-end by back propagation."
"LaLaLoc: Latent Layout Localisation in Dynamic, Unvisited Environments",H. Howard-Jenkins; J. -R. Ruiz-Sarmiento; V. A. Prisacariu,2021,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9710879,NO,"We present LaLaLoc to localise in environments without the need for prior visitation, and in a manner that is robust to large changes in scene appearance, such as a full rearrangement of furniture. Thus, LaLaLoc enables fine-grained pose estimation in a scene without the need for prior visitation, as well as being robust to dynamics, such as a change in furniture configuration."
Conditional Vehicle Trajectories Prediction in CARLA Urban Environment,T. Buhet; E. Wirbel; X. Perrotton,2019,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9022290,NO,"Mid-to-mid (environment abstraction to mid-level trajectory representation) or direct perception (raw signal to performance) approaches strive to handle more complex, real life environment and tasks (e.g. We propose an original architecture inspired from social pooling LSTM taking low and mid level data as input and producing trajectories as polynomials of time."
Instance Segmentation in CARLA: Methodology and Analysis for Pedestrian-oriented Synthetic Data Generation in Crowded Scenes,M. Lyssenko; C. Gladisch; C. Heinzemann; M. Woehrle; R. Triebel,2021,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9607548,NO,Our evaluation results show that per-pedestrian depth aggregation obtained from our instance segmentation is more precise than previously available approximations based on bounding boxes especially in the context of crowded scenes in urban automated driving. Recently proposed metrics for safety evaluation additionally require detailed per-instance annotations of dynamic properties such as distance and velocities that may not be available in openly accessible AD datasets.
MeteorNet: Deep Learning on Dynamic 3D Point Cloud Sequences,X. Liu; M. Yan; J. Bohg,2019,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9010250,NO,"Different from previous work that adopts a grid-based representation and applies 3D or 4D convolutions, our network directly processes point clouds. We benchmark our network on a variety of 3D recognition tasks including action recognition, semantic segmentation and scene flow estimation."
Spatial Uncertainty-Aware Semi-Supervised Crowd Counting,Y. Meng; H. Zhang; Y. Zhao; X. Yang; X. Qian; X. Huang; Y. Zheng,2021,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9710789,YES,"Besides, we introduce a simple yet effective differential transformation layer to enforce the inherent spatial consistency regularization between the main task and the surrogate task in the student model, which helps the surrogate task to yield more reliable predictions and generates high-quality uncertainty maps. Different from existing semi-supervised learning-based crowd counting methods, to exploit the unlabeled data, our proposed spatial uncertainty-aware teacher-student framework focuses on high confident regions’ information while addressing the noisy supervision from the unlabeled data in an end-to-end manner."
Exploiting sample correlation for crowd counting with multi-expert network,X. Liu; G. Li; Z. Han; W. Zhang; Y. Yang; Q. Huang; N. Sebe,2021,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9710485,NO,"Besides, to make better use of the proposed method, we design a simple network called FPN with Deconvolution Counting Network, which is a more suitable base model for the multi-expert counting network. Specifically, we propose a task-driven similarity metric based on sample’s mutual enhancement, referred as co-fine-tune similarity, which can find a more efficient subset of data for training the expert network."
Graph CNN for Moving Object Detection in Complex Environments from Unseen Videos,J. H. Giraldo; S. Javed; N. Werghi; T. Bouwmans,2021,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9607835,NO,"Moreover, some MOD deep learning methods show performance degradation in the presence of unseen video sequences because the testing and training splits of the same sequences are involved during the network learning process. MOD becomes very challenging when a video sequence captured from a static or moving camera suffers from the challenges: camouflage, shadow, dynamic backgrounds, and lighting variations, to name a few."
Rethinking Counting and Localization in Crowds: A Purely Point-Based Framework,Q. Song; C. Wang; Z. Jiang; Y. Wang; Y. Tai; C. Wang; J. Li; F. Huang; Y. Wu,2021,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9710076,NO,"For this framework, instead of merely reporting the absolute counting error at image level, we propose a new metric, called density Normalized Average Precision (nAP), to provide more comprehensive and more precise performance evaluation. P2PNet discards superfluous steps and directly predicts a set of point proposals to represent heads in an image, being consistent with the human annotation results."
Crowd Counting With Partial Annotations in an Image,Y. Xu; Z. Zhong; D. Lian; J. Li; Z. Li; X. Xu; S. Gao,2021,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9711085,YES,"Inspired by the repetitive patterns in the annotated and unannotated regions as well as the ones between them, we design a network with three components to tackle those unannotated regions: i) in an Unannotated Regions Characterization (URC) module, we employ a memory bank to only store the annotated features, which could help the visual features extracted from these annotated regions flow to these unannotated regions; ii) For each image, Feature Distribution Consistency (FDC) regularizes the feature distributions of annotated head and unannotated head regions to be consistent; iii) a Cross-regressor Consistency Regularization (CCR) module is designed to learn the visual features of unannotated regions in a self-supervised style. With only 10% annotated regions in each image, our proposed model achieves better performance than the recent methods and baselines under semi-supervised or active learning settings on all datasets."
Towards A Universal Model for Cross-Dataset Crowd Counting,Z. Ma; X. Hong; X. Wei; Y. Qiu; Y. Gong,2021,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9710112,NO,"We dissect that the crux of this problem is the catastrophic sensitivity of crowd counters to scale shift, which is very common in the real world and caused by factors such as different scene layouts and image resolutions. Benefiting from the proposed method, we have learned a universal model that generally works well on several datasets where can even outperform state-of-the-art models that are particularly fine-tuned for each dataset significantly."
STGAT: Modeling Spatial-Temporal Interactions for Human Trajectory Prediction,Y. Huang; H. Bi; Z. Li; T. Mao; Z. Wang,2019,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9010834,NO,"Because of the continuity and foresight of the pedestrian movements, the moving pedestrians in crowded spaces will consider both spatial and temporal interactions to avoid future collisions. Through comparisons with state-of-the-art methods, our model achieves superior performance on two publicly available crowd datasets (ETH and UCY) and produces more ""socially"" plausible trajectories for pedestrians."
AgentFormer: Agent-Aware Transformers for Socio-Temporal Multi-Agent Forecasting,Y. Yuan; X. Weng; Y. Ou; K. Kitani,2021,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9710708,NO,"Since standard attention operations disregard the agent identity of each element in the sequence, AgentFormer uses a novel agent-aware attention mechanism that preserves agent identities by attending to elements of the same agent differently than elements of other agents. Forecasting multi-agent trajectories requires modeling two key dimensions: (1) time dimension, where we model the influence of past agent states over future states; (2) social dimension, where we model how the state of each agent affects others."
Adversarial Attacks On Multi-Agent Communication,J. Tu; T. Wang; J. Wang; S. Manivasagam; M. Ren; R. Urtasun,2021,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9711249,NO,"Furthermore, we show that black-box transfer attacks are more difficult in this setting when compared to directly perturbing the inputs, as it is necessary to align the distribution of learned representations with domain adaptation. Thus, we aim to study the robustness of such systems and focus on exploring adversarial at-tacks in a novel multi-agent setting where communication is done through sharing learned intermediate representations of neural networks."
Variational Attention: Propagating Domain-Specific Knowledge for Multi-Domain Learning in Crowd Counting,B. Chen; Z. Yan; K. Li; P. Li; B. Wang; W. Zuo; L. Zhang,2021,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9710532,NO,"In this paper, we resort to the multi-domain joint learning and propose a simple but effective Domain-specific Knowledge Propagating Network (DKPNet) for unbiasedly learning the knowledge from multiple diverse data domains at the same time. In crowd counting, due to the problem of laborious labelling, it is perceived intractability of collecting a new large-scale dataset which has plentiful images with large diversity in density, scene, etc."
Uniformity in Heterogeneity: Diving Deep into Count Interval Partition for Crowd Counting,C. Wang; Q. Song; B. Zhang; Y. Wang; Y. Tai; X. Hu; C. Wang; J. Li; J. Ma; Y. Wu,2021,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9710074,NO,"Therefore, we propose a novel count interval partition criterion called Uniform Error Partition (UEP), which always keeps the expected counting error contributions equal for all intervals to minimize the prediction risk. The MCP criterion selects the best count proxy for each interval to represent its count value during inference, making the overall expected discretization error of an image nearly negligible."
Dynamic Context-Sensitive Filtering Network for Video Salient Object Detection,M. Zhang; J. Liu; Y. Wang; Y. Piao; S. Yao; W. Ji; J. Li; H. Lu; Z. Luo,2021,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9710498,YES,"In this work, we aim to answer the following questions: How can a model adjust itself to dynamic variations as well as perceive fine differences in the real-world environment; How are the temporal dynamics well introduced into spatial information over time? To this end, we propose a dynamic context-sensitive filtering network (DCFNet) equipped with a dynamic context-sensitive filtering module (DCFM) and an effective bidirectional dynamic fusion strategy."
Online Multi-Task Clustering for Human Motion Segmentation,G. Sun; Y. Cong; L. Wang; Z. Ding; Y. Fu,2019,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9022059,NO,"However, most existing state-of-the-arts address this problem upon an offline and single-agent scenario, while there are a lot of urgent requirements to segment videos captured from multiple agents for real-time application (e.g., surveillance system). Specifically, a linear autoencoder framework is designed to project motion sequences into a common motion-aware space across multiple collaborating tasks, while the decoder obtains motion-aware representation of each task via a temporal preserved regularizer."
ClusterSLAM: A SLAM Backend for Simultaneous Rigid Body Clustering and Motion Estimation,J. Huang; S. Yang; Z. Zhao; Y. -K. Lai; S. Hu,2019,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9008108,NO,"Evaluations on both synthetic scenes and KITTI demonstrate the capability of our approach, and further experiments considering online efficiency also show the effectiveness of our method for simultaneous tracking of ego-motion and multiple objects. In this paper, we exploit the consensus of 3D motions among the landmarks extracted from the same rigid body for clustering and estimating static and dynamic objects in a unified manner."
Neighbourhood Context Embeddings in Deep Inverse Reinforcement Learning for Predicting Pedestrian Motion Over Long Time Horizons,T. Fernando; S. Denman; S. Sridharan; C. Fookes,2019,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9022234,NO,"Predicting crowd behaviour in the distant future has increased in prominence among the computer vision community as it provides intelligence and flexibility for autonomous systems, enabling the early detection of abnormal events and better and more natural interactions between humans and autonomous systems such as driverless vehicles and field robots. Despite the fact that Deep Inverse Reinforcement Learning (D-IRL) based modelling paradigms offer flexibility and robustness when anticipating human behaviour across long time horizons, compared to their supervised learning counterparts, no existing state-of-the-art D-IRL methods consider path planning in situations where there are multiple moving pedestrians in the environment."
Learning an Event Sequence Embedding for Dense Event-Based Deep Stereo,S. Tulyakov; F. Fleuret; M. Kiefel; P. Gehler; M. Hirsch,2019,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9008838,NO,"The module builds a representation of an event sequence by firstly aggregating information locally across time, using a novel fully-connected layer for an irregularly sampled continuous domain, and then across discrete spatial domain. However, these cameras, originally developed for acquisition of static images rather than for sensing of dynamic uncontrolled visual environments, suffer from high power consumption, data rate, latency and low dynamic range."
A Dataset of Multi-Illumination Images in the Wild,L. Murmann; M. Gharbi; M. Aittala; F. Durand,2019,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9008252,NO,"Multi-illumination datasets are notoriously hard to capture, so the data is typically collected at small scale, in controlled environments, either using multiple light sources, or robotic gantries. We introduce a new multi-illumination dataset of more than 1000 real scenes, each captured in high dynamic range and high resolution, under 25 lighting conditions."
FuseMODNet: Real-Time Camera and LiDAR Based Moving Object Detection for Robust Low-Light Autonomous Driving,H. Rashed; M. Ramzy; V. Vaquero; A. El Sallab; G. Sistu; S. Yogamani,2019,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9022585,NO,"As dynamic objects represent higher collision risk than static ones, our own ego-trajectories have to be planned attending to the future states of the moving elements of the scene. In this work we propose a robust and real-time CNN architecture for Moving Object Detection (MOD) under low-light conditions by capturing motion information from both camera and LiDAR sensors."
Real-Time Aerial Suspicious Analysis (ASANA) System for the Identification and Re-Identification of Suspicious Individuals using the Bayesian ScatterNet Hybrid (BSH) Network,A. Singh; K. Kiran G.V.; O. Harsh; R. Kumar; K. Singh Rajput; C. S .S. Vamsi,2019,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9022217,NO,"Video monitoring and safety systems have been used to keep track of hostiles, conduct border control operations as well as to monitor the suspicious entities in public spaces. This paper introduces the Aerial Suspicious Analysis (ASANA) System for the Identification and Re-Identification of suspicious Individuals in large public areas using the Bayesian ScatterNet Hybrid (BSH) Network."
A Light Stage on Every Desk,S. Sengupta; B. Curless; I. Kemelmacher-Shlizerman; S. Seitz,2021,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9709911,NO,"Whereas existing light stages require expensive, room-scale spherical capture gantries and exist in only a few labs in the world, we demonstrate how to acquire useful data from a normal TV or desktop monitor. We train a deep network on images plus monitor patterns of a given user and learn to predict images of that user under any target illumination (monitor pattern)."
Bayesian Relational Memory for Semantic Visual Navigation,Y. Wu; Y. Wu; A. Tamar; S. Russell; G. Gkioxari; Y. Tian,2019,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9009539,NO,"BRM takes the form of a probabilistic relation graph over semantic entities (e.g., room types), which allows (1) capturing the layout prior from training environments, i.e., prior knowledge, (2) estimating posterior layout at test time, i.e., memory update, and (3) efficient planning for navigation, altogether. We introduce a new memory architecture, Bayesian Relational Memory (BRM), to improve the generalization ability for semantic visual navigation agents in unseen environments, where an agent is given a semantic target to navigate towards."
Embodied Amodal Recognition: Learning to Move to Perceive Objects,J. Yang; Z. Ren; M. Xu; X. Chen; D. Crandall; D. Parikh; D. Batra,2019,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9008379,NO,"In this work, we introduce the task of Embodied Amodel Recognition (EAR): an agent is instantiated in a 3D environment close to an occluded target object, and is free to move in the environment to perform object classification, amodal object localization, and amodal object segmentation. Experimental results show that: 1) agents with embodiment (movement) achieve better visual recognition performance than passive ones and 2) in order to improve visual recognition abilities, agents can learn strategic paths that are different from shortest paths."
Learning to Caption Images Through a Lifetime by Asking Questions,T. Shen; A. Kar; S. Fidler,2019,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9009050,NO,"Our agent is composed of three interacting modules, one that performs captioning, another that generates questions and a decision maker that learns when to ask questions by implicitly reasoning about the uncertainty of the agent and expertise of the teacher. In order to bring artificial agents into our lives, we will need to go beyond supervised learning on closed datasets to having the ability to continuously expand knowledge."
From Open Set to Closed Set: Counting Objects by Spatial Divide-and-Conquer,H. Xiong; H. Lu; C. Liu; L. Liu; Z. Cao; C. Shen,2019,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9010835,YES,"Visual counting, a task that predicts the number of objects from an image/video, is an open-set problem by nature, i.e., the number of population can vary in [0,+∞) in theory. Existing methods typically model this task in a regression manner, while they are likely to suffer from an unseen scene with counts out of the scope of the closed set."
Joint Prediction for Kinematic Trajectories in Vehicle-Pedestrian-Mixed Scenes,H. Bi; Z. Fang; T. Mao; Z. Wang; Z. Deng,2019,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9008812,NO,"However, they fall short of handling crowded vehicle-pedestrian-mixed scenes directly since vehicles, limited with kinematics in reality, should be treated as rigid, non-particle objects ideally. Through comparisons between our method with state-of-the-art approaches, we show the effectiveness and advantages of our method on kinematic trajectories prediction in vehicle-pedestrian-mixed scenes."
STIRNet: A Spatial-temporal Interaction-aware Recursive Network for Human Trajectory Prediction,Y. Peng; G. Zhang; X. Li; L. Zheng,2021,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9607725,NO,Experimental results on two public pedestrian trajectory datasets (ETH and UCY) demonstrate that our proposed model achieves superior performances compared with state-of-the-art methods on ADE and FDE metrics. Pedestrian trajectory prediction is one of the important research topics in the field of computer vision and a key technology of autonomous driving system.
A Framework for Semi-automatic Collection of Temporal Satellite Imagery for Analysis of Dynamic Regions,N. K. Motlagh; A. Radhakrishnan; J. Davis; R. Ilin,2021,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9607433,NO,"We address this need by presenting a framework to semi-automatically collect and label dynamic regions in satellite imagery using crowd-sourced OpenStreetMap data and available satellite imagery resources. The lack of a streamlined method to collect and label imagery over time makes it challenging to tackle these problems using popular, supervised deep learning approaches."
Learning to Localise and Count with Incomplete Dot-annotations,F. Chen; M. P. Pound; A. P. French,2021,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9607724,NO,"This reduction of annotations can be imposed by the researchers by asking annotators to intentionally label only 50% of what they see in each image - a form of ’few-click’ annotation. Whilst tuning of the key parameters is required, we find that setting conservative parameter values can help more realistic situations, where only small amounts of data have been missed by annotators."
Exploring the Limitations of Behavior Cloning for Autonomous Driving,F. Codevilla; E. Santana; A. Lopez; A. Gaidon,2019,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9009463,YES,"However, we confirm some limitations of the behavior cloning approach: some wellknown limitations (e.g., dataset bias and overfitting), new generalization issues (e.g., dynamic objects and the lack of a causal modeling), and training instabilities, all requiring further research before behavior cloning can graduate to real-world driving. Behavior cloning in particular has been successfully used to learn simple visuomotor policies end-to-end, but scaling to the full spectrum of driving behaviors remains an unsolved problem."
Neural Re-Simulation for Generating Bounces in Single Images,C. Innamorati; B. Russell; D. Kaufman; N. Mitra,2019,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9009056,NO,"The neural network can then be seen as learning to `correct' traditional simulation output, generated with incomplete and imprecise world information, to obtain context-specific, visually plausible re-simulated output - a process we call neural re-simulation. Given a starting trajectory, physically simulated with the estimated geometry of a single, static input image, we learn to 'correct' this trajectory to a visually plausible one via a neural network."
Human Trajectory Prediction via Counterfactual Analysis,G. Chen; J. Li; J. Lu; J. Zhou,2021,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9710303,NO,"Hence, we propose a counterfactual analysis method for human trajectory prediction to investigate the causality between the predicted trajectories and input clues and alleviate the negative effects brought by the environment bias. We show that our method achieves consistent improvement for different baselines and obtains the state-of-the-art results on public pedestrian trajectory forecasting benchmarks.1"
In-the-Wild Single Camera 3D Reconstruction Through Moving Water Surfaces,J. Xiong; W. Heidrich,2021,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9710376,NO,"We propose a novel differentiable framework, which, to our knowledge, is the first single-camera solution that is capable of simultaneously retrieving the structure of dynamic water surfaces and static underwater scene geometry in the wild. Experimental results show that our method is able to realize robust and quality reconstructions on a variety of scenes, both in a laboratory environment and in the wild, and even in a salt water environment."
Inference of Black Hole Fluid-Dynamics from Sparse Interferometric Measurements,A. Levis; D. Lee; J. A. Tropp; C. F. Gammie; K. L. Bouman,2021,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9711239,NO,"We are motivated by the task of imaging the stochastically evolving environment surrounding black holes, and demonstrate how flow parameters can be estimated from sparse interferometric measurements used in radio astronomical imaging. We analyze our approach on realistic simulations of black hole evolution and demonstrate its advantage over state-of-the-art dynamic black hole imaging techniques."
MOTSynth: How Can Synthetic Data Help Pedestrian Detection and Tracking?,M. Fabbri; G. Brasó; G. Maugeri; O. Cetintas; R. Gasparini; A. Ošep; S. Calderara; L. Leal-Taixé; R. Cucchiara,2021,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9711285,NO,"However, data acquisition in crowded public environments raises data privacy concerns – we are not allowed to simply record and store data without the explicit consent of all participants. Our experiments show that MOTSynth can be used as a replacement for real data on tasks such as pedestrian detection, re-identification, segmentation, and tracking."
Learning to drive from a world on rails,D. Chen; V. Koltun; P. Krähenbühl,2021,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9709920,NO,"Our approach computes action-values for each training trajectory using a tabular dynamic-programming evaluation of the Bellman equations; these action-values in turn supervise the final vision-based driving policy. This assumption greatly simplifies the learning problem, factorizing the dynamics into a non-reactive world model and a low-dimensional and compact forward model of the ego-vehicle."
BEV-Net: Assessing Social Distancing Compliance by Joint People Localization and Geometric Reasoning,Z. Dai; Y. Jiang; Y. Li; B. Liu; A. B. Chan; N. Vasconcelos,2021,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9710671,YES,"BEV-Net combines detection of head and feet locations, camera pose estimation, a differentiable homography module to map image into BEV coordinates, and geometric reasoning to produce a BEV map of the people locations in the scene. A dataset of crowd scenes with people annotations under a bird’s eye view (BEV) and ground truth for metric distances is introduced, and several measures for the evaluation of social distance detection systems are proposed."
Episodic Transformer for Vision-and-Language Navigation,A. Pashevich; C. Schmid; C. Sun,2021,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9710429,NO,"We demonstrate that encoding the history with a transformer is critical to solve compositional tasks, and that pretraining and joint training with synthetic instructions further improve the performance. To improve training, we leverage synthetic instructions as an intermediate representation that decouples understanding the visual appearance of an environment from the variations of natural language instructions."
DRIVE: Deep Reinforced Accident Anticipation with Visual Explanation,W. Bao; Q. Yu; Y. Kong,2021,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9710981,NO,"The method simulates both the bottom-up and top-down visual attention mechanism in a dashcam observation environment so that the decision from the pro-posed stochastic multi-task agent can be visually explained by attentive regions. Traffic accident anticipation aims to accurately and promptly predict the occurrence of a future accident from dashcam videos, which is vital for a safety-guaranteed self-driving system."
"X-World: Accessibility, Vision, and Autonomy Meet",J. Zhang; M. Zheng; M. Boyd; E. Ohn-Bar,2021,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9710390,NO,"A main reason for this issue is the absence of any large-scale, standardized vision benchmarks that incorporate relevant tasks and scenarios related to people with disabilities. Our contributions provide an initial step towards widespread deployment of vision-based agents that can perceive and model the interaction needs of diverse people with disabilities."
Relational Prior for Multi-Object Tracking,A. Moskalev; I. Sosnovik; A. Smeulders,2021,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9607526,NO,"Such an approach does not take inter-object relations into account, which may cause unreliable tracking for the members of the groups, especially in crowded scenarios, where individual cues become unreliable. Most of the current state-of-the-art trackers follow the approach of tracking each object independently, with the mechanism to handle the overlapping trajectories where necessary."
Wanderlust: Online Continual Object Detection in the Real World,J. Wang; X. Wang; Y. Shang-Guan; A. Gupta,2021,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9710595,YES,The emergence of new object categories in our benchmark follows a pattern similar to what a single person might see in their day-to-day life. We also introduce new evaluation metrics to evaluate the model performance and catastrophic forgetting and provide baseline studies for online continual object detection.
Great Ape Detection in Challenging Jungle Camera Trap Footage via Attention-Based Spatial and Temporal Feature Blending,X. Yang; M. Mirmehdi; T. Burghardt,2019,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9022562,NO,"It is applicable to challenging camera trap footage in complex jungle environments and extends a traditional feature pyramid architecture by adding self-attention driven feature blending in both the spatial as well as the temporal domain. We evaluate the framework using 500 camera trap videos of great apes from the Pan African Programme containing 180K frames, which we manually annotated with accurate per-frame animal bounding boxes."
Attentive and Contrastive Learning for Joint Depth and Motion Field Estimation,S. Lee; F. Rameau; F. Pan; I. S. Kweon,2021,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9710937,NO,"Experiments on KITTI, Cityscapes, and Waymo Open Dataset demonstrate the relevance of our approach and show that our method outperforms state-of-the-art algorithms for the tasks of self-supervised monocular depth estimation, object motion segmentation, monocular scene flow estimation, and visual odometry. Second, we propose an object motion field estimation through contrastive sample consensus, called CSAC, taking advantage of weak semantic prior (bounding box from an object detector) and geometric constraints (each object respects the rigid body motion model)."
"MAAD: A Model and Dataset for ""Attended Awareness"" in Driving",D. Gopinath; G. Rosman; S. Stent; K. Terahata; L. Fletcher; B. Argall; J. Leonard,2021,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9607423,YES,"Our results show that our model is able to reasonably estimate attended awareness in a controlled setting, and in the future could potentially be extended to real egocentric driving data to help enable more effective ahead-of-time warnings in safety systems and thereby augment driver performance. We define ""attended awareness"" to be those parts of a potentially dynamic scene which a person has attended to in recent history and which they are still likely to be physically aware of."
A System for Fusing Color and Near-Infrared Images in Radiance Domain,K. C. Ng; J. Shen; C. M. Ho,2021,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9607811,NO,We proposed two methods to correct the fusion color: linear scalings when raw images were used and color swapping with base-detail image decomposition in the presence of nonlinearity in the ISP pipeline. Previous dehazing methods based on RGB-NIR fusion exist but have rarely addressed the issue of color fidelity and potential see-through effect of fusing with NIR image.
Few-Shot Learning With Embedded Class Models and Shot-Free Meta Training,A. Ravichandran; R. Bhotika; S. Soatto,2019,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9009038,NO,"Rather than fixing the class prototypes to be the Euclidean average of sample embeddings, we allow them to live in a higher-dimensional space (embedded class models) and learn the prototypes along with the model parameters. The class representation function is defined implicitly, which allows us to deal with a variable number of shots per class with a simple constant-size architecture."
"Monocular, One-stage, Regression of Multiple 3D People",Y. Sun; Q. Bao; W. Liu; Y. Fu; M. J. Black; T. Mei,2021,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9710639,YES,"Our method simultaneously predicts a Body Center heatmap and a Mesh Parameter map, which can jointly describe the 3D body mesh on the pixel level. Through a body-center-guided sampling process, the body mesh parameters of all people in the image are easily extracted from the Mesh Parameter map."
Counting With Focus for Free,Z. Shi; P. Mettes; C. Snoek,2019,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9010851,YES,"Second, we propose supervised focus from global density, where the ratio of point annotations to image pixels is used in another branch to regularize the overall density estimation. Experiments on six datasets show that all our contributions reduce the counting error, regardless of the base network, resulting in state-of-the-art accuracy using only a single network."
Multi-Instance Pose Networks: Rethinking Top-Down Pose Estimation,R. Khirodkar; V. Chari; A. Agrawal; A. Tyagi,2021,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9710259,NO,"Interestingly, when fewer, high confidence bounding boxes are used, HRNet’s performance degrades (by 5 AP) on OCHuman, whereas MIPNet maintains a relatively stable performance (drop of 1 AP) for the same inputs. When using ground truth bounding boxes for inference, MIP-Net achieves an improvement of 0.7 AP on COCO, 0.9 AP on CrowdPose, and 9.1 AP on OCHuman validation sets compared to HRNet."
Pixel-Perfect Structure-from-Motion with Featuremetric Refinement,P. Lindenberger; P. -E. Sarlin; V. Larsson; M. Pollefeys,2021,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9710072,YES,"In this paper, we refine two key steps of structure-from-motion by a direct alignment of low-level image information from multiple views: we first adjust the initial keypoint locations prior to any geometric estimation, and subsequently refine points and camera poses as a post-processing. This refinement is robust to large detection noise and appearance changes, as it optimizes a featuremetric error based on dense features predicted by a neural network."
Foreground-Aware Pyramid Reconstruction for Alignment-Free Occluded Person Re-Identification,H. Lingxiao; Y. Wang; W. Liu; H. Zhao; Z. Sun; J. Feng,2019,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9010027,NO,"The effectiveness of the proposed method is clearly demonstrated by the experimental results (Rank-1 accuracy) on three occluded person datasets: Partial REID (78.30%), Partial iLIDS (68.08%), Occluded REID (81.00%), and three benchmark person datasets: Market1501 (95.42%), DukeMTMC (88.64%), CUHK03 (76.08%). Then an alignment-free matching approach namely Foreground-aware Pyramid Reconstruction (FPR) is developed to accurately compute matching scores between occluded persons, regardless of their different scales and sizes."
Making Higher Order MOT Scalable: An Efficient Approximate Solver for Lifted Disjoint Paths,A. Hornakova; T. Kaiser; P. Swoboda; M. Rolinek; B. Rosenhahn; R. Henschel,2021,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9710859,NO,"Our approximate solver enables us to process the MOT15/16/17 benchmarks without sacrificing solution quality and allows for solving MOT20, which has been out of reach up to now for LDP solvers due to its size and complexity. On all these four standard MOT benchmarks we achieve performance comparable or better than current state-of-the-art methods including a tracker based on an optimal LDP solver."
Image Retrieval on Real-life Images with Pre-trained Vision-and-Language Models,Z. Liu; C. Rodriguez-Opazo; D. Teney; S. Gould,2021,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9710082,YES,"Existing methods have only been applied to non-complex images within narrow domains, such as fashion products, thereby limiting the scope of study on in-depth visual reasoning in rich image and language contexts. To address this issue, we collect the Compose Image Retrieval on Real-life images (CIRR) dataset, which consists of over 36,000 pairs of crowd-sourced, open-domain images with human-generated modifying text."
"Unidentified Video Objects: A Benchmark for Dense, Open-World Segmentation",W. Wang; M. Feiszli; H. Wang; D. Tran,2021,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9710887,NO,"Besides shifting the focus to the open-world setup, UVO is significantly larger, providing approximately 6 times more videos compared with DAVIS, and 7 times more mask (instance) annotations per video compared with YouTube-VO(I)S. UVO is also more challenging as it includes many videos with crowded scenes and complex background motions. We believe that UVO is a versatile testbed for researchers to develop novel approaches for open-world class-agnostic object segmentation, and inspires new research directions towards a more comprehensive video understanding beyond classification and detection."
YouRefIt: Embodied Reference Understanding with Language and Gesture,Y. Chen; Q. Li; D. Kong; Y. L. Kei; S. -C. Zhu; T. Gao; Y. Zhu; S. Huang,2021,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9711427,NO,"To the best of our knowledge, this is the first embodied reference dataset that allows us to study referring expressions in daily physical scenes to understand referential behavior, human communication, and human-robot interaction. To tackle this problem, we introduce YouRefIt, a new crowd-sourced dataset of embodied reference collected in various physical scenes; the dataset contains 4,195 unique reference clips in 432 indoor scenes."
CrowdDriven: A New Challenging Dataset for Outdoor Visual Localization,A. Jafarzadeh; M. L. Antequera; P. Gargallo; Y. Kuang; C. Toft; F. Kahl; T. Sattler,2021,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9710297,NO,"We propose a new benchmark for visual localization in outdoor scenes, using crowd-sourced data to cover a wide range of geographical regions and camera devices with a focus on the failure cases of current algorithms. Visual localization is the problem of estimating the position and orientation from which a given image (or a sequence of images) is taken in a known scene."
A Machine Teaching Framework for Scalable Recognition,P. Wang; N. Vasconcelos,2021,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9710974,NO,"For this, CMaxGrad leverages counterfactual explanations, which take into account student predictions, thereby proving feedback that is student-specific, explicitly addresses the causes of student confusion, and adapts to the level of competence of the student. A small amount of data is first labeled by experts and then used to teach online annotators for the classes of interest, who finally label the entire dataset."
SDVTracker: Real-Time Multi-Sensor Association and Tracking for Self-Driving Vehicles,S. Gautam; G. P. Meyer; C. Vallespi-Gonzalez; B. C. Becker,2021,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9607432,NO,"We show this system significantly outperforms hand-engineered methods on a real-world urban driving dataset while running in less than 2.5 ms on CPU for a scene with 100 actors, making it suitable for self-driving applications where low latency and high accuracy is critical. We present a practical and lightweight tracking system, SDV-Tracker, that uses a deep learned model for association and state estimation in conjunction with an Interacting Multiple Model (IMM) filter."
Body-Face Joint Detection via Embedding and Head Hook,J. Wan; J. Deng; X. Qiu; F. Zhou,2021,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9711464,YES,"Since the problem is unexplored yet, we design a new metric named log-average miss matching rate (mMR−2) to evaluate the association performance and extend the CrowdHuman and CityPersons benchmarks by annotating each face box. Experiments show that our BFJ detector can maintain state-of-the-art performance in pedestrian detection on both one-stage and two-stage structures while greatly outperform various body-face association strategies."
When Pigs Fly: Contextual Reasoning in Synthetic and Natural Scenes,P. Bomatter; M. Zhang; D. Karev; S. Madan; C. Tseng; G. Kreiman,2021,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9710269,YES,"While previous work has focused on crowd-sourced out-of-context photographs from the web to study scene context, controlling the nature and extent of contextual violations has been a daunting task. We conducted psychophysics experiments to establish a human benchmark for out-of-context recognition, and then compared it with state-of-the-art computer vision models to quantify the gap between the two."
UVStyle-Net: Unsupervised Few-shot Learning of 3D Style Similarity Measure for B-Reps,P. Meltzer; H. Shayani; A. Khasahmadi; P. K. Jayaraman; A. Sanghi; J. Lambourne,2021,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9710940,YES,"We propose UVStyle-Net, a style similarity measure for B-Reps that leverages the style signals in the second order statistics of the activations in a pre-trained (unsupervised) 3D encoder, and learns their relative importance to a subjective end-user through few-shot learning. We also show it is able to generate meaningful style gradients with respect to the input shape, and that few-shot learning with as few as two positive examples selected by an end-user is sufficient to significantly improve the style measure."
LookOut: Diverse Multi-Future Prediction and Planning for Self-Driving,A. Cui; S. Casas; A. Sadat; R. Liao; R. Urtasun,2021,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9710950,NO,"In this paper, we present LOOKOUT, a novel autonomy system that perceives the environment, predicts a diverse set of futures of how the scene might unroll and estimates the trajectory of the SDV by optimizing a set of contingency plans over these future realizations. Through extensive evaluations, we show that our model demonstrates significantly more diverse and sample-efficient motion forecasting in a large-scale self-driving dataset as well as safer and less-conservative motion plans in long-term closed-loop simulations when compared to current state-of-the-art models."
RAIN: Reinforced Hybrid Attention Inference Network for Motion Forecasting,J. Li; F. Yang; H. Ma; S. Malla; M. Tomizuka; C. Choi,2021,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9710777,NO,"We validate the framework on both synthetic simulations and motion forecasting benchmarks in different domains, demonstrating that our method not only achieves state-of-the-art forecasting performance, but also provides interpretable and reasonable hybrid attention weights. Motion forecasting plays a significant role in various domains (e.g., autonomous driving, human-robot interaction), which aims to predict future motion sequences given a set of historical observations."
